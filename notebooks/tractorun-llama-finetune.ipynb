{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e07cc9e5-2598-464b-b459-43d7dabf77ce",
            "metadata": {
                "cell_id": "e07cc9e5-2598-464b-b459-43d7dabf77ce",
                "tracto": {
                    "metadata_version": "1",
                    "view_cell_type": "MD",
                    "view_source": "This notebook uses docker image `cr.eu-north1.nebius.cloud/e00faee7vas5hpsh3s/solutions/torch:v3` that was created as\n\n```\nFROM ghcr.io/tractoai/notebook-kernel-default:2024-12-17-18-49-00-ceb64d083\n\nUSER root\n\nRUN pip install torch transformers peft trl datasets\nRUN pip install tractorun -U\n\nUSER 1000\n```"
                }
            },
            "source": "This notebook uses docker image `cr.eu-north1.nebius.cloud/e00faee7vas5hpsh3s/solutions/torch:v3` that was created as\n\n```\nFROM ghcr.io/tractoai/notebook-kernel-default:2024-12-17-18-49-00-ceb64d083\n\nUSER root\n\nRUN pip install torch transformers peft trl datasets\nRUN pip install tractorun -U\n\nUSER 1000\n```"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "2e9c46c6-f744-42bc-9383-03df091e9486",
            "metadata": {
                "cell_id": "2e9c46c6-f744-42bc-9383-03df091e9486",
                "tracto": {
                    "metadata_version": "1",
                    "view_cell_type": "CODE",
                    "view_source": "import yt.wrapper as yt\nimport uuid"
                }
            },
            "outputs": [],
            "source": "import yt.wrapper as yt\nimport uuid"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "ab45d016-489a-4836-9f5e-88a759ab7db9",
            "metadata": {
                "cell_id": "ab45d016-489a-4836-9f5e-88a759ab7db9",
                "tracto": {
                    "metadata_version": "1",
                    "view_cell_type": "CODE",
                    "view_source": "working_dir = f\"//tmp/examples/tractorun-mnist_{uuid.uuid4()}\"\nyt.create(\"map_node\", working_dir, recursive=True)\nprint(working_dir)"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "//tmp/examples/tractorun-mnist_d6aea2cf-158a-4657-b12a-693d7d9e270f\n"
                }
            ],
            "source": "working_dir = f\"//tmp/examples/tractorun-mnist_{uuid.uuid4()}\"\nyt.create(\"map_node\", working_dir, recursive=True)\nprint(working_dir)"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "ce92aa75-3fe0-4b6d-975e-d7e264a555f6",
            "metadata": {
                "cell_id": "ce92aa75-3fe0-4b6d-975e-d7e264a555f6",
                "tracto": {
                    "metadata_version": "1",
                    "view_cell_type": "CODE",
                    "view_source": "from datasets import load_dataset, Dataset\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom tractorun.toolbox import Toolbox\n\nimport os\n\n\ndef train(toolbox: Toolbox):\n    ytc = toolbox.yt_client\n\n    # Load the dataset. Since the dataset is small we just read it to memory.\n    dataset = Dataset.from_list(list(ytc.read_table(\"//home/samples/shakespeare\")))\n\n    # Load the model + tokenizer. Here we just download the weights of the model from HF.\n    # To avoid downloading weights every time they can be uploaded to Cypress and passed as cypress_binds.\n    model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        use_cache = False\n    )\n\n    # PEFT config.\n    lora_alpha = 16\n    lora_dropout = 0.1\n    lora_r = 64\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]\n    )\n\n    # Args.\n    max_seq_length = 512\n    output_dir = \"./results\"\n    per_device_train_batch_size = 8\n    gradient_accumulation_steps = 2\n    optim = \"adamw_hf\"\n    logging_steps = 1\n    learning_rate = 2e-4\n    max_grad_norm = 0.3\n    max_steps = len(dataset) // (per_device_train_batch_size * gradient_accumulation_steps * int(os.environ[\"WORLD_SIZE\"]))\n    warmup_ratio = 0.1\n    lr_scheduler_type = \"cosine\"\n    training_arguments = TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=per_device_train_batch_size,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        optim=optim,\n        save_strategy=\"steps\",\n        save_steps=0.0,  #  Do not save intermediate checkpoints, save only result.\n        logging_steps=logging_steps,\n        learning_rate=learning_rate,\n        fp16=True,\n        max_grad_norm=max_grad_norm,\n        max_steps=max_steps,\n        warmup_ratio=warmup_ratio,\n        group_by_length=True,\n        lr_scheduler_type=lr_scheduler_type,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={'use_reentrant':False},\n    )\n\n    # Trainer \n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=max_seq_length,\n        tokenizer=tokenizer,\n        args=training_arguments,\n    )\n\n    trainer.train()\n\n    # Upload result from one peer only.\n    if os.environ[\"RANK\"] == \"0\":\n        # Now the result of the fine-tuning is stored on a local filesystem in a job's sandbox\n        # which will be removed after the job is completed.\n        # We will move it to the Cypress.\n\n        local_result_path = output_dir\n        tracto_result_path = f\"{working_dir}/result\"\n\n        print(\"Uploading result to {tracto_result_path}\")\n\n        ytc = toolbox.yt_client\n\n        # Cypress does not like paths with trailing slashes,\n        # so we join paths carefully.\n        def join_paths(l, r):\n            if l and r:\n                return f\"{l}/{r}\"\n            else:\n                return l + r\n\n        def dfs(path):\n            local_path = join_paths(local_result_path, path)\n            tracto_path = join_paths(tracto_result_path, path)\n\n            if os.path.isdir(local_path):\n                print(f\"Creating directory {tracto_path}\")\n                ytc.create(\"map_node\", tracto_path, ignore_existing=True, recursive=True)\n                for f in os.listdir(local_path):\n                    dfs(join_paths(path, f))\n            else:\n                print(f\"Uploading file {tracto_path}\")\n                with open(local_path, \"rb\") as f:\n                    ytc.write_file(tracto_path, f)\n        dfs(\"\")\n\n        print(\"Results uploaded to {tracto_result_path}\")\n"
                }
            },
            "outputs": [],
            "source": "from datasets import load_dataset, Dataset\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom tractorun.toolbox import Toolbox\n\nimport os\n\n\ndef train(toolbox: Toolbox):\n    ytc = toolbox.yt_client\n\n    # Load the dataset. Since the dataset is small we just read it to memory.\n    dataset = Dataset.from_list(list(ytc.read_table(\"//home/samples/shakespeare\")))\n\n    # Load the model + tokenizer. Here we just download the weights of the model from HF.\n    # To avoid downloading weights every time they can be uploaded to Cypress and passed as cypress_binds.\n    model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        use_cache = False\n    )\n\n    # PEFT config.\n    lora_alpha = 16\n    lora_dropout = 0.1\n    lora_r = 64\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]\n    )\n\n    # Args.\n    max_seq_length = 512\n    output_dir = \"./results\"\n    per_device_train_batch_size = 8\n    gradient_accumulation_steps = 2\n    optim = \"adamw_hf\"\n    logging_steps = 1\n    learning_rate = 2e-4\n    max_grad_norm = 0.3\n    max_steps = len(dataset) // (per_device_train_batch_size * gradient_accumulation_steps * int(os.environ[\"WORLD_SIZE\"]))\n    warmup_ratio = 0.1\n    lr_scheduler_type = \"cosine\"\n    training_arguments = TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=per_device_train_batch_size,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        optim=optim,\n        save_strategy=\"steps\",\n        save_steps=0.0,  #  Do not save intermediate checkpoints, save only result.\n        logging_steps=logging_steps,\n        learning_rate=learning_rate,\n        fp16=True,\n        max_grad_norm=max_grad_norm,\n        max_steps=max_steps,\n        warmup_ratio=warmup_ratio,\n        group_by_length=True,\n        lr_scheduler_type=lr_scheduler_type,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={'use_reentrant':False},\n    )\n\n    # Trainer \n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=max_seq_length,\n        tokenizer=tokenizer,\n        args=training_arguments,\n    )\n\n    trainer.train()\n\n    # Upload result from one peer only.\n    if os.environ[\"RANK\"] == \"0\":\n        # Now the result of the fine-tuning is stored on a local filesystem in a job's sandbox\n        # which will be removed after the job is completed.\n        # We will move it to the Cypress.\n\n        local_result_path = output_dir\n        tracto_result_path = f\"{working_dir}/result\"\n\n        print(\"Uploading result to {tracto_result_path}\")\n\n        ytc = toolbox.yt_client\n\n        # Cypress does not like paths with trailing slashes,\n        # so we join paths carefully.\n        def join_paths(l, r):\n            if l and r:\n                return f\"{l}/{r}\"\n            else:\n                return l + r\n\n        def dfs(path):\n            local_path = join_paths(local_result_path, path)\n            tracto_path = join_paths(tracto_result_path, path)\n\n            if os.path.isdir(local_path):\n                print(f\"Creating directory {tracto_path}\")\n                ytc.create(\"map_node\", tracto_path, ignore_existing=True, recursive=True)\n                for f in os.listdir(local_path):\n                    dfs(join_paths(path, f))\n            else:\n                print(f\"Uploading file {tracto_path}\")\n                with open(local_path, \"rb\") as f:\n                    ytc.write_file(tracto_path, f)\n        dfs(\"\")\n\n        print(\"Results uploaded to {tracto_result_path}\")\n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "1823f614-c1f0-4a0e-beec-9b7d1f46c360",
            "metadata": {
                "cell_id": "1823f614-c1f0-4a0e-beec-9b7d1f46c360",
                "tracto": {
                    "metadata_version": "1",
                    "view_cell_type": "CODE",
                    "view_source": "from tractorun.backend.tractorch import YtTensorDataset, Tractorch\nfrom tractorun.toolbox import Toolbox\nfrom tractorun.run import run\nfrom tractorun.mesh import Mesh\nfrom tractorun.resources import Resources\nfrom tractorun.stderr_reader import StderrMode\nfrom tractorun.env import EnvVariable\n\nrun(\n    train,\n    backend=Tractorch(),\n    yt_path=f\"{working_dir}/working_dir\",\n    # Let's run training with dp=16: two nodes with 8 GPUs each.\n    # Since PyTorch does not like multi-GPU processes we launch 8 processes at each node.\n    mesh=Mesh(node_count=16, process_per_node=1, gpu_per_process=8, pool_trees=[\"default_gpu_h100\"]),\n    resources=Resources(\n        cpu_limit=100,\n        memory_limit=858993459200,  # 800G\n    ),\n    proxy_stderr_mode=StderrMode.primary,\n    env=[\n        EnvVariable(name=\"NCCL_SOCKET_IFNAME\", value=\"eth0\"),\n        EnvVariable(name=\"NCCL_IB_SL\", value=\"1\"),\n        EnvVariable(name=\"NCCL_DEBUG\", value=\"INFO\"),\n        EnvVariable(name=\"NCCL_DEBUG_SUBSYS\", value=\"INIT\"),\n        EnvVariable(name=\"NCCL_IB_HCA\", value=\"mlx5\"),\n    ]\n)"
                }
            },
            "outputs": [],
            "source": "from tractorun.backend.tractorch import YtTensorDataset, Tractorch\nfrom tractorun.toolbox import Toolbox\nfrom tractorun.run import run\nfrom tractorun.mesh import Mesh\nfrom tractorun.resources import Resources\nfrom tractorun.stderr_reader import StderrMode\nfrom tractorun.env import EnvVariable\n\nrun(\n    train,\n    backend=Tractorch(),\n    yt_path=f\"{working_dir}/working_dir\",\n    # Let's run training with dp=16: two nodes with 8 GPUs each.\n    # Since PyTorch does not like multi-GPU processes we launch 8 processes at each node.\n    mesh=Mesh(node_count=16, process_per_node=1, gpu_per_process=8, pool_trees=[\"default_gpu_h100\"]),\n    resources=Resources(\n        cpu_limit=100,\n        memory_limit=858993459200,  # 800G\n    ),\n    proxy_stderr_mode=StderrMode.primary,\n    env=[\n        EnvVariable(name=\"NCCL_SOCKET_IFNAME\", value=\"eth0\"),\n        EnvVariable(name=\"NCCL_IB_SL\", value=\"1\"),\n        EnvVariable(name=\"NCCL_DEBUG\", value=\"INFO\"),\n        EnvVariable(name=\"NCCL_DEBUG_SUBSYS\", value=\"INIT\"),\n        EnvVariable(name=\"NCCL_IB_HCA\", value=\"mlx5\"),\n    ]\n)"
        }
    ],
    "metadata": {
        "tracto": {
            "is_solution_notebook": true,
            "metadata_version": "1",
            "notebook_cypress_id": "76af0478-cc75-49b5-a7fa-679535603abb"
        },
        "is_solution_notebook": true
    },
    "nbformat": 4,
    "nbformat_minor": 5
}