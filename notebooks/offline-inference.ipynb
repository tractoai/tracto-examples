{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "50e61bd4-6dd0-4d31-b7d6-88c99eb19fb3",
            "metadata": {
                "tracto": {
                    "execution_end": 1750361926046,
                    "execution_session_id": "23588d31-5f42-4aab-9b88-c0fb11412935",
                    "execution_start": 1750361925902,
                    "metadata_version": "1",
                    "source_hash": "af06a360",
                    "view_cell_type": "CODE",
                    "view_source": "import requests\nimport uuid\nimport yt.wrapper as yt"
                }
            },
            "outputs": [],
            "source": "import requests\nimport uuid\nimport yt.wrapper as yt"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "af015773-1874-43d5-88ca-e75eb554a2cf",
            "metadata": {
                "tracto": {
                    "execution_end": 1750361926143,
                    "execution_session_id": "23588d31-5f42-4aab-9b88-c0fb11412935",
                    "execution_start": 1750361926076,
                    "hidden_input": true,
                    "metadata_version": "1",
                    "source_hash": "07f9f7a2",
                    "view_cell_type": "CODE",
                    "view_source": "working_dir = f\"//tmp/examples/offline-inference_{uuid.uuid4()}\"\nyt.create(\"map_node\", working_dir, recursive=True)\nprint(working_dir)"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Current working directory: //home/equal_amethyst_vulture/tmp/demo_workdir/5f5f6e12f7a64e6ba9028884db5783c6\n"
                }
            ],
            "source": "# configure environment to run this notebooks\nimport uuid\nimport yt.wrapper as yt\n\nusername = yt.get_user_name()\nif yt.exists(f\"//sys/users/{username}/@user_info/home_path\"):\n    # prepare working directory on distributed file system\n    user_info = yt.get(f\"//sys/users/{yt.get_user_name()}/@user_info\")\n    homedir = user_info[\"home_path\"]\n    # find avaliable vm presets\n    cpu_pool_trees = [pool_tree for pool_tree in user_info[\"available_pool_trees\"] if pool_tree.endswith(\"cpu\")] or [\"default\"]\n    h100_pool_trees = [pool_tree for pool_tree in user_info[\"available_pool_trees\"] if pool_tree.endswith(\"h100\")]\n    h100_8_pool_trees = [pool_tree for pool_tree in user_info[\"available_pool_trees\"] if pool_tree.endswith(\"h100_8\")]\n    workdir = f\"{homedir}/tmp/demo_workdir/{uuid.uuid4().hex}\"\nelse:\n    cpu_pool_trees = [\"default\"]\n    h100_pool_trees = [\"gpu_h100\"]\n    h100_8_pool_trees = [\"gpu_h100\"]\n    workdir = f\"//tmp/examples/{uuid.uuid4().hex}\"\n\nyt.create(\"map_node\", workdir, recursive=True, ignore_existing=True)\nprint(\"Current working directory:\", workdir)"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "3cabac83-2950-4ab8-bf11-387098844995",
            "metadata": {
                "tracto": {
                    "execution_end": 1750361927017,
                    "execution_session_id": "23588d31-5f42-4aab-9b88-c0fb11412935",
                    "execution_start": 1750361926170,
                    "metadata_version": "1",
                    "source_hash": "189e63ff",
                    "view_cell_type": "CODE",
                    "view_source": "import requests\nimport yt.wrapper as yt\n\n\nnames = requests.get('https://raw.githubusercontent.com/dominictarr/random-name/refs/heads/master/first-names.txt').content.decode(\"utf-8\").split(\"\\r\\n\")\n\nyt.create(\"table\", f\"{working_dir}/names\", ignore_existing=True)\nyt.write_table(f\"{working_dir}/names\", [{\"name\": name} for name in names])"
                }
            },
            "outputs": [],
            "source": "names = requests.get('https://raw.githubusercontent.com/dominictarr/random-name/refs/heads/master/first-names.txt').content.decode(\"utf-8\").split(\"\\r\\n\")\n\nyt.create(\"table\", f\"{workdir}/names\", ignore_existing=True)\nyt.write_table(f\"{workdir}/names\", [{\"name\": name} for name in names])"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "87fbfacf-601f-4aaa-9157-af7426ac9e4d",
            "metadata": {
                "tracto": {
                    "execution_end": 1750365331445,
                    "execution_session_id": "23588d31-5f42-4aab-9b88-c0fb11412935",
                    "execution_start": 1750361927020,
                    "metadata_version": "1",
                    "source_hash": "cab0d60c",
                    "view_cell_type": "CODE",
                    "view_source": "class StoriesGenerator:\n    def __init__(self):\n        self.model_loaded = False\n\n    def __call__(self, row):\n        import sys\n        if not self.model_loaded:\n            from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n            import torch\n\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            self.model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\").to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n            self.model_loaded = True\n\n        name = row[\"name\"]\n        prompt = f\"{name} was a little child \"\n        print(\"Prompt {}\".format(prompt), file=sys.stderr)\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        output = self.model.generate(input_ids, max_length = 100, num_beams=1)\n        output_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        yield {\"story\": output_text}\n\nyt.create(\"table\", f\"{working_dir}/tales\", force=True)\n\nyt.run_map(\n    StoriesGenerator(),\n    f\"{working_dir}/names\",\n    \"f{working_dir}/tales\",\n    spec={\n        \"job_count\": 100,\n        \"pool_trees\": [\"default_gpu_h100\"],\n        \"mapper\": {\n            \"gpu_limit\": 1,\n            \"cpu_limit\": 4.0,\n        },\n    }\n)"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 19:38:48,551\tINFO\tOperation started: https://playground.tracto.ai/playground/operations/68bf1bec-6e479302-24dd03e8-6506a5ef/details\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 19:38:48,597\tINFO\t( 0 min) operation 68bf1bec-6e479302-24dd03e8-6506a5ef starting\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 19:38:49,202\tINFO\t( 0 min) Unrecognized spec: {'enable_partitioned_data_balancing': false, 'mapper': {'title': 'StoriesGenerator'}}\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 19:38:51,607\tINFO\t( 0 min) operation 68bf1bec-6e479302-24dd03e8-6506a5ef: running=0     completed=0     pending=1     failed=0     aborted=0     lost=0     total=1     blocked=0    \n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 19:38:55,215\tINFO\t( 0 min) operation 68bf1bec-6e479302-24dd03e8-6506a5ef: running=1     completed=0     pending=0     failed=0     aborted=0     lost=0     total=1     blocked=0    \n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:08:27,158\tINFO\t(29 min) operation 68bf1bec-6e479302-24dd03e8-6506a5ef completed\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:08:27,220\tINFO\t(29 min) Alerts: {'low_cpu_usage': {'code': 1, 'message': \"Average CPU usage of some of your job types is significantly lower than requested 'cpu_limit'. Consider decreasing cpu_limit in spec of your operation\", 'attributes': {'pid': 1, 'tid': 11773568412224608766, 'thread': 'Controller:12', 'fid': 18445936631659228141, 'host': 'ca-0.controller-agents.nebius-playground.svc.kyt.k8s.nebius.yt', 'datetime': '2025-06-19T20:08:22.961010Z', 'trace_id': '779c4e02-586a060b-21845564-e43b82f', 'span_id': 2348756362661160422}, 'inner_errors': [{'code': 1, 'message': 'Jobs of task \"map\" use 25.27% of requested cpu limit', 'attributes': {'pid': 1, 'tid': 11773568412224608766, 'thread': 'Controller:12', 'fid': 18445936631659228141, 'host': 'ca-0.controller-agents.nebius-playground.svc.kyt.k8s.nebius.yt', 'datetime': '2025-06-19T20:08:22.960993Z', 'trace_id': '779c4e02-586a060b-21845564-e43b82f', 'span_id': 2348756362661160422, 'cpu_time': 1790574, 'cpu_limit': 4.0, 'exec_time': 1771618}}]}}\n"
                },
                {
                    "data": {
                        "text/plain": "<yt.wrapper.operation_commands.Operation at 0x7f07c5e3e780>"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "class StoriesGenerator:\n    def __init__(self):\n        self.model_loaded = False\n\n    def __call__(self, row):\n        import sys\n        if not self.model_loaded:\n            from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n            import torch\n\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            self.model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\").to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n            self.model_loaded = True\n\n        name = row[\"name\"]\n        prompt = f\"{name} was a little child \"\n        print(\"Prompt {}\".format(prompt), file=sys.stderr)\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        output = self.model.generate(input_ids, max_length=100, num_beams=1)\n        output_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        yield {\"story\": output_text}\n\nyt.create(\"table\", f\"{workdir}/tales\", force=True)\n\nyt.run_map(\n    StoriesGenerator(),\n    f\"{workdir}/names\",\n    f\"{workdir}/tales\",\n    spec={\n        \"job_count\": 1,\n        \"pool_trees\": h100_pool_trees,\n        \"mapper\": {\n            \"gpu_limit\": 1,\n            \"cpu_limit\": 4.0,\n        },\n    }\n)"
        }
    ],
    "metadata": {
        "is_solution_notebook": true,
        "tracto": {
            "is_solution_notebook": true,
            "metadata_version": "1",
            "notebook_cypress_id": "1618086a-0a74-4a99-a7c5-bea127891e6c"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}