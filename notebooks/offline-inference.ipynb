{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "id": "50e61bd4-6dd0-4d31-b7d6-88c99eb19fb3",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "af06a360",
                    "view_cell_type": "CODE",
                    "view_source": "import requests\nimport uuid\nimport yt.wrapper as yt"
                }
            },
            "outputs": [],
            "source": "import requests\nimport uuid\nimport yt.wrapper as yt"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "af015773-1874-43d5-88ca-e75eb554a2cf",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "f8319beb",
                    "view_cell_type": "CODE",
                    "view_source": "working_dir = f\"//tmp/examples/offline-inference_{uuid.uuid4()}\"\nyt.create(\"map_node\", working_dir, recursive=True)\nprint(working_dir)"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "//tmp/examples/offline-inference_cc03dfb4-2237-4323-a356-14574e6999f6\n"
                }
            ],
            "source": "username = yt.get_user_name()\nif yt.exists(f\"//sys/users/{username}/@user_info/home_path\"):\n    home = yt.get(f\"//sys/users/{username}/@user_info/home_path\")\n    working_dir = f\"{home}/{uuid.uuid4().hex}\"\nelse:\n    working_dir = f\"//tmp/examples/{uuid.uuid4().hex}\"\nyt.create(\"map_node\", working_dir)\nprint(working_dir)"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "3cabac83-2950-4ab8-bf11-387098844995",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "6eee10db",
                    "view_cell_type": "CODE",
                    "view_source": "import requests\nimport yt.wrapper as yt\n\n\nnames = requests.get('https://raw.githubusercontent.com/dominictarr/random-name/refs/heads/master/first-names.txt').content.decode(\"utf-8\").split(\"\\r\\n\")\n\nyt.create(\"table\", f\"{working_dir}/names\", ignore_existing=True)\nyt.write_table(f\"{working_dir}/names\", [{\"name\": name} for name in names])"
                }
            },
            "outputs": [],
            "source": "names = requests.get('https://raw.githubusercontent.com/dominictarr/random-name/refs/heads/master/first-names.txt').content.decode(\"utf-8\").split(\"\\r\\n\")\n\nyt.create(\"table\", f\"{working_dir}/names\", ignore_existing=True)\nyt.write_table(f\"{working_dir}/names\", [{\"name\": name} for name in names])"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "87fbfacf-601f-4aaa-9157-af7426ac9e4d",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "45b836bc",
                    "view_cell_type": "CODE",
                    "view_source": "class StoriesGenerator:\n    def __init__(self):\n        self.model_loaded = False\n\n    def __call__(self, row):\n        import sys\n        if not self.model_loaded:\n            from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n            import torch\n\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            self.model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\").to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n            self.model_loaded = True\n\n        name = row[\"name\"]\n        prompt = f\"{name} was a little child \"\n        print(\"Prompt {}\".format(prompt), file=sys.stderr)\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        output = self.model.generate(input_ids, max_length = 100, num_beams=1)\n        output_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        yield {\"story\": output_text}\n\nyt.create(\"table\", f\"{working_dir}/tales\", force=True)\n\nyt.run_map(\n    StoriesGenerator(),\n    f\"{working_dir}/names\",\n    \"f{working_dir}/tales\",\n    spec={\n        \"job_count\": 100,\n        \"pool_trees\": [\"default_gpu_h100\"],\n        \"mapper\": {\n            \"gpu_limit\": 1,\n            \"cpu_limit\": 4.0,\n        },\n    }\n)"
                }
            },
            "outputs": [],
            "source": "class StoriesGenerator:\n    def __init__(self):\n        self.model_loaded = False\n\n    def __call__(self, row):\n        import sys\n        if not self.model_loaded:\n            from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n            import torch\n\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            self.model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\").to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n            self.model_loaded = True\n\n        name = row[\"name\"]\n        prompt = f\"{name} was a little child \"\n        print(\"Prompt {}\".format(prompt), file=sys.stderr)\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        output = self.model.generate(input_ids, max_length = 100, num_beams=1)\n        output_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        yield {\"story\": output_text}\n\nyt.create(\"table\", f\"{working_dir}/tales\", force=True)\n\nyt.run_map(\n    StoriesGenerator(),\n    f\"{working_dir}/names\",\n    f\"{working_dir}/tales\",\n    spec={\n        \"job_count\": 20,\n        \"pool_trees\": [\"gpu_h100\"],\n        \"mapper\": {\n            \"gpu_limit\": 1,\n            \"cpu_limit\": 4.0,\n        },\n    }\n)"
        }
    ],
    "metadata": {
        "is_solution_notebook": true,
        "tracto": {
            "is_solution_notebook": true,
            "metadata_version": "1",
            "notebook_cypress_id": "1618086a-0a74-4a99-a7c5-bea127891e6c"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}