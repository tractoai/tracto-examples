{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "50e61bd4-6dd0-4d31-b7d6-88c99eb19fb3",
   "metadata": {
    "tracto": {
     "metadata_version": "1",
     "view_cell_type": "CODE",
     "view_source": "import requests\nimport uuid\nimport yt.wrapper as yt"
    }
   },
   "outputs": [],
   "source": "import requests\nimport uuid\nimport yt.wrapper as yt"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af015773-1874-43d5-88ca-e75eb554a2cf",
   "metadata": {
    "tracto": {
     "metadata_version": "1",
     "view_cell_type": "CODE",
     "view_source": "working_dir = f\"//tmp/examples/offline-inference_{uuid.uuid4()}\"\nyt.create(\"map_node\", working_dir, recursive=True)\nprint(working_dir)"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "//tmp/examples/offline-inference_cc03dfb4-2237-4323-a356-14574e6999f6\n"
    }
   ],
   "source": "working_dir = f\"//tmp/examples/offline-inference_{uuid.uuid4()}\"\nyt.create(\"map_node\", working_dir, recursive=True)\nprint(working_dir)"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cabac83-2950-4ab8-bf11-387098844995",
   "metadata": {
    "tracto": {
     "metadata_version": "1",
     "view_cell_type": "CODE",
     "view_source": "import requests\nimport yt.wrapper as yt\n\n\nnames = requests.get('https://raw.githubusercontent.com/dominictarr/random-name/refs/heads/master/first-names.txt').content.decode(\"utf-8\").split(\"\\r\\n\")\n\nyt.create(\"table\", f\"{working_dir}/names\", ignore_existing=True)\nyt.write_table(f\"{working_dir}/names\", [{\"name\": name} for name in names])"
    }
   },
   "outputs": [],
   "source": "import requests\nimport yt.wrapper as yt\n\n\nnames = requests.get('https://raw.githubusercontent.com/dominictarr/random-name/refs/heads/master/first-names.txt').content.decode(\"utf-8\").split(\"\\r\\n\")\n\nyt.create(\"table\", f\"{working_dir}/names\", ignore_existing=True)\nyt.write_table(f\"{working_dir}/names\", [{\"name\": name} for name in names])"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fbfacf-601f-4aaa-9157-af7426ac9e4d",
   "metadata": {
    "tracto": {
     "metadata_version": "1",
     "view_cell_type": "CODE",
     "view_source": "class StoriesGenerator:\n    def __init__(self):\n        self.model_loaded = False\n\n    def __call__(self, row):\n        import sys\n        if not self.model_loaded:\n            from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n            import torch\n\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            self.model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\").to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n            self.model_loaded = True\n\n        name = row[\"name\"]\n        prompt = f\"{name} was a little child \"\n        print(\"Prompt {}\".format(prompt), file=sys.stderr)\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        output = self.model.generate(input_ids, max_length = 100, num_beams=1)\n        output_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n        yield {\"story\": output_text}\n\nyt.create(\"table\", f\"{working_dir}/tales\", force=True)\n\nyt.run_map(\n    StoriesGenerator(),\n    f\"{working_dir}/names\",\n    \"f{working_dir}/tales\",\n    spec={\n        \"job_count\": 100,\n        \"pool_trees\": [\"default_gpu_h100\"],\n        \"mapper\": {\n            \"gpu_limit\": 1,\n            \"cpu_limit\": 4.0,\n        },\n    }\n)"
    }
   },
   "outputs": [],
   "source": [
    "class StoriesGenerator:\n",
    "    def __init__(self):\n",
    "        self.model_loaded = False\n",
    "\n",
    "    def __call__(self, row):\n",
    "        import sys\n",
    "        if not self.model_loaded:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "            import torch\n",
    "\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\").to(self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "            self.model_loaded = True\n",
    "\n",
    "        name = row[\"name\"]\n",
    "        prompt = f\"{name} was a little child \"\n",
    "        print(\"Prompt {}\".format(prompt), file=sys.stderr)\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        output = self.model.generate(input_ids, max_length = 100, num_beams=1)\n",
    "        output_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        yield {\"story\": output_text}\n",
    "\n",
    "yt.create(\"table\", f\"{working_dir}/tales\", force=True)\n",
    "\n",
    "yt.run_map(\n",
    "    StoriesGenerator(),\n",
    "    f\"{working_dir}/names\",\n",
    "    f\"{working_dir}/tales\",\n",
    "    spec={\n",
    "        \"job_count\": 100,\n",
    "        \"pool_trees\": [\"default_gpu_h100\"],\n",
    "        \"mapper\": {\n",
    "            \"gpu_limit\": 1,\n",
    "            \"cpu_limit\": 4.0,\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "tracto": {
   "is_solution_notebook": true,
   "metadata_version": "1",
   "notebook_cypress_id": "1618086a-0a74-4a99-a7c5-bea127891e6c"
  },
  "is_solution_notebook": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
