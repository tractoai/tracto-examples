{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "66e45ffc-6eb0-4c2f-b119-5947a99b81f8",
            "metadata": {
                "cell_id": "66e45ffc-6eb0-4c2f-b119-5947a99b81f8",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "cbffb39e",
                    "view_cell_type": "MD",
                    "view_source": "# Tractorun\n\nThis notebook demonstrates the `Tractorun` library and CLI tool for running distributed machine learning tasks on the `Tracto`. `Tractorun` provides convenient tools to integrate with Tracto' distributed data processing system, enabling execution and management of machine learning training jobs.\n\nTractorun:\n1. Manages the configuration and coordination of distributed training.\n2. Provides tools for working with `YtDataset` (allows you to use data on Tracto as a dataset), checkpoints, saving models, interacting with `tensorproxy`, and more.\n3. Ensures integration with the Tracto ecosystem.\n\nIn this notebook, we cover the following steps:\n\n1. Uploading a PyTorch dataset to Tracto.\n2. Training a model using MNIST. We perform model training on the MNIST dataset directly from a Jupyter Notebook, leveraging Tracto as the computation platform.\n3. Running the same training with Tractorun CLI. We'll demonstrate how to run the same training job via the command line using the Tractorun CLI.\n\nWe use the official PyTorch [MNIST training example](https://github.com/pytorch/examples/blob/cdef4d43fb1a2c6c4349daa5080e4e8731c34569/mnist/main.py) as a reference and show how to modify it with minimal changes to run using `Tractorun`."
                }
            },
            "source": "# Tractorun\n\nThis notebook demonstrates the `Tractorun` library and CLI tool for running distributed machine learning tasks on the `Tracto`. `Tractorun` provides convenient tools to integrate with Tracto distributed data processing system, enabling execution and management of machine learning training jobs.\n\nTractorun:\n1. Manages the configuration and coordination of distributed training.\n2. Provides tools for working with `YtDataset` (allows you to use data on Tracto as a dataset), checkpoints, saving models, interacting with `tensorproxy`, and more.\n3. Ensures integration with the Tracto ecosystem.\n\nIn this notebook, we cover the following steps:\n\n1. Uploading a PyTorch dataset to Tracto.\n2. Training a model using MNIST. We perform model training on the MNIST dataset directly from a Jupyter Notebook, leveraging Tracto as the computation platform.\n3. Running the same training with Tractorun CLI. We'll demonstrate how to run the same training job via the command line using the Tractorun CLI.\n\nWe use the official PyTorch [MNIST training example](https://github.com/pytorch/examples/blob/cdef4d43fb1a2c6c4349daa5080e4e8731c34569/mnist/main.py) as a reference and show how to modify it with minimal changes to run using `Tractorun`."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "b1a47927-d36a-4f54-8f8d-ae110fd975b6",
            "metadata": {
                "cell_id": "b1a47927-d36a-4f54-8f8d-ae110fd975b6",
                "tracto": {
                    "execution_end": 1750365746690,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365746493,
                    "metadata_version": "1",
                    "source_hash": "0e1b4c9a",
                    "view_cell_type": "CODE",
                    "view_source": "from yt import wrapper as yt\nfrom yt import type_info"
                }
            },
            "outputs": [],
            "source": "from yt import wrapper as yt\nfrom yt import type_info"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "45a75edb-917f-424e-9d3e-2d739ce44dcd",
            "metadata": {
                "cell_id": "45a75edb-917f-424e-9d3e-2d739ce44dcd",
                "tracto": {
                    "execution_end": 1750365746697,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365746691,
                    "metadata_version": "1",
                    "source_hash": "d8cac660",
                    "view_cell_type": "CODE",
                    "view_source": "import uuid\nimport sys\nimport io"
                }
            },
            "outputs": [],
            "source": "import uuid\nimport sys\nimport io"
        },
        {
            "cell_type": "markdown",
            "id": "0061a831-66ed-440f-8b8c-383cef32b4cf",
            "metadata": {
                "cell_id": "0061a831-66ed-440f-8b8c-383cef32b4cf",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "466132cb",
                    "view_cell_type": "MD",
                    "view_source": "## Create a base directory for examples"
                }
            },
            "source": "## Create a base directory for examples"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "925cef8c-5f37-4644-84a0-3dfa419b5c58",
            "metadata": {
                "cell_id": "925cef8c-5f37-4644-84a0-3dfa419b5c58",
                "tracto": {
                    "execution_end": 1750365746787,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365746702,
                    "hidden_input": true,
                    "metadata_version": "1",
                    "source_hash": "07f9f7a2",
                    "view_cell_type": "CODE",
                    "view_source": "working_dir = f\"//tmp/examples/tractorun-mnist_{uuid.uuid4()}\"\nyt.create(\"map_node\", working_dir, recursive=True)\nprint(working_dir)"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Current working directory: //home/equal_amethyst_vulture/tmp/demo_workdir/594e35db3a434a748e844f4d22c07e6c\n"
                }
            ],
            "source": "# configure environment to run this notebooks\nimport uuid\nimport yt.wrapper as yt\n\nusername = yt.get_user_name()\nif yt.exists(f\"//sys/users/{username}/@user_info/home_path\"):\n    # prepare working directory on distributed file system\n    user_info = yt.get(f\"//sys/users/{yt.get_user_name()}/@user_info\")\n    homedir = user_info[\"home_path\"]\n    # find avaliable vm presets\n    cpu_pool_trees = [pool_tree for pool_tree in user_info[\"available_pool_trees\"] if pool_tree.endswith(\"cpu\")] or [\"default\"]\n    h100_pool_trees = [pool_tree for pool_tree in user_info[\"available_pool_trees\"] if pool_tree.endswith(\"h100\")]\n    h100_8_pool_trees = [pool_tree for pool_tree in user_info[\"available_pool_trees\"] if pool_tree.endswith(\"h100-8\")]\n    workdir = f\"{homedir}/tmp/demo_workdir/{uuid.uuid4().hex}\"\nelse:\n    cpu_pool_trees = [\"default\"]\n    h100_pool_trees = [\"gpu_h100\"]\n    h100_8_pool_trees = [\"gpu_h100\"]\n    workdir = f\"//tmp/examples/{uuid.uuid4().hex}\"\n\nyt.create(\"map_node\", workdir, recursive=True, ignore_existing=True)\nprint(\"Current working directory:\", workdir)"
        },
        {
            "cell_type": "markdown",
            "id": "77500c37-f2af-4fa2-8a31-0c9bcf189dd5",
            "metadata": {
                "cell_id": "77500c37-f2af-4fa2-8a31-0c9bcf189dd5",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "6ed4cc5e",
                    "view_cell_type": "MD",
                    "view_source": "## Ensure torch and torchvision exist\n\nLet's ensure that the system has installed `torch` and `torchvision`."
                }
            },
            "source": "## Ensure torch and torchvision exist\n\nLet's ensure that the system has installed `torch` and `torchvision`."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "b3493766-efc5-41c2-9c2f-4ad17698835b",
            "metadata": {
                "cell_id": "b3493766-efc5-41c2-9c2f-4ad17698835b",
                "tracto": {
                    "execution_end": 1750365761330,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365761329,
                    "metadata_version": "1",
                    "source_hash": "bc4fcc58",
                    "view_cell_type": "CODE",
                    "view_source": "from torchvision import datasets, transforms"
                }
            },
            "outputs": [],
            "source": "import torch\nimport torchvision \nfrom torchvision import datasets, transforms"
        },
        {
            "cell_type": "markdown",
            "id": "810cc5d0-2200-47a7-9ccc-993aa27449c7",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "4fcd0a70",
                    "view_cell_type": "MD"
                }
            },
            "source": "## Upload dataset to Tracto\n\nFor this demonstration, we will use the MNIST dataset from the `torchvision` library and upload it to Tracto. Some rows in the dataset exceed the standard limits, so we will set `table_writer={\"max_row_weight\": 50 * 1024 * 1024}`."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "fd53105d-91e4-4eed-a4d0-806a28c807be",
            "metadata": {
                "cell_id": "fd53105d-91e4-4eed-a4d0-806a28c807be",
                "tracto": {
                    "execution_end": 1750365767572,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365761350,
                    "metadata_version": "1",
                    "source_hash": "810b983b",
                    "view_cell_type": "CODE",
                    "view_source": "# https://github.com/pytorch/examples/blob/26de41904319c7094afc53a3ee809de47112d387/mnist/main.py#L119\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n    ],\n)\n\ndataset_train_local = datasets.MNIST(\"./mnist\", train=True, download=True)\ndataset_test_local = datasets.MNIST(\"./mnist\", train=False, download=True)"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  0%|          | 0.00/9.91M [00:00<?, ?B/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  1%|          | 98.3k/9.91M [00:00<00:19, 514kB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  2%|\u258f         | 197k/9.91M [00:00<00:18, 518kB/s] "
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  4%|\u258d         | 426k/9.91M [00:00<00:09, 1.04MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  9%|\u2589         | 918k/9.91M [00:00<00:04, 2.21MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r 20%|\u2588\u2588        | 2.03M/9.91M [00:00<00:01, 4.89MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r 43%|\u2588\u2588\u2588\u2588\u258e     | 4.29M/9.91M [00:00<00:00, 10.2MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8.03M/9.91M [00:00<00:00, 14.7MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.91M/9.91M [00:00<00:00, 10.1MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  0%|          | 0.00/28.9k [00:00<?, ?B/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.9k/28.9k [00:00<00:00, 304kB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  0%|          | 0.00/1.65M [00:00<?, ?B/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  6%|\u258c         | 98.3k/1.65M [00:00<00:03, 507kB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r 10%|\u2589         | 164k/1.65M [00:00<00:02, 570kB/s] "
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r 26%|\u2588\u2588\u258c       | 426k/1.65M [00:00<00:01, 991kB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 918k/1.65M [00:00<00:00, 2.10MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.65M/1.65M [00:00<00:00, 2.40MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r  0%|          | 0.00/4.54k [00:00<?, ?B/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54k/4.54k [00:00<00:00, 8.62MB/s]"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "\n"
                }
            ],
            "source": "# https://github.com/pytorch/examples/blob/26de41904319c7094afc53a3ee809de47112d387/mnist/main.py#L119\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n    ],\n)\n\ndataset_train_local = datasets.MNIST(\"./mnist\", train=True, download=True)\ndataset_test_local = datasets.MNIST(\"./mnist\", train=False, download=True)"
        },
        {
            "cell_type": "markdown",
            "id": "a919be5d-3451-4548-ae0a-b9f260b23bc0",
            "metadata": {
                "cell_id": "a919be5d-3451-4548-ae0a-b9f260b23bc0",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "52dc5c88",
                    "view_cell_type": "MD",
                    "view_source": "Let's upload on Tracto the MNIST dataset as tensors and as simple types. There are 4 columns:\n* `image` - raw png image. This column has the tag \"image/png\" which allows to draw images directly in the Tracto UI.\n* `number` - human-readable label.\n* `data` and `labels` - serialized tensor form of dataset's data and label.\n\nIt is more efficient to save ready-to-use tensors in YT right away to save time and resources during model training. In the following examples, we will work only with columns containing tensors."
                }
            },
            "source": "Let's upload on Tracto the MNIST dataset as tensors and as simple types. There are 4 columns:\n* `image` - raw png image. This column has the tag \"image/png\" which allows to draw images directly in the Tracto UI.\n* `number` - human-readable label.\n* `data` and `labels` - serialized tensor form of dataset's data and label.\n\nIt is more efficient to save ready-to-use tensors in YT right away to save time and resources during model training. In the following examples, we will work only with columns containing tensors."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "277ffd54-1473-41fc-9a26-e834a62934c6",
            "metadata": {
                "cell_id": "277ffd54-1473-41fc-9a26-e834a62934c6",
                "tracto": {
                    "execution_end": 1750365767583,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365767573,
                    "metadata_version": "1",
                    "source_hash": "adc11484",
                    "view_cell_type": "CODE",
                    "view_source": "schema = yt.schema.TableSchema()\nschema.add_column(\"image\", type_info.Tagged[type_info.String, \"image/png\"])\nschema.add_column(\"number\", type_info.Int8)\nschema.add_column(\"data\", type_info.String)\nschema.add_column(\"labels\", type_info.String)"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "TableSchema({'value': [{'name': 'image', 'type_v3': {'type_name': 'tagged', 'item': 'string', 'tag': 'image/png'}}, {'name': 'number', 'type_v3': 'int8'}, {'name': 'data', 'type_v3': 'string'}, {'name': 'labels', 'type_v3': 'string'}], 'attributes': {'strict': True, 'unique_keys': False}})"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "schema = yt.schema.TableSchema()\nschema.add_column(\"image\", type_info.Tagged[type_info.String, \"image/png\"])\nschema.add_column(\"number\", type_info.Int8)\nschema.add_column(\"data\", type_info.String)\nschema.add_column(\"labels\", type_info.String)"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "0c8a1b24-eec2-4598-8385-4fea84a74952",
            "metadata": {
                "cell_id": "0c8a1b24-eec2-4598-8385-4fea84a74952",
                "tracto": {
                    "execution_end": 1750365808000,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365767590,
                    "metadata_version": "1",
                    "source_hash": "9383c616",
                    "view_cell_type": "CODE",
                    "view_source": "from tractorun.backend.tractorch import TensorSerializer\n\ndataset_train_path = f\"{working_dir}/dataset_train\"\ndataset_test_path = f\"{working_dir}/dataset_test\"\nprint(dataset_train_path)\nprint(dataset_test_path)\n\nyt.create(\"table\", dataset_train_path, force=True, attributes={\"schema\": schema.to_yson_type()})\nyt.create(\"table\", dataset_test_path, force=True, attributes={\"schema\": schema.to_yson_type()})\n\ndef pil_to_png(image):\n    r = io.BytesIO()\n    image.save(r, format=\"PNG\")\n    return r.getvalue()\n\nts = TensorSerializer()\n\nyt_train_data = [\n    {\n        \"image\": pil_to_png(data),\n        \"number\": labels,\n        \"labels\": ts.serialize(labels),\n        \"data\": ts.serialize(transform(data)),\n    }\n    for data, labels in dataset_train_local\n]\nyt.write_table(dataset_train_path, yt_train_data, table_writer={\"max_row_weight\": 50 * 1024 * 1024})\n\nyt_test_data = [\n    {\n        \"image\": pil_to_png(data),\n        \"number\": labels,\n        \"labels\": ts.serialize(labels),\n        \"data\": ts.serialize(transform(data)),\n    }\n    for data, labels in dataset_test_local\n]\nyt.write_table(dataset_test_path, yt_test_data, table_writer={\"max_row_weight\": 50 * 1024 * 1024})"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "//home/equal_amethyst_vulture/tmp/demo_workdir/594e35db3a434a748e844f4d22c07e6c/dataset_train\n//home/equal_amethyst_vulture/tmp/demo_workdir/594e35db3a434a748e844f4d22c07e6c/dataset_test\n"
                }
            ],
            "source": "from tractorun.backend.tractorch import TensorSerializer\n\ndataset_train_path = f\"{workdir}/dataset_train\"\ndataset_test_path = f\"{workdir}/dataset_test\"\nprint(dataset_train_path)\nprint(dataset_test_path)\n\nyt.create(\"table\", dataset_train_path, force=True, attributes={\"schema\": schema.to_yson_type()})\nyt.create(\"table\", dataset_test_path, force=True, attributes={\"schema\": schema.to_yson_type()})\n\ndef pil_to_png(image):\n    r = io.BytesIO()\n    image.save(r, format=\"PNG\")\n    return r.getvalue()\n\nts = TensorSerializer()\n\nyt_train_data = [\n    {\n        \"image\": pil_to_png(data),\n        \"number\": labels,\n        \"labels\": ts.serialize(labels),\n        \"data\": ts.serialize(transform(data)),\n    }\n    for data, labels in dataset_train_local\n]\nyt.write_table(dataset_train_path, yt_train_data, table_writer={\"max_row_weight\": 50 * 1024 * 1024})\n\nyt_test_data = [\n    {\n        \"image\": pil_to_png(data),\n        \"number\": labels,\n        \"labels\": ts.serialize(labels),\n        \"data\": ts.serialize(transform(data)),\n    }\n    for data, labels in dataset_test_local\n]\nyt.write_table(dataset_test_path, yt_test_data, table_writer={\"max_row_weight\": 50 * 1024 * 1024})"
        },
        {
            "cell_type": "markdown",
            "id": "f698778d-9712-487f-ace8-3e37a2ef62f5",
            "metadata": {
                "cell_id": "f698778d-9712-487f-ace8-3e37a2ef62f5",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "beb0577b",
                    "view_cell_type": "MD",
                    "view_source": "Tractorun store some data to the training dir:\n1. Checkpoints.\n2. Metadata about each training run.\n3. Models.\n4. Some locks.\n5. etc\n\nLet's create and cleanup the training dir."
                }
            },
            "source": "## Run training\n\nTractorun store some data to the training dir:\n1. Checkpoints.\n2. Metadata about each training run.\n3. Models.\n4. Some locks.\n5. etc\n\nLet's create and cleanup the training dir."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "d8ec8d0a-f0e1-4f56-a14a-64525411715b",
            "metadata": {
                "cell_id": "d8ec8d0a-f0e1-4f56-a14a-64525411715b",
                "tracto": {
                    "execution_end": 1750365808030,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365808016,
                    "metadata_version": "1",
                    "source_hash": "9367cd04",
                    "view_cell_type": "CODE",
                    "view_source": "training_dir = f\"{working_dir}/tractorun\"\nyt.create(\"map_node\", training_dir, force=True)\n\nprint(training_dir)"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "//home/equal_amethyst_vulture/tmp/demo_workdir/594e35db3a434a748e844f4d22c07e6c/tractorun\n"
                }
            ],
            "source": "training_dir = f\"{workdir}/tractorun\"\nyt.create(\"map_node\", training_dir, force=True)\n\nprint(training_dir)"
        },
        {
            "cell_type": "markdown",
            "id": "c81b855e-415b-4e5b-b6ae-d8d411c50cfb",
            "metadata": {
                "cell_id": "c81b855e-415b-4e5b-b6ae-d8d411c50cfb",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "0cd58846",
                    "view_cell_type": "MD",
                    "view_source": "We use the official PyTorch [MNIST training example](https://github.com/pytorch/examples/blob/cdef4d43fb1a2c6c4349daa5080e4e8731c34569/mnist/main.py) as a reference and show how to modify it with minimal changes to run using Tractorun:\n1. Add `toolbox: Toolbox` to the main function. Toolbox object provides useful utils for training like checkpoint manager, coordination metadata, initialized ytsaurus client, and more.\n2. Add `file=sys.stderr` to each print.\n3. Use `YtTensorDataset` instead of default `torch.Dataset`.\n4. Call magic function `tractorun.run.run`.\n\n<details>\n  <summary>Show the full diff</summary>\n\n```diff\n@@ -6,6 +6,13 @@\n from torchvision import datasets, transforms\n from torch.optim.lr_scheduler import StepLR\n\n+from tractorun.backend.tractorch import YtTensorDataset, Tractorch\n+from tractorun.toolbox import Toolbox\n+from tractorun.run import run\n+from tractorun.mesh import Mesh\n+from tractorun.resources import Resources\n+from tractorun.stderr_reader import StderrMode\n+from tractorun.backend.tractorch.serializer import TensorSerializer\n\n class Net(nn.Module):\n     def __init__(self):\n@@ -45,7 +52,7 @@\n         if batch_idx % args.log_interval == 0:\n             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                 epoch, batch_idx * len(data), len(train_loader.dataset),\n-                100. * batch_idx / len(train_loader), loss.item()))\n+                100. * batch_idx / len(train_loader), loss.item()), file=sys.stderr)\n             if args.dry_run:\n                 break\n\n@@ -66,10 +73,10 @@\n\n     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n         test_loss, correct, len(test_loader.dataset),\n-        100. * correct / len(test_loader.dataset)))\n+        100. * correct / len(test_loader.dataset)), file=sys.stderr)\n\n\n-def main():\n+def main(toolbox: Toolbox):\n     # Training settings\n     parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n     parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n@@ -94,7 +101,7 @@\n                         help='how many batches to wait before logging training status')\n     parser.add_argument('--save-model', action='store_true', default=False,\n                         help='For Saving the current Model')\n-    args = parser.parse_args()\n+    args = parser.parse_args([])\n     use_cuda = not args.no_cuda and torch.cuda.is_available()\n     use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n@@ -120,10 +127,9 @@\n         transforms.ToTensor(),\n         transforms.Normalize((0.1307,), (0.3081,))\n         ])\n-    dataset1 = datasets.MNIST('../data', train=True, download=True,\n-                       transform=transform)\n-    dataset2 = datasets.MNIST('../data', train=False,\n-                       transform=transform)\n+    dataset1 = YtTensorDataset(toolbox=toolbox, path=dataset_train_path, columns=['data', 'labels'])\n+    dataset2 = YtTensorDataset(toolbox=toolbox, path=dataset_test_path, columns=['data', 'labels'])\n+\n     train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n     test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n@@ -137,9 +143,20 @@\n         scheduler.step()\n\n     if args.save_model:\n-        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n+        ts = TensorSerializer()\n+        toolbox.save_model(ts.serialize(model.state_dict()), dataset_train_path, metadata={})\n\n\n-if __name__ == '__main__':\n-    main()\n+run(\n+    main,\n+    backend=Tractorch(),\n+    yt_path=training_dir,\n+    mesh=Mesh(node_count=1, process_per_node=1, gpu_per_process=0),\n+    resources=Resources(\n+        cpu_limit=8,\n+        memory_limit=105899345920,\n+    ),\n+    proxy_stderr_mode=StderrMode.primary,\n+)\n```\n</details>"
                }
            },
            "source": "The model training process run in a Docker container. When launching from a Jupyter Notebook, it is important to ensure that the same container as in the `Kernel` is used.\n\nWe use the official PyTorch [MNIST training example](https://github.com/pytorch/examples/blob/cdef4d43fb1a2c6c4349daa5080e4e8731c34569/mnist/main.py) as a reference and show how to modify it with minimal changes to run using Tractorun:\n1. Add `toolbox: Toolbox` to the main function. Toolbox object provides useful utils for training like checkpoint manager, coordination metadata, initialized ytsaurus client, and more.\n2. Add `file=sys.stderr` to each print.\n3. Use `YtTensorDataset` instead of default `torch.Dataset`.\n4. Call magic function `tractorun.run.run`.\n\n<details>\n  <summary>Show the full diff</summary>\n\n```diff\n@@ -6,6 +6,13 @@\n from torchvision import datasets, transforms\n from torch.optim.lr_scheduler import StepLR\n\n+from tractorun.backend.tractorch import YtTensorDataset, Tractorch\n+from tractorun.toolbox import Toolbox\n+from tractorun.run import run\n+from tractorun.mesh import Mesh\n+from tractorun.resources import Resources\n+from tractorun.stderr_reader import StderrMode\n+from tractorun.backend.tractorch.serializer import TensorSerializer\n\n class Net(nn.Module):\n     def __init__(self):\n@@ -45,7 +52,7 @@\n         if batch_idx % args.log_interval == 0:\n             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                 epoch, batch_idx * len(data), len(train_loader.dataset),\n-                100. * batch_idx / len(train_loader), loss.item()))\n+                100. * batch_idx / len(train_loader), loss.item()), file=sys.stderr)\n             if args.dry_run:\n                 break\n\n@@ -66,10 +73,10 @@\n\n     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n         test_loss, correct, len(test_loader.dataset),\n-        100. * correct / len(test_loader.dataset)))\n+        100. * correct / len(test_loader.dataset)), file=sys.stderr)\n\n\n-def main():\n+def main(toolbox: Toolbox):\n     # Training settings\n     parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n     parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n@@ -94,7 +101,7 @@\n                         help='how many batches to wait before logging training status')\n     parser.add_argument('--save-model', action='store_true', default=False,\n                         help='For Saving the current Model')\n-    args = parser.parse_args()\n+    args = parser.parse_args([])\n     use_cuda = not args.no_cuda and torch.cuda.is_available()\n     use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n@@ -120,10 +127,9 @@\n         transforms.ToTensor(),\n         transforms.Normalize((0.1307,), (0.3081,))\n         ])\n-    dataset1 = datasets.MNIST('../data', train=True, download=True,\n-                       transform=transform)\n-    dataset2 = datasets.MNIST('../data', train=False,\n-                       transform=transform)\n+    dataset1 = YtTensorDataset(path=dataset_train_path, columns=['data', 'labels'])\n+    dataset2 = YtTensorDataset(path=dataset_test_path, columns=['data', 'labels'])\n+\n     train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n     test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n@@ -137,9 +143,20 @@\n         scheduler.step()\n\n     if args.save_model:\n-        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n+        ts = TensorSerializer()\n+        toolbox.save_model(ts.serialize(model.state_dict()), dataset_train_path, metadata={})\n\n\n-if __name__ == '__main__':\n-    main()\n+run(\n+    main,\n+    backend=Tractorch(),\n+    yt_path=training_dir,\n+    mesh=Mesh(node_count=1, process_per_node=1, gpu_per_process=1, pool_trees=h100_pool_trees),\n+    resources=Resources(\n+        cpu_limit=8,\n+        memory_limit=105899345920,\n+    ),\n+    proxy_stderr_mode=StderrMode.primary,\n+)\n```\n</details>"
        },
        {
            "cell_type": "markdown",
            "id": "61e04047-8b7c-4b16-9417-4781360c90e0",
            "metadata": {
                "cell_id": "61e04047-8b7c-4b16-9417-4781360c90e0",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "0c582f1b",
                    "view_cell_type": "MD",
                    "view_source": "<font color=\"red\">IMPORTANT NOTE</font> In this example we are running tractorun directly from Jupyter notebook.\n\nThis is a convenient method for experiments and demonstrations, as tractorun uses [pickle](https://docs.python.org/3/library/pickle.html) for easy serialization of the entire notebook state and transferring it to the cluster. This means that all variables will be available in the model training function, and tractorun will attempt to transfer all Python modules from the local environment to the cluster.\n\nHowever, this method does not ensure reproducibility of the run of model's training. For production processes, use the execution via the tractorun CLI, which is described below."
                }
            },
            "source": "<font color=\"red\">IMPORTANT NOTE</font> In this example we are running tractorun directly from Jupyter notebook.\n\nThis is a convenient method for experiments and demonstrations, as tractorun uses [pickle](https://docs.python.org/3/library/pickle.html) for easy serialization of the entire notebook state and transferring it to the cluster. This means that all variables will be available in the model training function, and tractorun will attempt to transfer all Python modules from the local environment to the cluster.\n\nHowever, this method does not ensure reproducibility of the run of model's training. For production processes, use the execution via the tractorun CLI, which is described below."
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "2ad67093-9e8e-4ecd-9790-62af82ff4206",
            "metadata": {
                "cell_id": "2ad67093-9e8e-4ecd-9790-62af82ff4206",
                "tracto": {
                    "execution_end": 1750366092476,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750365864474,
                    "metadata_version": "1",
                    "source_hash": "f086ff4d",
                    "view_cell_type": "CODE",
                    "view_source": "import argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom tractorun.backend.tractorch import YtTensorDataset, Tractorch\nfrom tractorun.toolbox import Toolbox\nfrom tractorun.run import run\nfrom tractorun.mesh import Mesh\nfrom tractorun.resources import Resources\nfrom tractorun.stderr_reader import StderrMode\nfrom tractorun.backend.tractorch.serializer import TensorSerializer\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()), file=sys.stderr)\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)), file=sys.stderr)\n\n\ndef main(toolbox: Toolbox):\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--no-mps', action='store_true', default=False,\n                        help='disables macOS GPU training')\n    parser.add_argument('--dry-run', action='store_true', default=False,\n                        help='quickly check a single pass')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args([])\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n    torch.manual_seed(args.seed)\n\n    if use_cuda:\n        device = torch.device(\"cuda\")\n    elif use_mps:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    train_kwargs = {'batch_size': args.batch_size}\n    test_kwargs = {'batch_size': args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    dataset1 = YtTensorDataset(toolbox=toolbox, path=dataset_train_path, yt_client=toolbox.yt_client, columns=['data', 'labels'])\n    dataset2 = YtTensorDataset(toolbox=toolbox, path=dataset_test_path, yt_client=toolbox.yt_client, columns=['data', 'labels'])\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n    if args.save_model:\n        ts = TensorSerializer()\n        toolbox.save_model(ts.serialize(model.state_dict()), dataset_train_path, metadata={})\n\n\nrun(\n    main,\n    backend=Tractorch(),\n    yt_path=training_dir,\n    mesh=Mesh(node_count=1, process_per_node=1, gpu_per_process=0),\n    resources=Resources(\n        cpu_limit=8,\n        memory_limit=105899345920,\n    ),\n    proxy_stderr_mode=StderrMode.primary,\n)"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:44:24,979\tWARNING\tCannot locate file of the module (__name__: torch.ops, __file__: _ops.py)\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:44:24,981\tWARNING\tCannot locate file of the module (__name__: torch.classes, __file__: _classes.py)\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:44:27,225\tINFO\tOperation started: https://playground.tracto.ai/playground/operations/833c9385-39d5f04a-24dd03e8-b3861697/details\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:44:27,268\tINFO\t( 0 min) operation 833c9385-39d5f04a-24dd03e8-b3861697 initializing\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:44:30,066\tINFO\t( 0 min) Unrecognized spec: {'enable_partitioned_data_balancing': false}\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:44:30,118\tINFO\t( 0 min) operation 833c9385-39d5f04a-24dd03e8-b3861697: running=0     completed=0     pending=1     failed=0     aborted=0     lost=0     total=1     blocked=0    \n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:44:31,292\tINFO\t( 0 min) operation 833c9385-39d5f04a-24dd03e8-b3861697: running=1     completed=0     pending=0     failed=0     aborted=0     lost=0     total=1     blocked=0    \n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/slot/sandbox/_py_runner.py:108: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n  __tar.extractall(destination)\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/slot/sandbox/_py_runner.py:108: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n  __tar.extractall(destination)\nFailed to write user statistics\n[rank0]:[W619 20:44:34.999815987 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.297187\nTrain Epoch: 1 [640/60000 (1%)]\tLoss: 1.151452\nTrain Epoch: 1 [1280/60000 (2%)]\tLoss: 0.655327\nTrain Epoch: 1 [1920/60000 (3%)]\tLoss: 0.280406\nTrain Epoch: 1 [2560/60000 (4%)]\tLoss: 0.459387\nTrain Epoch: 1 [3200/60000 (5%)]\tLoss: 0.322426\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.260275\nTrain Epoch: 1 [4480/60000 (7%)]\tLoss: 0.214572\nTrain Epoch: 1 [5120/60000 (9%)]\tLoss: 0.453676\nTrain Epoch: 1 [5760/60000 (10%)]\tLoss: 0.176776\nTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.233968\nTrain Epoch: 1 [7040/60000 (12%)]\tLoss: 0.151826\nTrain Epoch: 1 [7680/60000 (13%)]\tLoss: 0.212932\nTrain Epoch: 1 [8320/60000 (14%)]\tLoss: 0.099575\nTrain Epoch: 1 [8960/60000 (15%)]\tLoss: 0.310798\nTrain Epoch: 1 [9600/60000 (16%)]\tLoss: 0.116789\nTrain Epoch: 1 [10240/60000 (17%)]\tLoss: 0.205280\nTrain Epoch: 1 [10880/60000 (18%)]\tLoss: 0.091013\nTrain Epoch: 1 [11520/60000 (19%)]\tLoss: 0.472717\nTrain Epoch: 1 [12160/60000 (20%)]\tLoss: 0.210800\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.155833\nTrain Epoch: 1 [13440/60000 (22%)]\tLoss: 0.228109\nTrain Epoch: 1 [14080/60000 (23%)]\tLoss: 0.148850\nTrain Epoch: 1 [14720/60000 (25%)]\tLoss: 0.395773\nTrain Epoch: 1 [15360/60000 (26%)]\tLoss: 0.199488\nTrain Epoch: 1 [16000/60000 (27%)]\tLoss: 0.129073\nTrain Epoch: 1 [16640/60000 (28%)]\tLoss: 0.154095\nTrain Epoch: 1 [17280/60000 (29%)]\tLoss: 0.154797\nTrain Epoch: 1 [17920/60000 (30%)]\tLoss: 0.186295\nTrain Epoch: 1 [18560/60000 (31%)]\tLoss: 0.180962\nTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.213578\nTrain Epoch: 1 [19840/60000 (33%)]\tLoss: 0.130878\nTrain Epoch: 1 [20480/60000 (34%)]\tLoss: 0.083003\nTrain Epoch: 1 [21120/60000 (35%)]\tLoss: 0.136077\nTrain Epoch: 1 [21760/60000 (36%)]\tLoss: 0.047166\nTrain Epoch: 1 [22400/60000 (37%)]\tLoss: 0.116064\nTrain Epoch: 1 [23040/60000 (38%)]\tLoss: 0.205679\nTrain Epoch: 1 [23680/60000 (39%)]\tLoss: 0.188162\nTrain Epoch: 1 [24320/60000 (41%)]\tLoss: 0.032503\nTrain Epoch: 1 [24960/60000 (42%)]\tLoss: 0.108924\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.030290\nTrain Epoch: 1 [26240/60000 (44%)]\tLoss: 0.109807\nTrain Epoch: 1 [26880/60000 (45%)]\tLoss: 0.376081\nTrain Epoch: 1 [27520/60000 (46%)]\tLoss: 0.265171\nTrain Epoch: 1 [28160/60000 (47%)]\tLoss: 0.122725\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.176180\nTrain Epoch: 1 [29440/60000 (49%)]\tLoss: 0.170886\nTrain Epoch: 1 [30080/60000 (50%)]\tLoss: 0.114147\nTrain Epoch: 1 [30720/60000 (51%)]\tLoss: 0.091431\nTrain Epoch: 1 [31360/60000 (52%)]\tLoss: 0.162190\nTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.105227\nTrain Epoch: 1 [32640/60000 (54%)]\tLoss: 0.095047\nTrain Epoch: 1 [33280/60000 (55%)]\tLoss: 0.103836\nTrain Epoch: 1 [33920/60000 (57%)]\tLoss: 0.023094\nTrain Epoch: 1 [34560/60000 (58%)]\tLoss: 0.033909\nTrain Epoch: 1 [35200/60000 (59%)]\tLoss: 0.298395\nTrain Epoch: 1 [35840/60000 (60%)]\tLoss: 0.204469\nTrain Epoch: 1 [36480/60000 (61%)]\tLoss: 0.020137\nTrain Epoch: 1 [37120/60000 (62%)]\tLoss: 0.240941\nTrain Epoch: 1 [37760/60000 (63%)]\tLoss: 0.181144\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.113453\nTrain Epoch: 1 [39040/60000 (65%)]\tLoss: 0.014007\nTrain Epoch: 1 [39680/60000 (66%)]\tLoss: 0.046932\nTrain Epoch: 1 [40320/60000 (67%)]\tLoss: 0.073808\nTrain Epoch: 1 [40960/60000 (68%)]\tLoss: 0.136846\nTrain Epoch: 1 [41600/60000 (69%)]\tLoss: 0.052428\nTrain Epoch: 1 [42240/60000 (70%)]\tLoss: 0.012738\nTrain Epoch: 1 [42880/60000 (71%)]\tLoss: 0.205041\nTrain Epoch: 1 [43520/60000 (72%)]\tLoss: 0.093863\nTrain Epoch: 1 [44160/60000 (74%)]\tLoss: 0.017944\nTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.156354\nTrain Epoch: 1 [45440/60000 (76%)]\tLoss: 0.180018\nTrain Epoch: 1 [46080/60000 (77%)]\tLoss: 0.202034\nTrain Epoch: 1 [46720/60000 (78%)]\tLoss: 0.222469\nTrain Epoch: 1 [47360/60000 (79%)]\tLoss: 0.101083\nTrain Epoch: 1 [48000/60000 (80%)]\tLoss: 0.116674\nTrain Epoch: 1 [48640/60000 (81%)]\tLoss: 0.044034\nTrain Epoch: 1 [49280/60000 (82%)]\tLoss: 0.054982\nTrain Epoch: 1 [49920/60000 (83%)]\tLoss: 0.048923\nTrain Epoch: 1 [50560/60000 (84%)]\tLoss: 0.084616\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.157914\nTrain Epoch: 1 [51840/60000 (86%)]\tLoss: 0.012655\nTrain Epoch: 1 [52480/60000 (87%)]\tLoss: 0.019177\nTrain Epoch: 1 [53120/60000 (88%)]\tLoss: 0.095291\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.076519\nTrain Epoch: 1 [54400/60000 (91%)]\tLoss: 0.027908\nTrain Epoch: 1 [55040/60000 (92%)]\tLoss: 0.055535\nTrain Epoch: 1 [55680/60000 (93%)]\tLoss: 0.239158\nTrain Epoch: 1 [56320/60000 (94%)]\tLoss: 0.106052\nTrain Epoch: 1 [56960/60000 (95%)]\tLoss: 0.047536\nTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.147367\nTrain Epoch: 1 [58240/60000 (97%)]\tLoss: 0.034207\nTrain Epoch: 1 [58880/60000 (98%)]\tLoss: 0.019820\nTrain Epoch: 1 [59520/60000 (99%)]\tLoss: 0.001800\n\nTest set: Average loss: 0.0531, Accuracy: 9821/10000 (98%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.055343\nTrain Epoch: 2 [640/60000 (1%)]\tLoss: 0.012630\nTrain Epoch: 2 [1280/60000 (2%)]\tLoss: 0.051676\nTrain Epoch: 2 [1920/60000 (3%)]\tLoss: 0.186355\nTrain Epoch: 2 [2560/60000 (4%)]\tLoss: 0.038949\nTrain Epoch: 2 [3200/60000 (5%)]\tLoss: 0.071093\nTrain Epoch: 2 [3840/60000 (6%)]\tLoss: 0.013675\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.074458\nTrain Epoch: 2 [5120/60000 (9%)]\tLoss: 0.130145\nTrain Epoch: 2 [5760/60000 (10%)]\tLoss: 0.072042\nTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.175434\nTrain Epoch: 2 [7040/60000 (12%)]\tLoss: 0.208757\nTrain Epoch: 2 [7680/60000 (13%)]\tLoss: 0.057683\nTrain Epoch: 2 [8320/60000 (14%)]\tLoss: 0.030621\nTrain Epoch: 2 [8960/60000 (15%)]\tLoss: 0.173466\nTrain Epoch: 2 [9600/60000 (16%)]\tLoss: 0.051166\nTrain Epoch: 2 [10240/60000 (17%)]\tLoss: 0.166062\nTrain Epoch: 2 [10880/60000 (18%)]\tLoss: 0.020858\nTrain Epoch: 2 [11520/60000 (19%)]\tLoss: 0.129360\nTrain Epoch: 2 [12160/60000 (20%)]\tLoss: 0.077300\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.169679\nTrain Epoch: 2 [13440/60000 (22%)]\tLoss: 0.021208\nTrain Epoch: 2 [14080/60000 (23%)]\tLoss: 0.016967\nTrain Epoch: 2 [14720/60000 (25%)]\tLoss: 0.261026\nTrain Epoch: 2 [15360/60000 (26%)]\tLoss: 0.032498\nTrain Epoch: 2 [16000/60000 (27%)]\tLoss: 0.077778\nTrain Epoch: 2 [16640/60000 (28%)]\tLoss: 0.173440\nTrain Epoch: 2 [17280/60000 (29%)]\tLoss: 0.004483\nTrain Epoch: 2 [17920/60000 (30%)]\tLoss: 0.079376\nTrain Epoch: 2 [18560/60000 (31%)]\tLoss: 0.060435\nTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.110940\nTrain Epoch: 2 [19840/60000 (33%)]\tLoss: 0.046422\nTrain Epoch: 2 [20480/60000 (34%)]\tLoss: 0.022971\nTrain Epoch: 2 [21120/60000 (35%)]\tLoss: 0.158656\nTrain Epoch: 2 [21760/60000 (36%)]\tLoss: 0.016802\nTrain Epoch: 2 [22400/60000 (37%)]\tLoss: 0.003443\nTrain Epoch: 2 [23040/60000 (38%)]\tLoss: 0.182725\nTrain Epoch: 2 [23680/60000 (39%)]\tLoss: 0.111012\nTrain Epoch: 2 [24320/60000 (41%)]\tLoss: 0.013066\nTrain Epoch: 2 [24960/60000 (42%)]\tLoss: 0.022215\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.028423\nTrain Epoch: 2 [26240/60000 (44%)]\tLoss: 0.057520\nTrain Epoch: 2 [26880/60000 (45%)]\tLoss: 0.056292\nTrain Epoch: 2 [27520/60000 (46%)]\tLoss: 0.109932\nTrain Epoch: 2 [28160/60000 (47%)]\tLoss: 0.115284\nTrain Epoch: 2 [28800/60000 (48%)]\tLoss: 0.048093\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.016914\nTrain Epoch: 2 [30080/60000 (50%)]\tLoss: 0.034693\nTrain Epoch: 2 [30720/60000 (51%)]\tLoss: 0.006550\nTrain Epoch: 2 [31360/60000 (52%)]\tLoss: 0.039912\nTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.091674\nTrain Epoch: 2 [32640/60000 (54%)]\tLoss: 0.088837\nTrain Epoch: 2 [33280/60000 (55%)]\tLoss: 0.125683\nTrain Epoch: 2 [33920/60000 (57%)]\tLoss: 0.001679\nTrain Epoch: 2 [34560/60000 (58%)]\tLoss: 0.072327\nTrain Epoch: 2 [35200/60000 (59%)]\tLoss: 0.213168\nTrain Epoch: 2 [35840/60000 (60%)]\tLoss: 0.109259\nTrain Epoch: 2 [36480/60000 (61%)]\tLoss: 0.017648\nTrain Epoch: 2 [37120/60000 (62%)]\tLoss: 0.036501\nTrain Epoch: 2 [37760/60000 (63%)]\tLoss: 0.081584\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.098594\nTrain Epoch: 2 [39040/60000 (65%)]\tLoss: 0.031305\nTrain Epoch: 2 [39680/60000 (66%)]\tLoss: 0.071395\nTrain Epoch: 2 [40320/60000 (67%)]\tLoss: 0.029138\nTrain Epoch: 2 [40960/60000 (68%)]\tLoss: 0.073308\nTrain Epoch: 2 [41600/60000 (69%)]\tLoss: 0.047244\nTrain Epoch: 2 [42240/60000 (70%)]\tLoss: 0.013511\nTrain Epoch: 2 [42880/60000 (71%)]\tLoss: 0.023361\nTrain Epoch: 2 [43520/60000 (72%)]\tLoss: 0.015855\nTrain Epoch: 2 [44160/60000 (74%)]\tLoss: 0.004349\nTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.167593\nTrain Epoch: 2 [45440/60000 (76%)]\tLoss: 0.037365\nTrain Epoch: 2 [46080/60000 (77%)]\tLoss: 0.073395\nTrain Epoch: 2 [46720/60000 (78%)]\tLoss: 0.097431\nTrain Epoch: 2 [47360/60000 (79%)]\tLoss: 0.071117\nTrain Epoch: 2 [48000/60000 (80%)]\tLoss: 0.089292\nTrain Epoch: 2 [48640/60000 (81%)]\tLoss: 0.079015\nTrain Epoch: 2 [49280/60000 (82%)]\tLoss: 0.006771\nTrain Epoch: 2 [49920/60000 (83%)]\tLoss: 0.054702\nTrain Epoch: 2 [50560/60000 (84%)]\tLoss: 0.058995\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.157642\nTrain Epoch: 2 [51840/60000 (86%)]\tLoss: 0.010756\nTrain Epoch: 2 [52480/60000 (87%)]\tLoss: 0.033263\nTrain Epoch: 2 [53120/60000 (88%)]\tLoss: 0.111738\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.166027\nTrain Epoch: 2 [54400/60000 (91%)]\tLoss: 0.056248\nTrain Epoch: 2 [55040/60000 (92%)]\tLoss: 0.028482\nTrain Epoch: 2 [55680/60000 (93%)]\tLoss: 0.106696\nTrain Epoch: 2 [56320/60000 (94%)]\tLoss: 0.011191\nTrain Epoch: 2 [56960/60000 (95%)]\tLoss: 0.068173\nTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.113268\nTrain Epoch: 2 [58240/60000 (97%)]\tLoss: 0.010139\nTrain Epoch: 2 [58880/60000 (98%)]\tLoss: 0.001691\nTrain Epoch: 2 [59520/60000 (99%)]\tLoss: 0.001134\n\nTest set: Average loss: 0.0368, Accuracy: 9874/10000 (99%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.035998\nTrain Epoch: 3 [640/60000 (1%)]\tLoss: 0.035131\nTrain Epoch: 3 [1280/60000 (2%)]\tLoss: 0.022164\nTrain Epoch: 3 [1920/60000 (3%)]\tLoss: 0.050526\nTrain Epoch: 3 [2560/60000 (4%)]\tLoss: 0.036326\nTrain Epoch: 3 [3200/60000 (5%)]\tLoss: 0.076263\nTrain Epoch: 3 [3840/60000 (6%)]\tLoss: 0.008077\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.022358\nTrain Epoch: 3 [5120/60000 (9%)]\tLoss: 0.108047\nTrain Epoch: 3 [5760/60000 (10%)]\tLoss: 0.037212\nTrain Epoch: 3 [6400/60000 (11%)]\tLoss: 0.229487\nTrain Epoch: 3 [7040/60000 (12%)]\tLoss: 0.123731\nTrain Epoch: 3 [7680/60000 (13%)]\tLoss: 0.002899\nTrain Epoch: 3 [8320/60000 (14%)]\tLoss: 0.008598\nTrain Epoch: 3 [8960/60000 (15%)]\tLoss: 0.039022\nTrain Epoch: 3 [9600/60000 (16%)]\tLoss: 0.055236\nTrain Epoch: 3 [10240/60000 (17%)]\tLoss: 0.245753\nTrain Epoch: 3 [10880/60000 (18%)]\tLoss: 0.030345\nTrain Epoch: 3 [11520/60000 (19%)]\tLoss: 0.032746\nTrain Epoch: 3 [12160/60000 (20%)]\tLoss: 0.018732\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 0.047048\nTrain Epoch: 3 [13440/60000 (22%)]\tLoss: 0.001346\nTrain Epoch: 3 [14080/60000 (23%)]\tLoss: 0.061606\nTrain Epoch: 3 [14720/60000 (25%)]\tLoss: 0.105719\nTrain Epoch: 3 [15360/60000 (26%)]\tLoss: 0.020326\nTrain Epoch: 3 [16000/60000 (27%)]\tLoss: 0.057254\nTrain Epoch: 3 [16640/60000 (28%)]\tLoss: 0.175086\nTrain Epoch: 3 [17280/60000 (29%)]\tLoss: 0.001227\nTrain Epoch: 3 [17920/60000 (30%)]\tLoss: 0.038979\nTrain Epoch: 3 [18560/60000 (31%)]\tLoss: 0.008026\nTrain Epoch: 3 [19200/60000 (32%)]\tLoss: 0.103182\nTrain Epoch: 3 [19840/60000 (33%)]\tLoss: 0.107235\nTrain Epoch: 3 [20480/60000 (34%)]\tLoss: 0.010420\nTrain Epoch: 3 [21120/60000 (35%)]\tLoss: 0.135499\nTrain Epoch: 3 [21760/60000 (36%)]\tLoss: 0.023162\nTrain Epoch: 3 [22400/60000 (37%)]\tLoss: 0.003474\nTrain Epoch: 3 [23040/60000 (38%)]\tLoss: 0.063993\nTrain Epoch: 3 [23680/60000 (39%)]\tLoss: 0.028102\nTrain Epoch: 3 [24320/60000 (41%)]\tLoss: 0.001352\nTrain Epoch: 3 [24960/60000 (42%)]\tLoss: 0.003201\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 0.002796\nTrain Epoch: 3 [26240/60000 (44%)]\tLoss: 0.009640\nTrain Epoch: 3 [26880/60000 (45%)]\tLoss: 0.184511\nTrain Epoch: 3 [27520/60000 (46%)]\tLoss: 0.101277\nTrain Epoch: 3 [28160/60000 (47%)]\tLoss: 0.137858\nTrain Epoch: 3 [28800/60000 (48%)]\tLoss: 0.033973\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.026598\nTrain Epoch: 3 [30080/60000 (50%)]\tLoss: 0.049574\nTrain Epoch: 3 [30720/60000 (51%)]\tLoss: 0.009797\nTrain Epoch: 3 [31360/60000 (52%)]\tLoss: 0.028671\nTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.152943\nTrain Epoch: 3 [32640/60000 (54%)]\tLoss: 0.002780\nTrain Epoch: 3 [33280/60000 (55%)]\tLoss: 0.013898\nTrain Epoch: 3 [33920/60000 (57%)]\tLoss: 0.001850\nTrain Epoch: 3 [34560/60000 (58%)]\tLoss: 0.037016\nTrain Epoch: 3 [35200/60000 (59%)]\tLoss: 0.068334\nTrain Epoch: 3 [35840/60000 (60%)]\tLoss: 0.119198\nTrain Epoch: 3 [36480/60000 (61%)]\tLoss: 0.043351\nTrain Epoch: 3 [37120/60000 (62%)]\tLoss: 0.002975\nTrain Epoch: 3 [37760/60000 (63%)]\tLoss: 0.014533\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 0.046028\nTrain Epoch: 3 [39040/60000 (65%)]\tLoss: 0.004802\nTrain Epoch: 3 [39680/60000 (66%)]\tLoss: 0.044915\nTrain Epoch: 3 [40320/60000 (67%)]\tLoss: 0.016868\nTrain Epoch: 3 [40960/60000 (68%)]\tLoss: 0.092914\nTrain Epoch: 3 [41600/60000 (69%)]\tLoss: 0.020002\nTrain Epoch: 3 [42240/60000 (70%)]\tLoss: 0.067856\nTrain Epoch: 3 [42880/60000 (71%)]\tLoss: 0.057132\nTrain Epoch: 3 [43520/60000 (72%)]\tLoss: 0.104511\nTrain Epoch: 3 [44160/60000 (74%)]\tLoss: 0.076640\nTrain Epoch: 3 [44800/60000 (75%)]\tLoss: 0.053277\nTrain Epoch: 3 [45440/60000 (76%)]\tLoss: 0.037113\nTrain Epoch: 3 [46080/60000 (77%)]\tLoss: 0.089701\nTrain Epoch: 3 [46720/60000 (78%)]\tLoss: 0.074101\nTrain Epoch: 3 [47360/60000 (79%)]\tLoss: 0.084965\nTrain Epoch: 3 [48000/60000 (80%)]\tLoss: 0.025964\nTrain Epoch: 3 [48640/60000 (81%)]\tLoss: 0.016049\nTrain Epoch: 3 [49280/60000 (82%)]\tLoss: 0.016920\nTrain Epoch: 3 [49920/60000 (83%)]\tLoss: 0.043257\nTrain Epoch: 3 [50560/60000 (84%)]\tLoss: 0.093241\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.178684\nTrain Epoch: 3 [51840/60000 (86%)]\tLoss: 0.006504\nTrain Epoch: 3 [52480/60000 (87%)]\tLoss: 0.001427\nTrain Epoch: 3 [53120/60000 (88%)]\tLoss: 0.034453\nTrain Epoch: 3 [53760/60000 (90%)]\tLoss: 0.154635\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.044087\nTrain Epoch: 3 [55040/60000 (92%)]\tLoss: 0.008781\nTrain Epoch: 3 [55680/60000 (93%)]\tLoss: 0.075547\nTrain Epoch: 3 [56320/60000 (94%)]\tLoss: 0.028225\nTrain Epoch: 3 [56960/60000 (95%)]\tLoss: 0.023397\nTrain Epoch: 3 [57600/60000 (96%)]\tLoss: 0.007754\nTrain Epoch: 3 [58240/60000 (97%)]\tLoss: 0.003960\nTrain Epoch: 3 [58880/60000 (98%)]\tLoss: 0.031429\nTrain Epoch: 3 [59520/60000 (99%)]\tLoss: 0.000211\n\nTest set: Average loss: 0.0354, Accuracy: 9879/10000 (99%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.019309\nTrain Epoch: 4 [640/60000 (1%)]\tLoss: 0.007156\nTrain Epoch: 4 [1280/60000 (2%)]\tLoss: 0.050882\nTrain Epoch: 4 [1920/60000 (3%)]\tLoss: 0.099027\nTrain Epoch: 4 [2560/60000 (4%)]\tLoss: 0.005677\nTrain Epoch: 4 [3200/60000 (5%)]\tLoss: 0.043182\nTrain Epoch: 4 [3840/60000 (6%)]\tLoss: 0.000934\nTrain Epoch: 4 [4480/60000 (7%)]\tLoss: 0.027193\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.053861\nTrain Epoch: 4 [5760/60000 (10%)]\tLoss: 0.059850\nTrain Epoch: 4 [6400/60000 (11%)]\tLoss: 0.226501\nTrain Epoch: 4 [7040/60000 (12%)]\tLoss: 0.148055\nTrain Epoch: 4 [7680/60000 (13%)]\tLoss: 0.052073\nTrain Epoch: 4 [8320/60000 (14%)]\tLoss: 0.011789\nTrain Epoch: 4 [8960/60000 (15%)]\tLoss: 0.180389\nTrain Epoch: 4 [9600/60000 (16%)]\tLoss: 0.028161\nTrain Epoch: 4 [10240/60000 (17%)]\tLoss: 0.093487\nTrain Epoch: 4 [10880/60000 (18%)]\tLoss: 0.013568\nTrain Epoch: 4 [11520/60000 (19%)]\tLoss: 0.040349\nTrain Epoch: 4 [12160/60000 (20%)]\tLoss: 0.104169\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 0.047595\nTrain Epoch: 4 [13440/60000 (22%)]\tLoss: 0.011837\nTrain Epoch: 4 [14080/60000 (23%)]\tLoss: 0.013405\nTrain Epoch: 4 [14720/60000 (25%)]\tLoss: 0.100648\nTrain Epoch: 4 [15360/60000 (26%)]\tLoss: 0.050497\nTrain Epoch: 4 [16000/60000 (27%)]\tLoss: 0.023087\nTrain Epoch: 4 [16640/60000 (28%)]\tLoss: 0.062420\nTrain Epoch: 4 [17280/60000 (29%)]\tLoss: 0.001508\nTrain Epoch: 4 [17920/60000 (30%)]\tLoss: 0.032228\nTrain Epoch: 4 [18560/60000 (31%)]\tLoss: 0.029300\nTrain Epoch: 4 [19200/60000 (32%)]\tLoss: 0.031895\nTrain Epoch: 4 [19840/60000 (33%)]\tLoss: 0.084870\nTrain Epoch: 4 [20480/60000 (34%)]\tLoss: 0.007625\nTrain Epoch: 4 [21120/60000 (35%)]\tLoss: 0.039520\nTrain Epoch: 4 [21760/60000 (36%)]\tLoss: 0.008097\nTrain Epoch: 4 [22400/60000 (37%)]\tLoss: 0.000685\nTrain Epoch: 4 [23040/60000 (38%)]\tLoss: 0.016763\nTrain Epoch: 4 [23680/60000 (39%)]\tLoss: 0.038221\nTrain Epoch: 4 [24320/60000 (41%)]\tLoss: 0.001779\nTrain Epoch: 4 [24960/60000 (42%)]\tLoss: 0.004941\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 0.008351\nTrain Epoch: 4 [26240/60000 (44%)]\tLoss: 0.064124\nTrain Epoch: 4 [26880/60000 (45%)]\tLoss: 0.235339\nTrain Epoch: 4 [27520/60000 (46%)]\tLoss: 0.056976\nTrain Epoch: 4 [28160/60000 (47%)]\tLoss: 0.052070\nTrain Epoch: 4 [28800/60000 (48%)]\tLoss: 0.002723\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.024374\nTrain Epoch: 4 [30080/60000 (50%)]\tLoss: 0.046285\nTrain Epoch: 4 [30720/60000 (51%)]\tLoss: 0.014300\nTrain Epoch: 4 [31360/60000 (52%)]\tLoss: 0.022897\nTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 0.027582\nTrain Epoch: 4 [32640/60000 (54%)]\tLoss: 0.009686\nTrain Epoch: 4 [33280/60000 (55%)]\tLoss: 0.041614\nTrain Epoch: 4 [33920/60000 (57%)]\tLoss: 0.000505\nTrain Epoch: 4 [34560/60000 (58%)]\tLoss: 0.027413\nTrain Epoch: 4 [35200/60000 (59%)]\tLoss: 0.144564\nTrain Epoch: 4 [35840/60000 (60%)]\tLoss: 0.067465\nTrain Epoch: 4 [36480/60000 (61%)]\tLoss: 0.013156\nTrain Epoch: 4 [37120/60000 (62%)]\tLoss: 0.015246\nTrain Epoch: 4 [37760/60000 (63%)]\tLoss: 0.045308\nTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 0.115934\nTrain Epoch: 4 [39040/60000 (65%)]\tLoss: 0.001571\nTrain Epoch: 4 [39680/60000 (66%)]\tLoss: 0.040211\nTrain Epoch: 4 [40320/60000 (67%)]\tLoss: 0.011835\nTrain Epoch: 4 [40960/60000 (68%)]\tLoss: 0.043492\nTrain Epoch: 4 [41600/60000 (69%)]\tLoss: 0.014568\nTrain Epoch: 4 [42240/60000 (70%)]\tLoss: 0.025475\nTrain Epoch: 4 [42880/60000 (71%)]\tLoss: 0.005383\nTrain Epoch: 4 [43520/60000 (72%)]\tLoss: 0.065291\nTrain Epoch: 4 [44160/60000 (74%)]\tLoss: 0.007047\nTrain Epoch: 4 [44800/60000 (75%)]\tLoss: 0.033142\nTrain Epoch: 4 [45440/60000 (76%)]\tLoss: 0.027312\nTrain Epoch: 4 [46080/60000 (77%)]\tLoss: 0.036218\nTrain Epoch: 4 [46720/60000 (78%)]\tLoss: 0.055799\nTrain Epoch: 4 [47360/60000 (79%)]\tLoss: 0.038974\nTrain Epoch: 4 [48000/60000 (80%)]\tLoss: 0.017743\nTrain Epoch: 4 [48640/60000 (81%)]\tLoss: 0.003821\nTrain Epoch: 4 [49280/60000 (82%)]\tLoss: 0.007995\nTrain Epoch: 4 [49920/60000 (83%)]\tLoss: 0.062214\nTrain Epoch: 4 [50560/60000 (84%)]\tLoss: 0.029941\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 0.104660\nTrain Epoch: 4 [51840/60000 (86%)]\tLoss: 0.001841\nTrain Epoch: 4 [52480/60000 (87%)]\tLoss: 0.004581\nTrain Epoch: 4 [53120/60000 (88%)]\tLoss: 0.099187\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.085444\nTrain Epoch: 4 [54400/60000 (91%)]\tLoss: 0.065219\nTrain Epoch: 4 [55040/60000 (92%)]\tLoss: 0.009957\nTrain Epoch: 4 [55680/60000 (93%)]\tLoss: 0.081367\nTrain Epoch: 4 [56320/60000 (94%)]\tLoss: 0.007698\nTrain Epoch: 4 [56960/60000 (95%)]\tLoss: 0.002708\nTrain Epoch: 4 [57600/60000 (96%)]\tLoss: 0.067434\nTrain Epoch: 4 [58240/60000 (97%)]\tLoss: 0.005489\nTrain Epoch: 4 [58880/60000 (98%)]\tLoss: 0.000655\nTrain Epoch: 4 [59520/60000 (99%)]\tLoss: 0.000311\n\nTest set: Average loss: 0.0360, Accuracy: 9883/10000 (99%)\n\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.054344\nTrain Epoch: 5 [640/60000 (1%)]\tLoss: 0.007711\nTrain Epoch: 5 [1280/60000 (2%)]\tLoss: 0.004860\nTrain Epoch: 5 [1920/60000 (3%)]\tLoss: 0.122553\nTrain Epoch: 5 [2560/60000 (4%)]\tLoss: 0.022949\nTrain Epoch: 5 [3200/60000 (5%)]\tLoss: 0.021151\nTrain Epoch: 5 [3840/60000 (6%)]\tLoss: 0.001764\nTrain Epoch: 5 [4480/60000 (7%)]\tLoss: 0.019123\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.108693\nTrain Epoch: 5 [5760/60000 (10%)]\tLoss: 0.074519\nTrain Epoch: 5 [6400/60000 (11%)]\tLoss: 0.155919\nTrain Epoch: 5 [7040/60000 (12%)]\tLoss: 0.117220\nTrain Epoch: 5 [7680/60000 (13%)]\tLoss: 0.016270\nTrain Epoch: 5 [8320/60000 (14%)]\tLoss: 0.020996\nTrain Epoch: 5 [8960/60000 (15%)]\tLoss: 0.117874\nTrain Epoch: 5 [9600/60000 (16%)]\tLoss: 0.036429\nTrain Epoch: 5 [10240/60000 (17%)]\tLoss: 0.235311\nTrain Epoch: 5 [10880/60000 (18%)]\tLoss: 0.004112\nTrain Epoch: 5 [11520/60000 (19%)]\tLoss: 0.026990\nTrain Epoch: 5 [12160/60000 (20%)]\tLoss: 0.013289\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.078599\nTrain Epoch: 5 [13440/60000 (22%)]\tLoss: 0.001820\nTrain Epoch: 5 [14080/60000 (23%)]\tLoss: 0.009864\nTrain Epoch: 5 [14720/60000 (25%)]\tLoss: 0.072245\nTrain Epoch: 5 [15360/60000 (26%)]\tLoss: 0.019007\nTrain Epoch: 5 [16000/60000 (27%)]\tLoss: 0.038803\nTrain Epoch: 5 [16640/60000 (28%)]\tLoss: 0.111652\nTrain Epoch: 5 [17280/60000 (29%)]\tLoss: 0.015713\nTrain Epoch: 5 [17920/60000 (30%)]\tLoss: 0.002488\nTrain Epoch: 5 [18560/60000 (31%)]\tLoss: 0.016839\nTrain Epoch: 5 [19200/60000 (32%)]\tLoss: 0.120247\nTrain Epoch: 5 [19840/60000 (33%)]\tLoss: 0.014115\nTrain Epoch: 5 [20480/60000 (34%)]\tLoss: 0.005825\nTrain Epoch: 5 [21120/60000 (35%)]\tLoss: 0.037587\nTrain Epoch: 5 [21760/60000 (36%)]\tLoss: 0.031794\nTrain Epoch: 5 [22400/60000 (37%)]\tLoss: 0.003499\nTrain Epoch: 5 [23040/60000 (38%)]\tLoss: 0.057465\nTrain Epoch: 5 [23680/60000 (39%)]\tLoss: 0.043561\nTrain Epoch: 5 [24320/60000 (41%)]\tLoss: 0.000708\nTrain Epoch: 5 [24960/60000 (42%)]\tLoss: 0.001017\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.019573\nTrain Epoch: 5 [26240/60000 (44%)]\tLoss: 0.006059\nTrain Epoch: 5 [26880/60000 (45%)]\tLoss: 0.028023\nTrain Epoch: 5 [27520/60000 (46%)]\tLoss: 0.076646\nTrain Epoch: 5 [28160/60000 (47%)]\tLoss: 0.113340\nTrain Epoch: 5 [28800/60000 (48%)]\tLoss: 0.008981\nTrain Epoch: 5 [29440/60000 (49%)]\tLoss: 0.027962\nTrain Epoch: 5 [30080/60000 (50%)]\tLoss: 0.013651\nTrain Epoch: 5 [30720/60000 (51%)]\tLoss: 0.045368\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.003602\nTrain Epoch: 5 [32000/60000 (53%)]\tLoss: 0.046046\nTrain Epoch: 5 [32640/60000 (54%)]\tLoss: 0.071401\nTrain Epoch: 5 [33280/60000 (55%)]\tLoss: 0.035767\nTrain Epoch: 5 [33920/60000 (57%)]\tLoss: 0.001899\nTrain Epoch: 5 [34560/60000 (58%)]\tLoss: 0.012618\nTrain Epoch: 5 [35200/60000 (59%)]\tLoss: 0.083234\nTrain Epoch: 5 [35840/60000 (60%)]\tLoss: 0.022124\nTrain Epoch: 5 [36480/60000 (61%)]\tLoss: 0.004019\nTrain Epoch: 5 [37120/60000 (62%)]\tLoss: 0.052680\nTrain Epoch: 5 [37760/60000 (63%)]\tLoss: 0.078588\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 0.068163\nTrain Epoch: 5 [39040/60000 (65%)]\tLoss: 0.003660\nTrain Epoch: 5 [39680/60000 (66%)]\tLoss: 0.018614\nTrain Epoch: 5 [40320/60000 (67%)]\tLoss: 0.010991\nTrain Epoch: 5 [40960/60000 (68%)]\tLoss: 0.094300\nTrain Epoch: 5 [41600/60000 (69%)]\tLoss: 0.023087\nTrain Epoch: 5 [42240/60000 (70%)]\tLoss: 0.011758\nTrain Epoch: 5 [42880/60000 (71%)]\tLoss: 0.003261\nTrain Epoch: 5 [43520/60000 (72%)]\tLoss: 0.017048\nTrain Epoch: 5 [44160/60000 (74%)]\tLoss: 0.123491\nTrain Epoch: 5 [44800/60000 (75%)]\tLoss: 0.029624\nTrain Epoch: 5 [45440/60000 (76%)]\tLoss: 0.017441\nTrain Epoch: 5 [46080/60000 (77%)]\tLoss: 0.028510\nTrain Epoch: 5 [46720/60000 (78%)]\tLoss: 0.093132\nTrain Epoch: 5 [47360/60000 (79%)]\tLoss: 0.027666\nTrain Epoch: 5 [48000/60000 (80%)]\tLoss: 0.012826\nTrain Epoch: 5 [48640/60000 (81%)]\tLoss: 0.009660\nTrain Epoch: 5 [49280/60000 (82%)]\tLoss: 0.009987\nTrain Epoch: 5 [49920/60000 (83%)]\tLoss: 0.042045\nTrain Epoch: 5 [50560/60000 (84%)]\tLoss: 0.015197\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.091364\nTrain Epoch: 5 [51840/60000 (86%)]\tLoss: 0.028184\nTrain Epoch: 5 [52480/60000 (87%)]\tLoss: 0.001963\nTrain Epoch: 5 [53120/60000 (88%)]\tLoss: 0.011945\nTrain Epoch: 5 [53760/60000 (90%)]\tLoss: 0.129830\nTrain Epoch: 5 [54400/60000 (91%)]\tLoss: 0.027570\nTrain Epoch: 5 [55040/60000 (92%)]\tLoss: 0.022113\nTrain Epoch: 5 [55680/60000 (93%)]\tLoss: 0.055596\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.001959\nTrain Epoch: 5 [56960/60000 (95%)]\tLoss: 0.004104\nTrain Epoch: 5 [57600/60000 (96%)]\tLoss: 0.008255\nTrain Epoch: 5 [58240/60000 (97%)]\tLoss: 0.001951\nTrain Epoch: 5 [58880/60000 (98%)]\tLoss: 0.000648\nTrain Epoch: 5 [59520/60000 (99%)]\tLoss: 0.000096\n\nTest set: Average loss: 0.0316, Accuracy: 9898/10000 (99%)\n\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 0.006521\nTrain Epoch: 6 [640/60000 (1%)]\tLoss: 0.001722\nTrain Epoch: 6 [1280/60000 (2%)]\tLoss: 0.004176\nTrain Epoch: 6 [1920/60000 (3%)]\tLoss: 0.134405\nTrain Epoch: 6 [2560/60000 (4%)]\tLoss: 0.003887\nTrain Epoch: 6 [3200/60000 (5%)]\tLoss: 0.005606\nTrain Epoch: 6 [3840/60000 (6%)]\tLoss: 0.000385\nTrain Epoch: 6 [4480/60000 (7%)]\tLoss: 0.001570\nTrain Epoch: 6 [5120/60000 (9%)]\tLoss: 0.046688\nTrain Epoch: 6 [5760/60000 (10%)]\tLoss: 0.035598\nTrain Epoch: 6 [6400/60000 (11%)]\tLoss: 0.124485\nTrain Epoch: 6 [7040/60000 (12%)]\tLoss: 0.139262\nTrain Epoch: 6 [7680/60000 (13%)]\tLoss: 0.020779\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.005818\nTrain Epoch: 6 [8960/60000 (15%)]\tLoss: 0.010697\nTrain Epoch: 6 [9600/60000 (16%)]\tLoss: 0.017743\nTrain Epoch: 6 [10240/60000 (17%)]\tLoss: 0.129848\nTrain Epoch: 6 [10880/60000 (18%)]\tLoss: 0.009244\nTrain Epoch: 6 [11520/60000 (19%)]\tLoss: 0.005059\nTrain Epoch: 6 [12160/60000 (20%)]\tLoss: 0.013361\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.016184\nTrain Epoch: 6 [13440/60000 (22%)]\tLoss: 0.001956\nTrain Epoch: 6 [14080/60000 (23%)]\tLoss: 0.010101\nTrain Epoch: 6 [14720/60000 (25%)]\tLoss: 0.118115\nTrain Epoch: 6 [15360/60000 (26%)]\tLoss: 0.005864\nTrain Epoch: 6 [16000/60000 (27%)]\tLoss: 0.046491\nTrain Epoch: 6 [16640/60000 (28%)]\tLoss: 0.097228\nTrain Epoch: 6 [17280/60000 (29%)]\tLoss: 0.001063\nTrain Epoch: 6 [17920/60000 (30%)]\tLoss: 0.016022\nTrain Epoch: 6 [18560/60000 (31%)]\tLoss: 0.061619\nTrain Epoch: 6 [19200/60000 (32%)]\tLoss: 0.004513\nTrain Epoch: 6 [19840/60000 (33%)]\tLoss: 0.077337\nTrain Epoch: 6 [20480/60000 (34%)]\tLoss: 0.018433\nTrain Epoch: 6 [21120/60000 (35%)]\tLoss: 0.100158\nTrain Epoch: 6 [21760/60000 (36%)]\tLoss: 0.005683\nTrain Epoch: 6 [22400/60000 (37%)]\tLoss: 0.002378\nTrain Epoch: 6 [23040/60000 (38%)]\tLoss: 0.078398\nTrain Epoch: 6 [23680/60000 (39%)]\tLoss: 0.042993\nTrain Epoch: 6 [24320/60000 (41%)]\tLoss: 0.009551\nTrain Epoch: 6 [24960/60000 (42%)]\tLoss: 0.002378\nTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001487\nTrain Epoch: 6 [26240/60000 (44%)]\tLoss: 0.013270\nTrain Epoch: 6 [26880/60000 (45%)]\tLoss: 0.055770\nTrain Epoch: 6 [27520/60000 (46%)]\tLoss: 0.134836\nTrain Epoch: 6 [28160/60000 (47%)]\tLoss: 0.037595\nTrain Epoch: 6 [28800/60000 (48%)]\tLoss: 0.004749\nTrain Epoch: 6 [29440/60000 (49%)]\tLoss: 0.004692\nTrain Epoch: 6 [30080/60000 (50%)]\tLoss: 0.004910\nTrain Epoch: 6 [30720/60000 (51%)]\tLoss: 0.003640\nTrain Epoch: 6 [31360/60000 (52%)]\tLoss: 0.044302\nTrain Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006964\nTrain Epoch: 6 [32640/60000 (54%)]\tLoss: 0.065497\nTrain Epoch: 6 [33280/60000 (55%)]\tLoss: 0.024912\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.001976\nTrain Epoch: 6 [34560/60000 (58%)]\tLoss: 0.000835\nTrain Epoch: 6 [35200/60000 (59%)]\tLoss: 0.162731\nTrain Epoch: 6 [35840/60000 (60%)]\tLoss: 0.017803\nTrain Epoch: 6 [36480/60000 (61%)]\tLoss: 0.010016\nTrain Epoch: 6 [37120/60000 (62%)]\tLoss: 0.008382\nTrain Epoch: 6 [37760/60000 (63%)]\tLoss: 0.049732\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 0.035544\nTrain Epoch: 6 [39040/60000 (65%)]\tLoss: 0.000407\nTrain Epoch: 6 [39680/60000 (66%)]\tLoss: 0.023710\nTrain Epoch: 6 [40320/60000 (67%)]\tLoss: 0.046133\nTrain Epoch: 6 [40960/60000 (68%)]\tLoss: 0.073369\nTrain Epoch: 6 [41600/60000 (69%)]\tLoss: 0.018679\nTrain Epoch: 6 [42240/60000 (70%)]\tLoss: 0.015616\nTrain Epoch: 6 [42880/60000 (71%)]\tLoss: 0.007525\nTrain Epoch: 6 [43520/60000 (72%)]\tLoss: 0.049972\nTrain Epoch: 6 [44160/60000 (74%)]\tLoss: 0.013925\nTrain Epoch: 6 [44800/60000 (75%)]\tLoss: 0.014418\nTrain Epoch: 6 [45440/60000 (76%)]\tLoss: 0.029230\nTrain Epoch: 6 [46080/60000 (77%)]\tLoss: 0.046578\nTrain Epoch: 6 [46720/60000 (78%)]\tLoss: 0.080428\nTrain Epoch: 6 [47360/60000 (79%)]\tLoss: 0.070220\nTrain Epoch: 6 [48000/60000 (80%)]\tLoss: 0.004039\nTrain Epoch: 6 [48640/60000 (81%)]\tLoss: 0.017543\nTrain Epoch: 6 [49280/60000 (82%)]\tLoss: 0.026053\nTrain Epoch: 6 [49920/60000 (83%)]\tLoss: 0.012531\nTrain Epoch: 6 [50560/60000 (84%)]\tLoss: 0.023078\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.057610\nTrain Epoch: 6 [51840/60000 (86%)]\tLoss: 0.001125\nTrain Epoch: 6 [52480/60000 (87%)]\tLoss: 0.004046\nTrain Epoch: 6 [53120/60000 (88%)]\tLoss: 0.006521\nTrain Epoch: 6 [53760/60000 (90%)]\tLoss: 0.073194\nTrain Epoch: 6 [54400/60000 (91%)]\tLoss: 0.009555\nTrain Epoch: 6 [55040/60000 (92%)]\tLoss: 0.003563\nTrain Epoch: 6 [55680/60000 (93%)]\tLoss: 0.044483\nTrain Epoch: 6 [56320/60000 (94%)]\tLoss: 0.007985\nTrain Epoch: 6 [56960/60000 (95%)]\tLoss: 0.009336\nTrain Epoch: 6 [57600/60000 (96%)]\tLoss: 0.085358\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.000951\nTrain Epoch: 6 [58880/60000 (98%)]\tLoss: 0.000854\nTrain Epoch: 6 [59520/60000 (99%)]\tLoss: 0.000125\n\nTest set: Average loss: 0.0318, Accuracy: 9898/10000 (99%)\n\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.020314\nTrain Epoch: 7 [640/60000 (1%)]\tLoss: 0.001630\nTrain Epoch: 7 [1280/60000 (2%)]\tLoss: 0.006681\nTrain Epoch: 7 [1920/60000 (3%)]\tLoss: 0.005109\nTrain Epoch: 7 [2560/60000 (4%)]\tLoss: 0.001678\nTrain Epoch: 7 [3200/60000 (5%)]\tLoss: 0.049836\nTrain Epoch: 7 [3840/60000 (6%)]\tLoss: 0.001421\nTrain Epoch: 7 [4480/60000 (7%)]\tLoss: 0.076282\nTrain Epoch: 7 [5120/60000 (9%)]\tLoss: 0.009371\nTrain Epoch: 7 [5760/60000 (10%)]\tLoss: 0.030672\nTrain Epoch: 7 [6400/60000 (11%)]\tLoss: 0.069098\nTrain Epoch: 7 [7040/60000 (12%)]\tLoss: 0.144593\nTrain Epoch: 7 [7680/60000 (13%)]\tLoss: 0.010357\nTrain Epoch: 7 [8320/60000 (14%)]\tLoss: 0.004213\nTrain Epoch: 7 [8960/60000 (15%)]\tLoss: 0.027069\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.005852\nTrain Epoch: 7 [10240/60000 (17%)]\tLoss: 0.113450\nTrain Epoch: 7 [10880/60000 (18%)]\tLoss: 0.001453\nTrain Epoch: 7 [11520/60000 (19%)]\tLoss: 0.006323\nTrain Epoch: 7 [12160/60000 (20%)]\tLoss: 0.026364\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.047649\nTrain Epoch: 7 [13440/60000 (22%)]\tLoss: 0.019212\nTrain Epoch: 7 [14080/60000 (23%)]\tLoss: 0.018120\nTrain Epoch: 7 [14720/60000 (25%)]\tLoss: 0.071631\nTrain Epoch: 7 [15360/60000 (26%)]\tLoss: 0.002499\nTrain Epoch: 7 [16000/60000 (27%)]\tLoss: 0.084108\nTrain Epoch: 7 [16640/60000 (28%)]\tLoss: 0.041769\nTrain Epoch: 7 [17280/60000 (29%)]\tLoss: 0.000278\nTrain Epoch: 7 [17920/60000 (30%)]\tLoss: 0.029181\nTrain Epoch: 7 [18560/60000 (31%)]\tLoss: 0.006735\nTrain Epoch: 7 [19200/60000 (32%)]\tLoss: 0.054518\nTrain Epoch: 7 [19840/60000 (33%)]\tLoss: 0.038299\nTrain Epoch: 7 [20480/60000 (34%)]\tLoss: 0.007866\nTrain Epoch: 7 [21120/60000 (35%)]\tLoss: 0.025332\nTrain Epoch: 7 [21760/60000 (36%)]\tLoss: 0.001993\nTrain Epoch: 7 [22400/60000 (37%)]\tLoss: 0.000879\nTrain Epoch: 7 [23040/60000 (38%)]\tLoss: 0.066880\nTrain Epoch: 7 [23680/60000 (39%)]\tLoss: 0.057380\nTrain Epoch: 7 [24320/60000 (41%)]\tLoss: 0.000719\nTrain Epoch: 7 [24960/60000 (42%)]\tLoss: 0.001511\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.004281\nTrain Epoch: 7 [26240/60000 (44%)]\tLoss: 0.007450\nTrain Epoch: 7 [26880/60000 (45%)]\tLoss: 0.008884\nTrain Epoch: 7 [27520/60000 (46%)]\tLoss: 0.058467\nTrain Epoch: 7 [28160/60000 (47%)]\tLoss: 0.023451\nTrain Epoch: 7 [28800/60000 (48%)]\tLoss: 0.012239\nTrain Epoch: 7 [29440/60000 (49%)]\tLoss: 0.013680\nTrain Epoch: 7 [30080/60000 (50%)]\tLoss: 0.036608\nTrain Epoch: 7 [30720/60000 (51%)]\tLoss: 0.010993\nTrain Epoch: 7 [31360/60000 (52%)]\tLoss: 0.011943\nTrain Epoch: 7 [32000/60000 (53%)]\tLoss: 0.041241\nTrain Epoch: 7 [32640/60000 (54%)]\tLoss: 0.013988\nTrain Epoch: 7 [33280/60000 (55%)]\tLoss: 0.006011\nTrain Epoch: 7 [33920/60000 (57%)]\tLoss: 0.017526\nTrain Epoch: 7 [34560/60000 (58%)]\tLoss: 0.015772\nTrain Epoch: 7 [35200/60000 (59%)]\tLoss: 0.060769\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.081311\nTrain Epoch: 7 [36480/60000 (61%)]\tLoss: 0.001407\nTrain Epoch: 7 [37120/60000 (62%)]\tLoss: 0.008014\nTrain Epoch: 7 [37760/60000 (63%)]\tLoss: 0.016569\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 0.059136\nTrain Epoch: 7 [39040/60000 (65%)]\tLoss: 0.001025\nTrain Epoch: 7 [39680/60000 (66%)]\tLoss: 0.017827\nTrain Epoch: 7 [40320/60000 (67%)]\tLoss: 0.059819\nTrain Epoch: 7 [40960/60000 (68%)]\tLoss: 0.004870\nTrain Epoch: 7 [41600/60000 (69%)]\tLoss: 0.027897\nTrain Epoch: 7 [42240/60000 (70%)]\tLoss: 0.042376\nTrain Epoch: 7 [42880/60000 (71%)]\tLoss: 0.003144\nTrain Epoch: 7 [43520/60000 (72%)]\tLoss: 0.019763\nTrain Epoch: 7 [44160/60000 (74%)]\tLoss: 0.005481\nTrain Epoch: 7 [44800/60000 (75%)]\tLoss: 0.020775\nTrain Epoch: 7 [45440/60000 (76%)]\tLoss: 0.083557\nTrain Epoch: 7 [46080/60000 (77%)]\tLoss: 0.012784\nTrain Epoch: 7 [46720/60000 (78%)]\tLoss: 0.047670\nTrain Epoch: 7 [47360/60000 (79%)]\tLoss: 0.008587\nTrain Epoch: 7 [48000/60000 (80%)]\tLoss: 0.022388\nTrain Epoch: 7 [48640/60000 (81%)]\tLoss: 0.001703\nTrain Epoch: 7 [49280/60000 (82%)]\tLoss: 0.011862\nTrain Epoch: 7 [49920/60000 (83%)]\tLoss: 0.010263\nTrain Epoch: 7 [50560/60000 (84%)]\tLoss: 0.023681\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.187673\nTrain Epoch: 7 [51840/60000 (86%)]\tLoss: 0.023777\nTrain Epoch: 7 [52480/60000 (87%)]\tLoss: 0.001469\nTrain Epoch: 7 [53120/60000 (88%)]\tLoss: 0.018244\nTrain Epoch: 7 [53760/60000 (90%)]\tLoss: 0.023056\nTrain Epoch: 7 [54400/60000 (91%)]\tLoss: 0.006857\nTrain Epoch: 7 [55040/60000 (92%)]\tLoss: 0.001386\nTrain Epoch: 7 [55680/60000 (93%)]\tLoss: 0.074469\nTrain Epoch: 7 [56320/60000 (94%)]\tLoss: 0.007396\nTrain Epoch: 7 [56960/60000 (95%)]\tLoss: 0.000944\nTrain Epoch: 7 [57600/60000 (96%)]\tLoss: 0.006526\nTrain Epoch: 7 [58240/60000 (97%)]\tLoss: 0.000487\nTrain Epoch: 7 [58880/60000 (98%)]\tLoss: 0.006872\nTrain Epoch: 7 [59520/60000 (99%)]\tLoss: 0.008411\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nTest set: Average loss: 0.0304, Accuracy: 9907/10000 (99%)\n\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.004063\nTrain Epoch: 8 [640/60000 (1%)]\tLoss: 0.001479\nTrain Epoch: 8 [1280/60000 (2%)]\tLoss: 0.019700\nTrain Epoch: 8 [1920/60000 (3%)]\tLoss: 0.126736\nTrain Epoch: 8 [2560/60000 (4%)]\tLoss: 0.016570\nTrain Epoch: 8 [3200/60000 (5%)]\tLoss: 0.056089\nTrain Epoch: 8 [3840/60000 (6%)]\tLoss: 0.001782\nTrain Epoch: 8 [4480/60000 (7%)]\tLoss: 0.001029\nTrain Epoch: 8 [5120/60000 (9%)]\tLoss: 0.012595\nTrain Epoch: 8 [5760/60000 (10%)]\tLoss: 0.082368\nTrain Epoch: 8 [6400/60000 (11%)]\tLoss: 0.106580\nTrain Epoch: 8 [7040/60000 (12%)]\tLoss: 0.079309\nTrain Epoch: 8 [7680/60000 (13%)]\tLoss: 0.003038\nTrain Epoch: 8 [8320/60000 (14%)]\tLoss: 0.000525\nTrain Epoch: 8 [8960/60000 (15%)]\tLoss: 0.032746\nTrain Epoch: 8 [9600/60000 (16%)]\tLoss: 0.005388\nTrain Epoch: 8 [10240/60000 (17%)]\tLoss: 0.082380\nTrain Epoch: 8 [10880/60000 (18%)]\tLoss: 0.002736\nTrain Epoch: 8 [11520/60000 (19%)]\tLoss: 0.018367\nTrain Epoch: 8 [12160/60000 (20%)]\tLoss: 0.046208\nTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 0.036942\nTrain Epoch: 8 [13440/60000 (22%)]\tLoss: 0.000923\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.001747\nTrain Epoch: 8 [14720/60000 (25%)]\tLoss: 0.050172\nTrain Epoch: 8 [15360/60000 (26%)]\tLoss: 0.001966\nTrain Epoch: 8 [16000/60000 (27%)]\tLoss: 0.012371\nTrain Epoch: 8 [16640/60000 (28%)]\tLoss: 0.076443\nTrain Epoch: 8 [17280/60000 (29%)]\tLoss: 0.000581\nTrain Epoch: 8 [17920/60000 (30%)]\tLoss: 0.025203\nTrain Epoch: 8 [18560/60000 (31%)]\tLoss: 0.031874\nTrain Epoch: 8 [19200/60000 (32%)]\tLoss: 0.008874\nTrain Epoch: 8 [19840/60000 (33%)]\tLoss: 0.031275\nTrain Epoch: 8 [20480/60000 (34%)]\tLoss: 0.001979\nTrain Epoch: 8 [21120/60000 (35%)]\tLoss: 0.034780\nTrain Epoch: 8 [21760/60000 (36%)]\tLoss: 0.001828\nTrain Epoch: 8 [22400/60000 (37%)]\tLoss: 0.007505\nTrain Epoch: 8 [23040/60000 (38%)]\tLoss: 0.096978\nTrain Epoch: 8 [23680/60000 (39%)]\tLoss: 0.006166\nTrain Epoch: 8 [24320/60000 (41%)]\tLoss: 0.000387\nTrain Epoch: 8 [24960/60000 (42%)]\tLoss: 0.000761\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 0.091099\nTrain Epoch: 8 [26240/60000 (44%)]\tLoss: 0.007009\nTrain Epoch: 8 [26880/60000 (45%)]\tLoss: 0.051381\nTrain Epoch: 8 [27520/60000 (46%)]\tLoss: 0.013620\nTrain Epoch: 8 [28160/60000 (47%)]\tLoss: 0.060092\nTrain Epoch: 8 [28800/60000 (48%)]\tLoss: 0.022676\nTrain Epoch: 8 [29440/60000 (49%)]\tLoss: 0.009761\nTrain Epoch: 8 [30080/60000 (50%)]\tLoss: 0.010682\nTrain Epoch: 8 [30720/60000 (51%)]\tLoss: 0.056669\nTrain Epoch: 8 [31360/60000 (52%)]\tLoss: 0.006102\nTrain Epoch: 8 [32000/60000 (53%)]\tLoss: 0.025806\nTrain Epoch: 8 [32640/60000 (54%)]\tLoss: 0.001735\nTrain Epoch: 8 [33280/60000 (55%)]\tLoss: 0.057443\nTrain Epoch: 8 [33920/60000 (57%)]\tLoss: 0.001165\nTrain Epoch: 8 [34560/60000 (58%)]\tLoss: 0.015387\nTrain Epoch: 8 [35200/60000 (59%)]\tLoss: 0.056614\nTrain Epoch: 8 [35840/60000 (60%)]\tLoss: 0.116908\nTrain Epoch: 8 [36480/60000 (61%)]\tLoss: 0.008160\nTrain Epoch: 8 [37120/60000 (62%)]\tLoss: 0.019704\nTrain Epoch: 8 [37760/60000 (63%)]\tLoss: 0.021773\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.020542\nTrain Epoch: 8 [39040/60000 (65%)]\tLoss: 0.003437\nTrain Epoch: 8 [39680/60000 (66%)]\tLoss: 0.022369\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.009877\nTrain Epoch: 8 [40960/60000 (68%)]\tLoss: 0.029416\nTrain Epoch: 8 [41600/60000 (69%)]\tLoss: 0.034469\nTrain Epoch: 8 [42240/60000 (70%)]\tLoss: 0.003994\nTrain Epoch: 8 [42880/60000 (71%)]\tLoss: 0.000302\nTrain Epoch: 8 [43520/60000 (72%)]\tLoss: 0.018143\nTrain Epoch: 8 [44160/60000 (74%)]\tLoss: 0.003671\nTrain Epoch: 8 [44800/60000 (75%)]\tLoss: 0.028928\nTrain Epoch: 8 [45440/60000 (76%)]\tLoss: 0.023253\nTrain Epoch: 8 [46080/60000 (77%)]\tLoss: 0.021851\nTrain Epoch: 8 [46720/60000 (78%)]\tLoss: 0.038021\nTrain Epoch: 8 [47360/60000 (79%)]\tLoss: 0.028606\nTrain Epoch: 8 [48000/60000 (80%)]\tLoss: 0.004224\nTrain Epoch: 8 [48640/60000 (81%)]\tLoss: 0.011960\nTrain Epoch: 8 [49280/60000 (82%)]\tLoss: 0.011032\nTrain Epoch: 8 [49920/60000 (83%)]\tLoss: 0.013814\nTrain Epoch: 8 [50560/60000 (84%)]\tLoss: 0.018310\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.178460\nTrain Epoch: 8 [51840/60000 (86%)]\tLoss: 0.013559\nTrain Epoch: 8 [52480/60000 (87%)]\tLoss: 0.001097\nTrain Epoch: 8 [53120/60000 (88%)]\tLoss: 0.052585\nTrain Epoch: 8 [53760/60000 (90%)]\tLoss: 0.096986\nTrain Epoch: 8 [54400/60000 (91%)]\tLoss: 0.041562\nTrain Epoch: 8 [55040/60000 (92%)]\tLoss: 0.001196\nTrain Epoch: 8 [55680/60000 (93%)]\tLoss: 0.009348\nTrain Epoch: 8 [56320/60000 (94%)]\tLoss: 0.014647\nTrain Epoch: 8 [56960/60000 (95%)]\tLoss: 0.001175\nTrain Epoch: 8 [57600/60000 (96%)]\tLoss: 0.047912\nTrain Epoch: 8 [58240/60000 (97%)]\tLoss: 0.005807\nTrain Epoch: 8 [58880/60000 (98%)]\tLoss: 0.025312\nTrain Epoch: 8 [59520/60000 (99%)]\tLoss: 0.000415\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nTest set: Average loss: 0.0304, Accuracy: 9909/10000 (99%)\n\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.016340\nTrain Epoch: 9 [640/60000 (1%)]\tLoss: 0.003754\nTrain Epoch: 9 [1280/60000 (2%)]\tLoss: 0.016004\nTrain Epoch: 9 [1920/60000 (3%)]\tLoss: 0.067676\nTrain Epoch: 9 [2560/60000 (4%)]\tLoss: 0.003083\nTrain Epoch: 9 [3200/60000 (5%)]\tLoss: 0.004060\nTrain Epoch: 9 [3840/60000 (6%)]\tLoss: 0.000459\nTrain Epoch: 9 [4480/60000 (7%)]\tLoss: 0.002810\nTrain Epoch: 9 [5120/60000 (9%)]\tLoss: 0.008098\nTrain Epoch: 9 [5760/60000 (10%)]\tLoss: 0.053083\nTrain Epoch: 9 [6400/60000 (11%)]\tLoss: 0.059711\nTrain Epoch: 9 [7040/60000 (12%)]\tLoss: 0.190745\nTrain Epoch: 9 [7680/60000 (13%)]\tLoss: 0.035529\nTrain Epoch: 9 [8320/60000 (14%)]\tLoss: 0.000474\nTrain Epoch: 9 [8960/60000 (15%)]\tLoss: 0.004617\nTrain Epoch: 9 [9600/60000 (16%)]\tLoss: 0.001136\nTrain Epoch: 9 [10240/60000 (17%)]\tLoss: 0.062059\nTrain Epoch: 9 [10880/60000 (18%)]\tLoss: 0.003114\nTrain Epoch: 9 [11520/60000 (19%)]\tLoss: 0.059257\nTrain Epoch: 9 [12160/60000 (20%)]\tLoss: 0.021972\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 0.029488\nTrain Epoch: 9 [13440/60000 (22%)]\tLoss: 0.000305\nTrain Epoch: 9 [14080/60000 (23%)]\tLoss: 0.001619\nTrain Epoch: 9 [14720/60000 (25%)]\tLoss: 0.019347\nTrain Epoch: 9 [15360/60000 (26%)]\tLoss: 0.001676\nTrain Epoch: 9 [16000/60000 (27%)]\tLoss: 0.004385\nTrain Epoch: 9 [16640/60000 (28%)]\tLoss: 0.087467\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.000201\nTrain Epoch: 9 [17920/60000 (30%)]\tLoss: 0.060596\nTrain Epoch: 9 [18560/60000 (31%)]\tLoss: 0.000711\nTrain Epoch: 9 [19200/60000 (32%)]\tLoss: 0.053916\nTrain Epoch: 9 [19840/60000 (33%)]\tLoss: 0.009719\nTrain Epoch: 9 [20480/60000 (34%)]\tLoss: 0.005698\nTrain Epoch: 9 [21120/60000 (35%)]\tLoss: 0.028015\nTrain Epoch: 9 [21760/60000 (36%)]\tLoss: 0.002298\nTrain Epoch: 9 [22400/60000 (37%)]\tLoss: 0.029313\nTrain Epoch: 9 [23040/60000 (38%)]\tLoss: 0.010569\nTrain Epoch: 9 [23680/60000 (39%)]\tLoss: 0.015233\nTrain Epoch: 9 [24320/60000 (41%)]\tLoss: 0.002462\nTrain Epoch: 9 [24960/60000 (42%)]\tLoss: 0.003073\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.009034\nTrain Epoch: 9 [26240/60000 (44%)]\tLoss: 0.012971\nTrain Epoch: 9 [26880/60000 (45%)]\tLoss: 0.026330\nTrain Epoch: 9 [27520/60000 (46%)]\tLoss: 0.047364\nTrain Epoch: 9 [28160/60000 (47%)]\tLoss: 0.003557\nTrain Epoch: 9 [28800/60000 (48%)]\tLoss: 0.009606\nTrain Epoch: 9 [29440/60000 (49%)]\tLoss: 0.006400\nTrain Epoch: 9 [30080/60000 (50%)]\tLoss: 0.016552\nTrain Epoch: 9 [30720/60000 (51%)]\tLoss: 0.003154\nTrain Epoch: 9 [31360/60000 (52%)]\tLoss: 0.024171\nTrain Epoch: 9 [32000/60000 (53%)]\tLoss: 0.069285\nTrain Epoch: 9 [32640/60000 (54%)]\tLoss: 0.002765\nTrain Epoch: 9 [33280/60000 (55%)]\tLoss: 0.007869\nTrain Epoch: 9 [33920/60000 (57%)]\tLoss: 0.010112\nTrain Epoch: 9 [34560/60000 (58%)]\tLoss: 0.001299\nTrain Epoch: 9 [35200/60000 (59%)]\tLoss: 0.123325\nTrain Epoch: 9 [35840/60000 (60%)]\tLoss: 0.005538\nTrain Epoch: 9 [36480/60000 (61%)]\tLoss: 0.004393\nTrain Epoch: 9 [37120/60000 (62%)]\tLoss: 0.002159\nTrain Epoch: 9 [37760/60000 (63%)]\tLoss: 0.003950\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.048948\nTrain Epoch: 9 [39040/60000 (65%)]\tLoss: 0.000435\nTrain Epoch: 9 [39680/60000 (66%)]\tLoss: 0.025611\nTrain Epoch: 9 [40320/60000 (67%)]\tLoss: 0.004054\nTrain Epoch: 9 [40960/60000 (68%)]\tLoss: 0.018813\nTrain Epoch: 9 [41600/60000 (69%)]\tLoss: 0.037193\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.036610\nTrain Epoch: 9 [42880/60000 (71%)]\tLoss: 0.004125\nTrain Epoch: 9 [43520/60000 (72%)]\tLoss: 0.046355\nTrain Epoch: 9 [44160/60000 (74%)]\tLoss: 0.002121\nTrain Epoch: 9 [44800/60000 (75%)]\tLoss: 0.016921\nTrain Epoch: 9 [45440/60000 (76%)]\tLoss: 0.026151\nTrain Epoch: 9 [46080/60000 (77%)]\tLoss: 0.028508\nTrain Epoch: 9 [46720/60000 (78%)]\tLoss: 0.041300\nTrain Epoch: 9 [47360/60000 (79%)]\tLoss: 0.025121\nTrain Epoch: 9 [48000/60000 (80%)]\tLoss: 0.009761\nTrain Epoch: 9 [48640/60000 (81%)]\tLoss: 0.002168\nTrain Epoch: 9 [49280/60000 (82%)]\tLoss: 0.002589\nTrain Epoch: 9 [49920/60000 (83%)]\tLoss: 0.005593\nTrain Epoch: 9 [50560/60000 (84%)]\tLoss: 0.052310\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.086427\nTrain Epoch: 9 [51840/60000 (86%)]\tLoss: 0.107612\nTrain Epoch: 9 [52480/60000 (87%)]\tLoss: 0.000798\nTrain Epoch: 9 [53120/60000 (88%)]\tLoss: 0.001643\nTrain Epoch: 9 [53760/60000 (90%)]\tLoss: 0.026258\nTrain Epoch: 9 [54400/60000 (91%)]\tLoss: 0.012588\nTrain Epoch: 9 [55040/60000 (92%)]\tLoss: 0.003325\nTrain Epoch: 9 [55680/60000 (93%)]\tLoss: 0.013419\nTrain Epoch: 9 [56320/60000 (94%)]\tLoss: 0.004575\nTrain Epoch: 9 [56960/60000 (95%)]\tLoss: 0.008604\nTrain Epoch: 9 [57600/60000 (96%)]\tLoss: 0.025239\nTrain Epoch: 9 [58240/60000 (97%)]\tLoss: 0.000181\nTrain Epoch: 9 [58880/60000 (98%)]\tLoss: 0.002617\nTrain Epoch: 9 [59520/60000 (99%)]\tLoss: 0.000116\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nTest set: Average loss: 0.0303, Accuracy: 9905/10000 (99%)\n\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.015041\nTrain Epoch: 10 [640/60000 (1%)]\tLoss: 0.002217\nTrain Epoch: 10 [1280/60000 (2%)]\tLoss: 0.016372\nTrain Epoch: 10 [1920/60000 (3%)]\tLoss: 0.085451\nTrain Epoch: 10 [2560/60000 (4%)]\tLoss: 0.005600\nTrain Epoch: 10 [3200/60000 (5%)]\tLoss: 0.010120\nTrain Epoch: 10 [3840/60000 (6%)]\tLoss: 0.000380\nTrain Epoch: 10 [4480/60000 (7%)]\tLoss: 0.004861\nTrain Epoch: 10 [5120/60000 (9%)]\tLoss: 0.012943\nTrain Epoch: 10 [5760/60000 (10%)]\tLoss: 0.011710\nTrain Epoch: 10 [6400/60000 (11%)]\tLoss: 0.091843\nTrain Epoch: 10 [7040/60000 (12%)]\tLoss: 0.094336\nTrain Epoch: 10 [7680/60000 (13%)]\tLoss: 0.006765\nTrain Epoch: 10 [8320/60000 (14%)]\tLoss: 0.001023\nTrain Epoch: 10 [8960/60000 (15%)]\tLoss: 0.005691\nTrain Epoch: 10 [9600/60000 (16%)]\tLoss: 0.015316\nTrain Epoch: 10 [10240/60000 (17%)]\tLoss: 0.035851\nTrain Epoch: 10 [10880/60000 (18%)]\tLoss: 0.009459\nTrain Epoch: 10 [11520/60000 (19%)]\tLoss: 0.018002\nTrain Epoch: 10 [12160/60000 (20%)]\tLoss: 0.033699\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 0.014278\nTrain Epoch: 10 [13440/60000 (22%)]\tLoss: 0.002101\nTrain Epoch: 10 [14080/60000 (23%)]\tLoss: 0.002308\nTrain Epoch: 10 [14720/60000 (25%)]\tLoss: 0.017746\nTrain Epoch: 10 [15360/60000 (26%)]\tLoss: 0.003059\nTrain Epoch: 10 [16000/60000 (27%)]\tLoss: 0.014498\nTrain Epoch: 10 [16640/60000 (28%)]\tLoss: 0.019726\nTrain Epoch: 10 [17280/60000 (29%)]\tLoss: 0.006204\nTrain Epoch: 10 [17920/60000 (30%)]\tLoss: 0.017501\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.036423\nTrain Epoch: 10 [19200/60000 (32%)]\tLoss: 0.048185\nTrain Epoch: 10 [19840/60000 (33%)]\tLoss: 0.033190\nTrain Epoch: 10 [20480/60000 (34%)]\tLoss: 0.002150\nTrain Epoch: 10 [21120/60000 (35%)]\tLoss: 0.030285\nTrain Epoch: 10 [21760/60000 (36%)]\tLoss: 0.001983\nTrain Epoch: 10 [22400/60000 (37%)]\tLoss: 0.022611\nTrain Epoch: 10 [23040/60000 (38%)]\tLoss: 0.033913\nTrain Epoch: 10 [23680/60000 (39%)]\tLoss: 0.023643\nTrain Epoch: 10 [24320/60000 (41%)]\tLoss: 0.003661\nTrain Epoch: 10 [24960/60000 (42%)]\tLoss: 0.006175\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002232\nTrain Epoch: 10 [26240/60000 (44%)]\tLoss: 0.013477\nTrain Epoch: 10 [26880/60000 (45%)]\tLoss: 0.018427\nTrain Epoch: 10 [27520/60000 (46%)]\tLoss: 0.041685\nTrain Epoch: 10 [28160/60000 (47%)]\tLoss: 0.013825\nTrain Epoch: 10 [28800/60000 (48%)]\tLoss: 0.012541\nTrain Epoch: 10 [29440/60000 (49%)]\tLoss: 0.020207\nTrain Epoch: 10 [30080/60000 (50%)]\tLoss: 0.034366\nTrain Epoch: 10 [30720/60000 (51%)]\tLoss: 0.005808\nTrain Epoch: 10 [31360/60000 (52%)]\tLoss: 0.031512\nTrain Epoch: 10 [32000/60000 (53%)]\tLoss: 0.038589\nTrain Epoch: 10 [32640/60000 (54%)]\tLoss: 0.020586\nTrain Epoch: 10 [33280/60000 (55%)]\tLoss: 0.006387\nTrain Epoch: 10 [33920/60000 (57%)]\tLoss: 0.000781\nTrain Epoch: 10 [34560/60000 (58%)]\tLoss: 0.028357\nTrain Epoch: 10 [35200/60000 (59%)]\tLoss: 0.101689\nTrain Epoch: 10 [35840/60000 (60%)]\tLoss: 0.094884\nTrain Epoch: 10 [36480/60000 (61%)]\tLoss: 0.004880\nTrain Epoch: 10 [37120/60000 (62%)]\tLoss: 0.004891\nTrain Epoch: 10 [37760/60000 (63%)]\tLoss: 0.048501\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.064288\nTrain Epoch: 10 [39040/60000 (65%)]\tLoss: 0.001795\nTrain Epoch: 10 [39680/60000 (66%)]\tLoss: 0.022089\nTrain Epoch: 10 [40320/60000 (67%)]\tLoss: 0.005794\nTrain Epoch: 10 [40960/60000 (68%)]\tLoss: 0.041167\nTrain Epoch: 10 [41600/60000 (69%)]\tLoss: 0.004554\nTrain Epoch: 10 [42240/60000 (70%)]\tLoss: 0.001433\nTrain Epoch: 10 [42880/60000 (71%)]\tLoss: 0.003200\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.006352\nTrain Epoch: 10 [44160/60000 (74%)]\tLoss: 0.005062\nTrain Epoch: 10 [44800/60000 (75%)]\tLoss: 0.003515\nTrain Epoch: 10 [45440/60000 (76%)]\tLoss: 0.004774\nTrain Epoch: 10 [46080/60000 (77%)]\tLoss: 0.033402\nTrain Epoch: 10 [46720/60000 (78%)]\tLoss: 0.049754\nTrain Epoch: 10 [47360/60000 (79%)]\tLoss: 0.025877\nTrain Epoch: 10 [48000/60000 (80%)]\tLoss: 0.060993\nTrain Epoch: 10 [48640/60000 (81%)]\tLoss: 0.018543\nTrain Epoch: 10 [49280/60000 (82%)]\tLoss: 0.001351\nTrain Epoch: 10 [49920/60000 (83%)]\tLoss: 0.051879\nTrain Epoch: 10 [50560/60000 (84%)]\tLoss: 0.027974\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.119608\nTrain Epoch: 10 [51840/60000 (86%)]\tLoss: 0.001472\nTrain Epoch: 10 [52480/60000 (87%)]\tLoss: 0.007494\nTrain Epoch: 10 [53120/60000 (88%)]\tLoss: 0.012106\nTrain Epoch: 10 [53760/60000 (90%)]\tLoss: 0.098425\nTrain Epoch: 10 [54400/60000 (91%)]\tLoss: 0.002462\nTrain Epoch: 10 [55040/60000 (92%)]\tLoss: 0.000957\nTrain Epoch: 10 [55680/60000 (93%)]\tLoss: 0.043708\nTrain Epoch: 10 [56320/60000 (94%)]\tLoss: 0.002220\nTrain Epoch: 10 [56960/60000 (95%)]\tLoss: 0.001852\nTrain Epoch: 10 [57600/60000 (96%)]\tLoss: 0.053531\nTrain Epoch: 10 [58240/60000 (97%)]\tLoss: 0.017780\nTrain Epoch: 10 [58880/60000 (98%)]\tLoss: 0.000064\nTrain Epoch: 10 [59520/60000 (99%)]\tLoss: 0.000284\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nTest set: Average loss: 0.0295, Accuracy: 9908/10000 (99%)\n\nTrain Epoch: 11 [0/60000 (0%)]\tLoss: 0.000960\nTrain Epoch: 11 [640/60000 (1%)]\tLoss: 0.004648\nTrain Epoch: 11 [1280/60000 (2%)]\tLoss: 0.023096\nTrain Epoch: 11 [1920/60000 (3%)]\tLoss: 0.003452\nTrain Epoch: 11 [2560/60000 (4%)]\tLoss: 0.004179\nTrain Epoch: 11 [3200/60000 (5%)]\tLoss: 0.002803\nTrain Epoch: 11 [3840/60000 (6%)]\tLoss: 0.000226\nTrain Epoch: 11 [4480/60000 (7%)]\tLoss: 0.004216\nTrain Epoch: 11 [5120/60000 (9%)]\tLoss: 0.062512\nTrain Epoch: 11 [5760/60000 (10%)]\tLoss: 0.014293\nTrain Epoch: 11 [6400/60000 (11%)]\tLoss: 0.207131\nTrain Epoch: 11 [7040/60000 (12%)]\tLoss: 0.144313\nTrain Epoch: 11 [7680/60000 (13%)]\tLoss: 0.032846\nTrain Epoch: 11 [8320/60000 (14%)]\tLoss: 0.002571\nTrain Epoch: 11 [8960/60000 (15%)]\tLoss: 0.019241\nTrain Epoch: 11 [9600/60000 (16%)]\tLoss: 0.001535\nTrain Epoch: 11 [10240/60000 (17%)]\tLoss: 0.067222\nTrain Epoch: 11 [10880/60000 (18%)]\tLoss: 0.009303\nTrain Epoch: 11 [11520/60000 (19%)]\tLoss: 0.012442\nTrain Epoch: 11 [12160/60000 (20%)]\tLoss: 0.003280\nTrain Epoch: 11 [12800/60000 (21%)]\tLoss: 0.010792\nTrain Epoch: 11 [13440/60000 (22%)]\tLoss: 0.000313\nTrain Epoch: 11 [14080/60000 (23%)]\tLoss: 0.000830\nTrain Epoch: 11 [14720/60000 (25%)]\tLoss: 0.042270\nTrain Epoch: 11 [15360/60000 (26%)]\tLoss: 0.004422\nTrain Epoch: 11 [16000/60000 (27%)]\tLoss: 0.034213\nTrain Epoch: 11 [16640/60000 (28%)]\tLoss: 0.093602\nTrain Epoch: 11 [17280/60000 (29%)]\tLoss: 0.000399\nTrain Epoch: 11 [17920/60000 (30%)]\tLoss: 0.032287\nTrain Epoch: 11 [18560/60000 (31%)]\tLoss: 0.013112\nTrain Epoch: 11 [19200/60000 (32%)]\tLoss: 0.057097\nTrain Epoch: 11 [19840/60000 (33%)]\tLoss: 0.025357\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.001413\nTrain Epoch: 11 [21120/60000 (35%)]\tLoss: 0.048082\nTrain Epoch: 11 [21760/60000 (36%)]\tLoss: 0.003182\nTrain Epoch: 11 [22400/60000 (37%)]\tLoss: 0.057902\nTrain Epoch: 11 [23040/60000 (38%)]\tLoss: 0.046270\nTrain Epoch: 11 [23680/60000 (39%)]\tLoss: 0.025155\nTrain Epoch: 11 [24320/60000 (41%)]\tLoss: 0.002480\nTrain Epoch: 11 [24960/60000 (42%)]\tLoss: 0.002284\nTrain Epoch: 11 [25600/60000 (43%)]\tLoss: 0.001182\nTrain Epoch: 11 [26240/60000 (44%)]\tLoss: 0.002983\nTrain Epoch: 11 [26880/60000 (45%)]\tLoss: 0.007286\nTrain Epoch: 11 [27520/60000 (46%)]\tLoss: 0.084341\nTrain Epoch: 11 [28160/60000 (47%)]\tLoss: 0.028436\nTrain Epoch: 11 [28800/60000 (48%)]\tLoss: 0.007681\nTrain Epoch: 11 [29440/60000 (49%)]\tLoss: 0.010584\nTrain Epoch: 11 [30080/60000 (50%)]\tLoss: 0.021826\nTrain Epoch: 11 [30720/60000 (51%)]\tLoss: 0.028049\nTrain Epoch: 11 [31360/60000 (52%)]\tLoss: 0.060550\nTrain Epoch: 11 [32000/60000 (53%)]\tLoss: 0.032492\nTrain Epoch: 11 [32640/60000 (54%)]\tLoss: 0.016343\nTrain Epoch: 11 [33280/60000 (55%)]\tLoss: 0.010139\nTrain Epoch: 11 [33920/60000 (57%)]\tLoss: 0.000651\nTrain Epoch: 11 [34560/60000 (58%)]\tLoss: 0.003476\nTrain Epoch: 11 [35200/60000 (59%)]\tLoss: 0.076233\nTrain Epoch: 11 [35840/60000 (60%)]\tLoss: 0.016992\nTrain Epoch: 11 [36480/60000 (61%)]\tLoss: 0.001389\nTrain Epoch: 11 [37120/60000 (62%)]\tLoss: 0.011678\nTrain Epoch: 11 [37760/60000 (63%)]\tLoss: 0.006328\nTrain Epoch: 11 [38400/60000 (64%)]\tLoss: 0.068951\nTrain Epoch: 11 [39040/60000 (65%)]\tLoss: 0.000879\nTrain Epoch: 11 [39680/60000 (66%)]\tLoss: 0.017106\nTrain Epoch: 11 [40320/60000 (67%)]\tLoss: 0.006365\nTrain Epoch: 11 [40960/60000 (68%)]\tLoss: 0.040273\nTrain Epoch: 11 [41600/60000 (69%)]\tLoss: 0.016161\nTrain Epoch: 11 [42240/60000 (70%)]\tLoss: 0.004739\nTrain Epoch: 11 [42880/60000 (71%)]\tLoss: 0.000707\nTrain Epoch: 11 [43520/60000 (72%)]\tLoss: 0.010637\nTrain Epoch: 11 [44160/60000 (74%)]\tLoss: 0.002757\nTrain Epoch: 11 [44800/60000 (75%)]\tLoss: 0.008177\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.005843\nTrain Epoch: 11 [46080/60000 (77%)]\tLoss: 0.020663\nTrain Epoch: 11 [46720/60000 (78%)]\tLoss: 0.036596\nTrain Epoch: 11 [47360/60000 (79%)]\tLoss: 0.054589\nTrain Epoch: 11 [48000/60000 (80%)]\tLoss: 0.041059\nTrain Epoch: 11 [48640/60000 (81%)]\tLoss: 0.004891\nTrain Epoch: 11 [49280/60000 (82%)]\tLoss: 0.001543\nTrain Epoch: 11 [49920/60000 (83%)]\tLoss: 0.002356\nTrain Epoch: 11 [50560/60000 (84%)]\tLoss: 0.027077\nTrain Epoch: 11 [51200/60000 (85%)]\tLoss: 0.125219\nTrain Epoch: 11 [51840/60000 (86%)]\tLoss: 0.013355\nTrain Epoch: 11 [52480/60000 (87%)]\tLoss: 0.005234\nTrain Epoch: 11 [53120/60000 (88%)]\tLoss: 0.006281\nTrain Epoch: 11 [53760/60000 (90%)]\tLoss: 0.103300\nTrain Epoch: 11 [54400/60000 (91%)]\tLoss: 0.016936\nTrain Epoch: 11 [55040/60000 (92%)]\tLoss: 0.000513\nTrain Epoch: 11 [55680/60000 (93%)]\tLoss: 0.017531\nTrain Epoch: 11 [56320/60000 (94%)]\tLoss: 0.005028\nTrain Epoch: 11 [56960/60000 (95%)]\tLoss: 0.001848\nTrain Epoch: 11 [57600/60000 (96%)]\tLoss: 0.007708\nTrain Epoch: 11 [58240/60000 (97%)]\tLoss: 0.000490\nTrain Epoch: 11 [58880/60000 (98%)]\tLoss: 0.021458\nTrain Epoch: 11 [59520/60000 (99%)]\tLoss: 0.000887\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nTest set: Average loss: 0.0281, Accuracy: 9915/10000 (99%)\n\nTrain Epoch: 12 [0/60000 (0%)]\tLoss: 0.003167\nTrain Epoch: 12 [640/60000 (1%)]\tLoss: 0.047702\nTrain Epoch: 12 [1280/60000 (2%)]\tLoss: 0.008914\nTrain Epoch: 12 [1920/60000 (3%)]\tLoss: 0.051048\nTrain Epoch: 12 [2560/60000 (4%)]\tLoss: 0.004460\nTrain Epoch: 12 [3200/60000 (5%)]\tLoss: 0.021736\nTrain Epoch: 12 [3840/60000 (6%)]\tLoss: 0.000137\nTrain Epoch: 12 [4480/60000 (7%)]\tLoss: 0.006839\nTrain Epoch: 12 [5120/60000 (9%)]\tLoss: 0.006008\nTrain Epoch: 12 [5760/60000 (10%)]\tLoss: 0.028456\nTrain Epoch: 12 [6400/60000 (11%)]\tLoss: 0.033879\nTrain Epoch: 12 [7040/60000 (12%)]\tLoss: 0.238809\nTrain Epoch: 12 [7680/60000 (13%)]\tLoss: 0.000965\nTrain Epoch: 12 [8320/60000 (14%)]\tLoss: 0.015683\nTrain Epoch: 12 [8960/60000 (15%)]\tLoss: 0.004899\nTrain Epoch: 12 [9600/60000 (16%)]\tLoss: 0.000941\nTrain Epoch: 12 [10240/60000 (17%)]\tLoss: 0.062946\nTrain Epoch: 12 [10880/60000 (18%)]\tLoss: 0.009197\nTrain Epoch: 12 [11520/60000 (19%)]\tLoss: 0.031141\nTrain Epoch: 12 [12160/60000 (20%)]\tLoss: 0.010374\nTrain Epoch: 12 [12800/60000 (21%)]\tLoss: 0.031120\nTrain Epoch: 12 [13440/60000 (22%)]\tLoss: 0.000346\nTrain Epoch: 12 [14080/60000 (23%)]\tLoss: 0.005090\nTrain Epoch: 12 [14720/60000 (25%)]\tLoss: 0.033950\nTrain Epoch: 12 [15360/60000 (26%)]\tLoss: 0.007096\nTrain Epoch: 12 [16000/60000 (27%)]\tLoss: 0.053577\nTrain Epoch: 12 [16640/60000 (28%)]\tLoss: 0.012993\nTrain Epoch: 12 [17280/60000 (29%)]\tLoss: 0.009295\nTrain Epoch: 12 [17920/60000 (30%)]\tLoss: 0.005455\nTrain Epoch: 12 [18560/60000 (31%)]\tLoss: 0.000795\nTrain Epoch: 12 [19200/60000 (32%)]\tLoss: 0.001426\nTrain Epoch: 12 [19840/60000 (33%)]\tLoss: 0.020169\nTrain Epoch: 12 [20480/60000 (34%)]\tLoss: 0.007802\nTrain Epoch: 12 [21120/60000 (35%)]\tLoss: 0.014270\nTrain Epoch: 12 [21760/60000 (36%)]\tLoss: 0.000626\nTrain Epoch: 12 [22400/60000 (37%)]\tLoss: 0.011864\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.004323\nTrain Epoch: 12 [23680/60000 (39%)]\tLoss: 0.010287\nTrain Epoch: 12 [24320/60000 (41%)]\tLoss: 0.003811\nTrain Epoch: 12 [24960/60000 (42%)]\tLoss: 0.021731\nTrain Epoch: 12 [25600/60000 (43%)]\tLoss: 0.001024\nTrain Epoch: 12 [26240/60000 (44%)]\tLoss: 0.004052\nTrain Epoch: 12 [26880/60000 (45%)]\tLoss: 0.019363\nTrain Epoch: 12 [27520/60000 (46%)]\tLoss: 0.104785\nTrain Epoch: 12 [28160/60000 (47%)]\tLoss: 0.002464\nTrain Epoch: 12 [28800/60000 (48%)]\tLoss: 0.000334\nTrain Epoch: 12 [29440/60000 (49%)]\tLoss: 0.026607\nTrain Epoch: 12 [30080/60000 (50%)]\tLoss: 0.030087\nTrain Epoch: 12 [30720/60000 (51%)]\tLoss: 0.008889\nTrain Epoch: 12 [31360/60000 (52%)]\tLoss: 0.014932\nTrain Epoch: 12 [32000/60000 (53%)]\tLoss: 0.016188\nTrain Epoch: 12 [32640/60000 (54%)]\tLoss: 0.001876\nTrain Epoch: 12 [33280/60000 (55%)]\tLoss: 0.016714\nTrain Epoch: 12 [33920/60000 (57%)]\tLoss: 0.000842\nTrain Epoch: 12 [34560/60000 (58%)]\tLoss: 0.001787\nTrain Epoch: 12 [35200/60000 (59%)]\tLoss: 0.041304\nTrain Epoch: 12 [35840/60000 (60%)]\tLoss: 0.049372\nTrain Epoch: 12 [36480/60000 (61%)]\tLoss: 0.008412\nTrain Epoch: 12 [37120/60000 (62%)]\tLoss: 0.009041\nTrain Epoch: 12 [37760/60000 (63%)]\tLoss: 0.002058\nTrain Epoch: 12 [38400/60000 (64%)]\tLoss: 0.037169\nTrain Epoch: 12 [39040/60000 (65%)]\tLoss: 0.001740\nTrain Epoch: 12 [39680/60000 (66%)]\tLoss: 0.005590\nTrain Epoch: 12 [40320/60000 (67%)]\tLoss: 0.001546\nTrain Epoch: 12 [40960/60000 (68%)]\tLoss: 0.052672\nTrain Epoch: 12 [41600/60000 (69%)]\tLoss: 0.012296\nTrain Epoch: 12 [42240/60000 (70%)]\tLoss: 0.006118\nTrain Epoch: 12 [42880/60000 (71%)]\tLoss: 0.005661\nTrain Epoch: 12 [43520/60000 (72%)]\tLoss: 0.027550\nTrain Epoch: 12 [44160/60000 (74%)]\tLoss: 0.057266\nTrain Epoch: 12 [44800/60000 (75%)]\tLoss: 0.012879\nTrain Epoch: 12 [45440/60000 (76%)]\tLoss: 0.012365\nTrain Epoch: 12 [46080/60000 (77%)]\tLoss: 0.005649\nTrain Epoch: 12 [46720/60000 (78%)]\tLoss: 0.023796\nTrain Epoch: 12 [47360/60000 (79%)]\tLoss: 0.012874\nTrain Epoch: 12 [48000/60000 (80%)]\tLoss: 0.011361\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.019059\nTrain Epoch: 12 [49280/60000 (82%)]\tLoss: 0.043080\nTrain Epoch: 12 [49920/60000 (83%)]\tLoss: 0.010917\nTrain Epoch: 12 [50560/60000 (84%)]\tLoss: 0.027835\nTrain Epoch: 12 [51200/60000 (85%)]\tLoss: 0.096073\nTrain Epoch: 12 [51840/60000 (86%)]\tLoss: 0.001065\nTrain Epoch: 12 [52480/60000 (87%)]\tLoss: 0.001847\nTrain Epoch: 12 [53120/60000 (88%)]\tLoss: 0.004331\nTrain Epoch: 12 [53760/60000 (90%)]\tLoss: 0.035913\nTrain Epoch: 12 [54400/60000 (91%)]\tLoss: 0.018254\nTrain Epoch: 12 [55040/60000 (92%)]\tLoss: 0.001035\nTrain Epoch: 12 [55680/60000 (93%)]\tLoss: 0.027624\nTrain Epoch: 12 [56320/60000 (94%)]\tLoss: 0.010064\nTrain Epoch: 12 [56960/60000 (95%)]\tLoss: 0.005551\nTrain Epoch: 12 [57600/60000 (96%)]\tLoss: 0.006935\nTrain Epoch: 12 [58240/60000 (97%)]\tLoss: 0.000142\nTrain Epoch: 12 [58880/60000 (98%)]\tLoss: 0.001538\nTrain Epoch: 12 [59520/60000 (99%)]\tLoss: 0.001892\n\nTest set: Average loss: 0.0284, Accuracy: 9911/10000 (99%)\n\nTrain Epoch: 13 [0/60000 (0%)]\tLoss: 0.000402\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.001331\nTrain Epoch: 13 [1280/60000 (2%)]\tLoss: 0.006754\nTrain Epoch: 13 [1920/60000 (3%)]\tLoss: 0.089798\nTrain Epoch: 13 [2560/60000 (4%)]\tLoss: 0.051625\nTrain Epoch: 13 [3200/60000 (5%)]\tLoss: 0.012814\nTrain Epoch: 13 [3840/60000 (6%)]\tLoss: 0.000468\nTrain Epoch: 13 [4480/60000 (7%)]\tLoss: 0.022673\nTrain Epoch: 13 [5120/60000 (9%)]\tLoss: 0.016970\nTrain Epoch: 13 [5760/60000 (10%)]\tLoss: 0.022078\nTrain Epoch: 13 [6400/60000 (11%)]\tLoss: 0.176903\nTrain Epoch: 13 [7040/60000 (12%)]\tLoss: 0.275088\nTrain Epoch: 13 [7680/60000 (13%)]\tLoss: 0.025053\nTrain Epoch: 13 [8320/60000 (14%)]\tLoss: 0.004641\nTrain Epoch: 13 [8960/60000 (15%)]\tLoss: 0.062781\nTrain Epoch: 13 [9600/60000 (16%)]\tLoss: 0.003280\nTrain Epoch: 13 [10240/60000 (17%)]\tLoss: 0.033968\nTrain Epoch: 13 [10880/60000 (18%)]\tLoss: 0.015032\nTrain Epoch: 13 [11520/60000 (19%)]\tLoss: 0.018973\nTrain Epoch: 13 [12160/60000 (20%)]\tLoss: 0.062533\nTrain Epoch: 13 [12800/60000 (21%)]\tLoss: 0.027602\nTrain Epoch: 13 [13440/60000 (22%)]\tLoss: 0.011991\nTrain Epoch: 13 [14080/60000 (23%)]\tLoss: 0.002013\nTrain Epoch: 13 [14720/60000 (25%)]\tLoss: 0.027235\nTrain Epoch: 13 [15360/60000 (26%)]\tLoss: 0.039850\nTrain Epoch: 13 [16000/60000 (27%)]\tLoss: 0.019587\nTrain Epoch: 13 [16640/60000 (28%)]\tLoss: 0.042315\nTrain Epoch: 13 [17280/60000 (29%)]\tLoss: 0.002268\nTrain Epoch: 13 [17920/60000 (30%)]\tLoss: 0.015554\nTrain Epoch: 13 [18560/60000 (31%)]\tLoss: 0.045551\nTrain Epoch: 13 [19200/60000 (32%)]\tLoss: 0.050593\nTrain Epoch: 13 [19840/60000 (33%)]\tLoss: 0.044335\nTrain Epoch: 13 [20480/60000 (34%)]\tLoss: 0.001419\nTrain Epoch: 13 [21120/60000 (35%)]\tLoss: 0.043823\nTrain Epoch: 13 [21760/60000 (36%)]\tLoss: 0.000816\nTrain Epoch: 13 [22400/60000 (37%)]\tLoss: 0.009577\nTrain Epoch: 13 [23040/60000 (38%)]\tLoss: 0.023228\nTrain Epoch: 13 [23680/60000 (39%)]\tLoss: 0.030843\nTrain Epoch: 13 [24320/60000 (41%)]\tLoss: 0.000838\nTrain Epoch: 13 [24960/60000 (42%)]\tLoss: 0.000381\nTrain Epoch: 13 [25600/60000 (43%)]\tLoss: 0.028405\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.002475\nTrain Epoch: 13 [26880/60000 (45%)]\tLoss: 0.045094\nTrain Epoch: 13 [27520/60000 (46%)]\tLoss: 0.019584\nTrain Epoch: 13 [28160/60000 (47%)]\tLoss: 0.002607\nTrain Epoch: 13 [28800/60000 (48%)]\tLoss: 0.002466\nTrain Epoch: 13 [29440/60000 (49%)]\tLoss: 0.007722\nTrain Epoch: 13 [30080/60000 (50%)]\tLoss: 0.008015\nTrain Epoch: 13 [30720/60000 (51%)]\tLoss: 0.017145\nTrain Epoch: 13 [31360/60000 (52%)]\tLoss: 0.011505\nTrain Epoch: 13 [32000/60000 (53%)]\tLoss: 0.049680\nTrain Epoch: 13 [32640/60000 (54%)]\tLoss: 0.011568\nTrain Epoch: 13 [33280/60000 (55%)]\tLoss: 0.013808\nTrain Epoch: 13 [33920/60000 (57%)]\tLoss: 0.000463\nTrain Epoch: 13 [34560/60000 (58%)]\tLoss: 0.000506\nTrain Epoch: 13 [35200/60000 (59%)]\tLoss: 0.104844\nTrain Epoch: 13 [35840/60000 (60%)]\tLoss: 0.071545\nTrain Epoch: 13 [36480/60000 (61%)]\tLoss: 0.009891\nTrain Epoch: 13 [37120/60000 (62%)]\tLoss: 0.006128\nTrain Epoch: 13 [37760/60000 (63%)]\tLoss: 0.019572\nTrain Epoch: 13 [38400/60000 (64%)]\tLoss: 0.116664\nTrain Epoch: 13 [39040/60000 (65%)]\tLoss: 0.008285\nTrain Epoch: 13 [39680/60000 (66%)]\tLoss: 0.010789\nTrain Epoch: 13 [40320/60000 (67%)]\tLoss: 0.011434\nTrain Epoch: 13 [40960/60000 (68%)]\tLoss: 0.024353\nTrain Epoch: 13 [41600/60000 (69%)]\tLoss: 0.001627\nTrain Epoch: 13 [42240/60000 (70%)]\tLoss: 0.016676\nTrain Epoch: 13 [42880/60000 (71%)]\tLoss: 0.003350\nTrain Epoch: 13 [43520/60000 (72%)]\tLoss: 0.005872\nTrain Epoch: 13 [44160/60000 (74%)]\tLoss: 0.000986\nTrain Epoch: 13 [44800/60000 (75%)]\tLoss: 0.023181\nTrain Epoch: 13 [45440/60000 (76%)]\tLoss: 0.011369\nTrain Epoch: 13 [46080/60000 (77%)]\tLoss: 0.005142\nTrain Epoch: 13 [46720/60000 (78%)]\tLoss: 0.053106\nTrain Epoch: 13 [47360/60000 (79%)]\tLoss: 0.033030\nTrain Epoch: 13 [48000/60000 (80%)]\tLoss: 0.003803\nTrain Epoch: 13 [48640/60000 (81%)]\tLoss: 0.002350\nTrain Epoch: 13 [49280/60000 (82%)]\tLoss: 0.015619\nTrain Epoch: 13 [49920/60000 (83%)]\tLoss: 0.020163\nTrain Epoch: 13 [50560/60000 (84%)]\tLoss: 0.031223\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.038575\nTrain Epoch: 13 [51840/60000 (86%)]\tLoss: 0.009572\nTrain Epoch: 13 [52480/60000 (87%)]\tLoss: 0.002421\nTrain Epoch: 13 [53120/60000 (88%)]\tLoss: 0.003066\nTrain Epoch: 13 [53760/60000 (90%)]\tLoss: 0.109131\nTrain Epoch: 13 [54400/60000 (91%)]\tLoss: 0.003604\nTrain Epoch: 13 [55040/60000 (92%)]\tLoss: 0.005472\nTrain Epoch: 13 [55680/60000 (93%)]\tLoss: 0.032734\nTrain Epoch: 13 [56320/60000 (94%)]\tLoss: 0.011528\nTrain Epoch: 13 [56960/60000 (95%)]\tLoss: 0.001302\nTrain Epoch: 13 [57600/60000 (96%)]\tLoss: 0.028463\nTrain Epoch: 13 [58240/60000 (97%)]\tLoss: 0.004621\nTrain Epoch: 13 [58880/60000 (98%)]\tLoss: 0.000518\nTrain Epoch: 13 [59520/60000 (99%)]\tLoss: 0.000641\n\nTest set: Average loss: 0.0282, Accuracy: 9916/10000 (99%)\n\nTrain Epoch: 14 [0/60000 (0%)]\tLoss: 0.003419\nTrain Epoch: 14 [640/60000 (1%)]\tLoss: 0.000537\nTrain Epoch: 14 [1280/60000 (2%)]\tLoss: 0.053181\nTrain Epoch: 14 [1920/60000 (3%)]\tLoss: 0.013391\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.002235\nTrain Epoch: 14 [3200/60000 (5%)]\tLoss: 0.006807\nTrain Epoch: 14 [3840/60000 (6%)]\tLoss: 0.003513\nTrain Epoch: 14 [4480/60000 (7%)]\tLoss: 0.004041\nTrain Epoch: 14 [5120/60000 (9%)]\tLoss: 0.046514\nTrain Epoch: 14 [5760/60000 (10%)]\tLoss: 0.030836\nTrain Epoch: 14 [6400/60000 (11%)]\tLoss: 0.138029\nTrain Epoch: 14 [7040/60000 (12%)]\tLoss: 0.138985\nTrain Epoch: 14 [7680/60000 (13%)]\tLoss: 0.005225\nTrain Epoch: 14 [8320/60000 (14%)]\tLoss: 0.003691\nTrain Epoch: 14 [8960/60000 (15%)]\tLoss: 0.043210\nTrain Epoch: 14 [9600/60000 (16%)]\tLoss: 0.007291\nTrain Epoch: 14 [10240/60000 (17%)]\tLoss: 0.068542\nTrain Epoch: 14 [10880/60000 (18%)]\tLoss: 0.001177\nTrain Epoch: 14 [11520/60000 (19%)]\tLoss: 0.013633\nTrain Epoch: 14 [12160/60000 (20%)]\tLoss: 0.020149\nTrain Epoch: 14 [12800/60000 (21%)]\tLoss: 0.022902\nTrain Epoch: 14 [13440/60000 (22%)]\tLoss: 0.000106\nTrain Epoch: 14 [14080/60000 (23%)]\tLoss: 0.019971\nTrain Epoch: 14 [14720/60000 (25%)]\tLoss: 0.014268\nTrain Epoch: 14 [15360/60000 (26%)]\tLoss: 0.024161\nTrain Epoch: 14 [16000/60000 (27%)]\tLoss: 0.018132\nTrain Epoch: 14 [16640/60000 (28%)]\tLoss: 0.109281\nTrain Epoch: 14 [17280/60000 (29%)]\tLoss: 0.000618\nTrain Epoch: 14 [17920/60000 (30%)]\tLoss: 0.001262\nTrain Epoch: 14 [18560/60000 (31%)]\tLoss: 0.005227\nTrain Epoch: 14 [19200/60000 (32%)]\tLoss: 0.037542\nTrain Epoch: 14 [19840/60000 (33%)]\tLoss: 0.009928\nTrain Epoch: 14 [20480/60000 (34%)]\tLoss: 0.000457\nTrain Epoch: 14 [21120/60000 (35%)]\tLoss: 0.009604\nTrain Epoch: 14 [21760/60000 (36%)]\tLoss: 0.000514\nTrain Epoch: 14 [22400/60000 (37%)]\tLoss: 0.000358\nTrain Epoch: 14 [23040/60000 (38%)]\tLoss: 0.099895\nTrain Epoch: 14 [23680/60000 (39%)]\tLoss: 0.065969\nTrain Epoch: 14 [24320/60000 (41%)]\tLoss: 0.002624\nTrain Epoch: 14 [24960/60000 (42%)]\tLoss: 0.002591\nTrain Epoch: 14 [25600/60000 (43%)]\tLoss: 0.003904\nTrain Epoch: 14 [26240/60000 (44%)]\tLoss: 0.004380\nTrain Epoch: 14 [26880/60000 (45%)]\tLoss: 0.020968\nTrain Epoch: 14 [27520/60000 (46%)]\tLoss: 0.018631\nTrain Epoch: 14 [28160/60000 (47%)]\tLoss: 0.039545\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.003833\nTrain Epoch: 14 [29440/60000 (49%)]\tLoss: 0.013458\nTrain Epoch: 14 [30080/60000 (50%)]\tLoss: 0.005005\nTrain Epoch: 14 [30720/60000 (51%)]\tLoss: 0.007368\nTrain Epoch: 14 [31360/60000 (52%)]\tLoss: 0.046243\nTrain Epoch: 14 [32000/60000 (53%)]\tLoss: 0.019229\nTrain Epoch: 14 [32640/60000 (54%)]\tLoss: 0.006932\nTrain Epoch: 14 [33280/60000 (55%)]\tLoss: 0.018923\nTrain Epoch: 14 [33920/60000 (57%)]\tLoss: 0.017587\nTrain Epoch: 14 [34560/60000 (58%)]\tLoss: 0.000746\nTrain Epoch: 14 [35200/60000 (59%)]\tLoss: 0.102284\nTrain Epoch: 14 [35840/60000 (60%)]\tLoss: 0.023384\nTrain Epoch: 14 [36480/60000 (61%)]\tLoss: 0.005457\nTrain Epoch: 14 [37120/60000 (62%)]\tLoss: 0.006157\nTrain Epoch: 14 [37760/60000 (63%)]\tLoss: 0.031551\nTrain Epoch: 14 [38400/60000 (64%)]\tLoss: 0.074524\nTrain Epoch: 14 [39040/60000 (65%)]\tLoss: 0.000644\nTrain Epoch: 14 [39680/60000 (66%)]\tLoss: 0.012539\nTrain Epoch: 14 [40320/60000 (67%)]\tLoss: 0.001708\nTrain Epoch: 14 [40960/60000 (68%)]\tLoss: 0.129643\nTrain Epoch: 14 [41600/60000 (69%)]\tLoss: 0.010349\nTrain Epoch: 14 [42240/60000 (70%)]\tLoss: 0.006322\nTrain Epoch: 14 [42880/60000 (71%)]\tLoss: 0.006110\nTrain Epoch: 14 [43520/60000 (72%)]\tLoss: 0.012425\nTrain Epoch: 14 [44160/60000 (74%)]\tLoss: 0.006984\nTrain Epoch: 14 [44800/60000 (75%)]\tLoss: 0.049652\nTrain Epoch: 14 [45440/60000 (76%)]\tLoss: 0.010378\nTrain Epoch: 14 [46080/60000 (77%)]\tLoss: 0.009092\nTrain Epoch: 14 [46720/60000 (78%)]\tLoss: 0.021234\nTrain Epoch: 14 [47360/60000 (79%)]\tLoss: 0.082017\nTrain Epoch: 14 [48000/60000 (80%)]\tLoss: 0.002501\nTrain Epoch: 14 [48640/60000 (81%)]\tLoss: 0.014645\nTrain Epoch: 14 [49280/60000 (82%)]\tLoss: 0.012155\nTrain Epoch: 14 [49920/60000 (83%)]\tLoss: 0.009038\nTrain Epoch: 14 [50560/60000 (84%)]\tLoss: 0.026919\nTrain Epoch: 14 [51200/60000 (85%)]\tLoss: 0.053299\nTrain Epoch: 14 [51840/60000 (86%)]\tLoss: 0.003620\nTrain Epoch: 14 [52480/60000 (87%)]\tLoss: 0.006168\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.019469\nTrain Epoch: 14 [53760/60000 (90%)]\tLoss: 0.059504\nTrain Epoch: 14 [54400/60000 (91%)]\tLoss: 0.031320\nTrain Epoch: 14 [55040/60000 (92%)]\tLoss: 0.004289\nTrain Epoch: 14 [55680/60000 (93%)]\tLoss: 0.062647\nTrain Epoch: 14 [56320/60000 (94%)]\tLoss: 0.004157\nTrain Epoch: 14 [56960/60000 (95%)]\tLoss: 0.003114\nTrain Epoch: 14 [57600/60000 (96%)]\tLoss: 0.019688\nTrain Epoch: 14 [58240/60000 (97%)]\tLoss: 0.002009\nTrain Epoch: 14 [58880/60000 (98%)]\tLoss: 0.000172\nTrain Epoch: 14 [59520/60000 (99%)]\tLoss: 0.003201\n\nTest set: Average loss: 0.0286, Accuracy: 9917/10000 (99%)\n\n[rank0]:[W619 20:48:00.670466910 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-06-19 20:48:06,908\tINFO\t( 3 min) operation 833c9385-39d5f04a-24dd03e8-b3861697 completed\n"
                },
                {
                    "data": {
                        "text/plain": "RunInfo(operation_spec={'intermediate_data_medium': 'nvme', 'intermediate_data_account': 'equal_amethyst_vulture', 'started_by': {'hostname': 'end-end-4.exec-nodes-end.nebius-playground.svc.kyt.k8s.nebius.yt', 'pid': 618, 'wrapper_version': '0.13.28', 'python_version': '3.12.10', 'binary_name': 'ipykernel_launcher.py', 'command': ['/slot/sandbox/jlab/lib/python42/site-packages/ipykernel_launcher.py', '-f', '/slot/sandbox/.local/share/jupyter/runtime/kernel-2a501a16-d03c-4234-9f78-97ef00f8ebd3.json'], 'user': 'root', 'platform': 'Debian GNU/Linux 12 (bookworm)'}, 'fail_on_job_restart': True, 'is_gang': True, 'annotations': {'is_tractorun': True}, 'tasks': {'task': {'command': 'python3 _py_runner.py wrapped.pickle config_dump _modules_info modules/_main_module.py _main_module PY_SOURCE', 'job_count': 1, 'gpu_limit': 1, 'port_count': 1, 'cpu_limit': 10, 'memory_limit': 32213319843, 'docker_image': 'cr.eu-north1.nebius.cloud/e00faee7vas5hpsh3s/solutions/examples:v5', 'file_paths': [{'value': '//home/equal_amethyst_vulture/tmp_files/new_cache/cf/f1847b6cc799f99027b5bee82315c1cf', 'attributes': {'executable': False, 'file_name': '__bootstrap_config'}}, {'value': '//home/equal_amethyst_vulture/tmp_files/new_cache/f8/2404f538721084ea1eaf0ebca55586f8', 'attributes': {'executable': False, 'file_name': '_py_runner.py'}}, {'value': '//home/equal_amethyst_vulture/tmp_files/new_cache/67/6610b8b55b7915b0009866b52b196167', 'attributes': {'executable': True, 'file_name': 'wrapped.pickle'}}, {'value': '//home/equal_amethyst_vulture/tmp_files/new_cache/92/158811d1bbd3fee3770ac56732001d92', 'attributes': {'executable': True, 'file_name': 'config_dump'}}, {'value': '//home/equal_amethyst_vulture/tmp_files/new_cache/60/2ad7948e8efe1a82da43d11118877860', 'attributes': {'executable': True, 'file_name': 'modules_00.tar.gz'}}, {'value': '//home/equal_amethyst_vulture/tmp_files/new_cache/4e/1e1503431e8c5343f676978efb84354e', 'attributes': {'executable': True, 'file_name': '_modules_info'}}, {'value': '//home/equal_amethyst_vulture/tmp_files/new_cache/dc/db1f4692485ea68b83a225d93f65c9dc', 'attributes': {'executable': False, 'file_name': 'modules/_main_module.py'}}], 'environment': {'YT_ALLOW_HTTP_REQUESTS_TO_YT_FROM_JOB': '1', 'YT_USER_CONFIG': '{}', 'PYTHONDONTWRITEBYTECODE': '1', 'BIND_PATHS': '{\"files\": [], \"dirs\": []}', 'PYTHONPATH': '$PYTHONPATH', 'BOOTSTRAP_CONFIG_FILENAME': '/slot/sandbox/__bootstrap_config', 'YT_LOG_LEVEL': 'INFO', 'YT_FORBID_REQUESTS_FROM_JOB': '1'}, 'title': 'wrapped', 'use_yamr_descriptors': False, 'redirect_stdout_to_stderr': True, 'check_input_fully_consumed': False, 'tmpfs_path': 'tmpfs', 'tmpfs_size': 1065123}}, 'title': 'Tractorun //home/equal_amethyst_vulture/tmp/demo_workdir/594e35db3a434a748e844f4d22c07e6c/tractorun', 'pool_trees': ['gpu_h100'], 'max_stderr_count': 150}, operation_id='833c9385-39d5f04a-24dd03e8-b3861697')"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom tractorun.backend.tractorch import YtTensorDataset, Tractorch\nfrom tractorun.toolbox import Toolbox\nfrom tractorun.run import run\nfrom tractorun.mesh import Mesh\nfrom tractorun.resources import Resources\nfrom tractorun.stderr_reader import StderrMode\nfrom tractorun.backend.tractorch.serializer import TensorSerializer\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()), file=sys.stderr)\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)), file=sys.stderr)\n\n\ndef main(toolbox: Toolbox):\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--no-mps', action='store_true', default=False,\n                        help='disables macOS GPU training')\n    parser.add_argument('--dry-run', action='store_true', default=False,\n                        help='quickly check a single pass')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args([])\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n    torch.manual_seed(args.seed)\n\n    if use_cuda:\n        device = torch.device(\"cuda\")\n    elif use_mps:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    train_kwargs = {'batch_size': args.batch_size}\n    test_kwargs = {'batch_size': args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': False}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    dataset1 = YtTensorDataset(path=dataset_train_path, yt_client=toolbox.yt_client, columns=['data', 'labels'])\n    dataset2 = YtTensorDataset(path=dataset_test_path, yt_client=toolbox.yt_client, columns=['data', 'labels'])\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n    if args.save_model:\n        ts = TensorSerializer()\n        toolbox.save_model(ts.serialize(model.state_dict()), dataset_train_path, metadata={})\n\n\nrun(\n    main,\n    backend=Tractorch(),\n    yt_path=training_dir,\n    mesh=Mesh(node_count=1, process_per_node=1, gpu_per_process=1, pool_trees=h100_pool_trees),\n    resources=Resources(\n        cpu_limit=10,\n        memory_limit=32212254720,\n    ),\n    proxy_stderr_mode=StderrMode.primary,\n)"
        },
        {
            "cell_type": "markdown",
            "id": "452c01b0-19e9-4eed-b9fa-9ff8677ebd4b",
            "metadata": {
                "cell_id": "452c01b0-19e9-4eed-b9fa-9ff8677ebd4b",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "ea78890b",
                    "view_cell_type": "MD",
                    "view_source": "Let's consider a production-like scenario for running model training through the `Tractorun CLI`.\n\nThe `Tractorun CLI` allows:\n1. Make model training reproducible.\n2. Separating the model training code from the training run parameters. `Tractorun CLI` enables configuring the training process via a configuration file and CLI parameters.\n3. Running the training module from any host where Python and `Tractorun` are installed.\n\nWe will use the official PyTorch [MNIST training example](https://github.com/pytorch/examples/blob/cdef4d43fb1a2c6c4349daa5080e4e8731c34569/mnist/main.py) again.\nHow to modify it with minimal changes to run using Tractorun:\n1. Add `toolbox = prepare_and_get_toolbox(backend=Tractorch())` to the main function. Toolbox object provides useful utils for training like checkpoint manager, coordination metadata, initialized ytsaurus client, and more.\n2. Add `file=sys.stderr` to each print.\n3. Use `YtTensorDataset` instead of default `torch.Dataset`.\n\n<details>\n  <summary>Show the full diff</summary>\n\n```diff\n@@ -6,6 +6,15 @@\n from torchvision import datasets, transforms\n from torch.optim.lr_scheduler import StepLR\n\n+import sys\n+from tractorun.backend.tractorch import YtTensorDataset, Tractorch\n+from tractorun.toolbox import Toolbox\n+from tractorun.run import run\n+from tractorun.mesh import Mesh\n+from tractorun.resources import Resources\n+from tractorun.stderr_reader import StderrMode\n+from tractorun.backend.tractorch.serializer import TensorSerializer\n+from tractorun.run import prepare_and_get_toolbox\n\n class Net(nn.Module):\n     def __init__(self):\n@@ -45,7 +54,7 @@\n         if batch_idx % args.log_interval == 0:\n             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                 epoch, batch_idx * len(data), len(train_loader.dataset),\n-                100. * batch_idx / len(train_loader), loss.item()))\n+                100. * batch_idx / len(train_loader), loss.item()), file=sys.stderr)\n             if args.dry_run:\n                 break\n\n@@ -66,10 +75,13 @@\n\n     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n         test_loss, correct, len(test_loader.dataset),\n-        100. * correct / len(test_loader.dataset)))\n+        100. * correct / len(test_loader.dataset)), file=sys.stderr)\n\n\n def main():\n+    toolbox = prepare_and_get_toolbox(backend=Tractorch())\n+    user_config = toolbox.get_user_config()\n+\n     # Training settings\n     parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n     parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n@@ -94,7 +106,7 @@\n                         help='how many batches to wait before logging training status')\n     parser.add_argument('--save-model', action='store_true', default=False,\n                         help='For Saving the current Model')\n-    args = parser.parse_args()\n+    args = parser.parse_args([])\n     use_cuda = not args.no_cuda and torch.cuda.is_available()\n     use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n@@ -120,10 +132,9 @@\n         transforms.ToTensor(),\n         transforms.Normalize((0.1307,), (0.3081,))\n         ])\n-    dataset1 = datasets.MNIST('../data', train=True, download=True,\n-                       transform=transform)\n-    dataset2 = datasets.MNIST('../data', train=False,\n-                       transform=transform)\n+    dataset1 = YtTensorDataset(toolbox=toolbox, path=user_config[\"dataset_train_path\"], columns=['data', 'labels'])\n+    dataset2 = YtTensorDataset(toolbox=toolbox, path=user_config[\"dataset_test_path\"], columns=['data', 'labels'])\n+\n     train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n     test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n@@ -137,9 +148,9 @@\n         scheduler.step()\n\n     if args.save_model:\n-        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n+        ts = TensorSerializer()\n+        toolbox.save_model(ts.serialize(model.state_dict()), dataset_train_path, metadata={})\n\n\n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     main()\n```\n</details>"
                }
            },
            "source": "## Tractorun cli\n\nLet's consider a production-like scenario for running model training through the `Tractorun CLI`.\n\nThe `Tractorun CLI` allows:\n1. Make model training reproducible.\n2. Separating the model training code from the training run parameters. `Tractorun CLI` enables configuring the training process via a configuration file and CLI parameters.\n3. Running the training module from any host where Python and `Tractorun` are installed.\n\nWe will use the official PyTorch [MNIST training example](https://github.com/pytorch/examples/blob/cdef4d43fb1a2c6c4349daa5080e4e8731c34569/mnist/main.py) again.\nHow to modify it with minimal changes to run using Tractorun:\n1. Add `toolbox = prepare_and_get_toolbox(backend=Tractorch())` to the main function. Toolbox object provides useful utils for training like checkpoint manager, coordination metadata, initialized ytsaurus client, and more.\n2. Add `file=sys.stderr` to each print.\n3. Use `YtTensorDataset` instead of default `torch.Dataset`.\n\n<details>\n  <summary>Show the full diff</summary>\n\n```diff\n@@ -6,6 +6,15 @@\n from torchvision import datasets, transforms\n from torch.optim.lr_scheduler import StepLR\n\n+import sys\n+from tractorun.backend.tractorch import YtTensorDataset, Tractorch\n+from tractorun.toolbox import Toolbox\n+from tractorun.run import run\n+from tractorun.mesh import Mesh\n+from tractorun.resources import Resources\n+from tractorun.stderr_reader import StderrMode\n+from tractorun.backend.tractorch.serializer import TensorSerializer\n+from tractorun.run import prepare_and_get_toolbox\n\n class Net(nn.Module):\n     def __init__(self):\n@@ -45,7 +54,7 @@\n         if batch_idx % args.log_interval == 0:\n             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                 epoch, batch_idx * len(data), len(train_loader.dataset),\n-                100. * batch_idx / len(train_loader), loss.item()))\n+                100. * batch_idx / len(train_loader), loss.item()), file=sys.stderr)\n             if args.dry_run:\n                 break\n\n@@ -66,10 +75,13 @@\n\n     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n         test_loss, correct, len(test_loader.dataset),\n-        100. * correct / len(test_loader.dataset)))\n+        100. * correct / len(test_loader.dataset)), file=sys.stderr)\n\n\n def main():\n+    toolbox = prepare_and_get_toolbox(backend=Tractorch())\n+    user_config = toolbox.get_user_config()\n+\n     # Training settings\n     parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n     parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n@@ -94,7 +106,7 @@\n                         help='how many batches to wait before logging training status')\n     parser.add_argument('--save-model', action='store_true', default=False,\n                         help='For Saving the current Model')\n-    args = parser.parse_args()\n+    args = parser.parse_args([])\n     use_cuda = not args.no_cuda and torch.cuda.is_available()\n     use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n@@ -120,10 +132,9 @@\n         transforms.ToTensor(),\n         transforms.Normalize((0.1307,), (0.3081,))\n         ])\n-    dataset1 = datasets.MNIST('../data', train=True, download=True,\n-                       transform=transform)\n-    dataset2 = datasets.MNIST('../data', train=False,\n-                       transform=transform)\n+    dataset1 = YtTensorDataset(toolbox=toolbox, path=user_config[\"dataset_train_path\"], columns=['data', 'labels'])\n+    dataset2 = YtTensorDataset(toolbox=toolbox, path=user_config[\"dataset_test_path\"], columns=['data', 'labels'])\n+\n     train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n     test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n@@ -137,9 +148,9 @@\n         scheduler.step()\n\n     if args.save_model:\n-        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n+        ts = TensorSerializer()\n+        toolbox.save_model(ts.serialize(model.state_dict()), dataset_train_path, metadata={})\n\n\n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     main()\n```\n</details>"
        },
        {
            "cell_type": "markdown",
            "id": "105c4a39-06ae-4a69-aea9-d303d779bc9d",
            "metadata": {
                "cell_id": "105c4a39-06ae-4a69-aea9-d303d779bc9d",
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "428c9508",
                    "view_cell_type": "MD",
                    "view_source": "Let's create two files:\n1. `main.py` with our model-training code.\n2. `run_config.yaml` with training configuration."
                }
            },
            "source": "Let's create two files:\n1. `main.py` with our model-training code.\n2. `run_config.yaml` with training configuration."
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "4fb7d69f-71c7-4c81-b379-c381629d6043",
            "metadata": {
                "cell_id": "4fb7d69f-71c7-4c81-b379-c381629d6043",
                "tracto": {
                    "execution_end": 1750366131371,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750366131367,
                    "metadata_version": "1",
                    "source_hash": "7a8709ab",
                    "view_cell_type": "CODE",
                    "view_source": "code = r\"\"\"\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nimport sys\nfrom tractorun.backend.tractorch import YtTensorDataset, Tractorch\nfrom tractorun.toolbox import Toolbox\nfrom tractorun.run import run\nfrom tractorun.mesh import Mesh\nfrom tractorun.resources import Resources\nfrom tractorun.stderr_reader import StderrMode\nfrom tractorun.backend.tractorch.serializer import TensorSerializer\nfrom tractorun.run import prepare_and_get_toolbox\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()), file=sys.stderr)\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)), file=sys.stderr)\n\n\ndef main():\n    toolbox = prepare_and_get_toolbox(backend=Tractorch())\n    user_config = toolbox.get_user_config()\n\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--no-mps', action='store_true', default=False,\n                        help='disables macOS GPU training')\n    parser.add_argument('--dry-run', action='store_true', default=False,\n                        help='quickly check a single pass')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args([])\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n    torch.manual_seed(args.seed)\n\n    if use_cuda:\n        device = torch.device(\"cuda\")\n    elif use_mps:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    train_kwargs = {'batch_size': args.batch_size}\n    test_kwargs = {'batch_size': args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    dataset1 = YtTensorDataset(toolbox=toolbox, yt_client=toolbox.yt_client, path=user_config[\"dataset_train_path\"], columns=['data', 'labels'])\n    dataset2 = YtTensorDataset(toolbox=toolbox, yt_client=toolbox.yt_client, path=user_config[\"dataset_test_path\"], columns=['data', 'labels'])\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n    if args.save_model:\n        ts = TensorSerializer()\n        toolbox.save_model(ts.serialize(model.state_dict()), dataset_train_path, metadata={})\n\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\nwith open(\"main.py\", \"w\") as f:\n    f.write(code)"
                }
            },
            "outputs": [],
            "source": "code = r\"\"\"\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nimport sys\nfrom tractorun.backend.tractorch import YtTensorDataset, Tractorch\nfrom tractorun.toolbox import Toolbox\nfrom tractorun.run import run\nfrom tractorun.mesh import Mesh\nfrom tractorun.resources import Resources\nfrom tractorun.stderr_reader import StderrMode\nfrom tractorun.backend.tractorch.serializer import TensorSerializer\nfrom tractorun.run import prepare_and_get_toolbox\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()), file=sys.stderr)\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)), file=sys.stderr)\n\n\ndef main():\n    toolbox = prepare_and_get_toolbox(backend=Tractorch())\n    user_config = toolbox.get_user_config()\n\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--no-mps', action='store_true', default=False,\n                        help='disables macOS GPU training')\n    parser.add_argument('--dry-run', action='store_true', default=False,\n                        help='quickly check a single pass')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args([])\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n    torch.manual_seed(args.seed)\n\n    if use_cuda:\n        device = torch.device(\"cuda\")\n    elif use_mps:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    train_kwargs = {'batch_size': args.batch_size}\n    test_kwargs = {'batch_size': args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    dataset1 = YtTensorDataset(toolbox=toolbox, yt_client=toolbox.yt_client, path=user_config[\"dataset_train_path\"], columns=['data', 'labels'])\n    dataset2 = YtTensorDataset(toolbox=toolbox, yt_client=toolbox.yt_client, path=user_config[\"dataset_test_path\"], columns=['data', 'labels'])\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n    if args.save_model:\n        ts = TensorSerializer()\n        toolbox.save_model(ts.serialize(model.state_dict()), dataset_train_path, metadata={})\n\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\nwith open(\"main.py\", \"w\") as f:\n    f.write(code)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "ce36442d-e406-4f0d-ad6e-45c0876771c2",
            "metadata": {
                "cell_id": "ce36442d-e406-4f0d-ad6e-45c0876771c2",
                "tracto": {
                    "execution_end": 1750366266865,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750366266860,
                    "metadata_version": "1",
                    "source_hash": "2b05da1e",
                    "view_cell_type": "CODE",
                    "view_source": "import yaml\n\n\nconfig = {\n    \"yt_path\": training_dir,\n    \"mesh\": {\n        \"node_count\": 1,\n        \"process_per_node\": 1,\n        \"gpu_per_process\": 0,\n    },  \n    \"user_config\": {\n        \"dataset_train_path\": dataset_train_path,\n        \"dataset_test_path\": dataset_test_path,\n    },\n    \"resources\": {\n        \"cpu_limit\": 8,\n        \"memory_limit\": 105899345920,\n    },\n    \"bind_local\": [\"./main.py:/tractorun/main.py\"],\n    \"command\": [\"python3\", \"/tractorun/main.py\"],\n    \"proxy_stderr_mode\": \"primary\",\n}\n\nwith open(\"run_config.yaml\", \"w\") as f:\n    yaml.dump(config, f)"
                }
            },
            "outputs": [],
            "source": "import yaml\n\n\nconfig = {\n    \"yt_path\": training_dir,\n    \"mesh\": {\n        \"node_count\": 1,\n        \"process_per_node\": 1,\n        \"gpu_per_process\": 0,\n        \"pool_trees\": list(map(str, h100_pool_trees)),\n    },  \n    \"user_config\": {\n        \"dataset_train_path\": dataset_train_path,\n        \"dataset_test_path\": dataset_test_path,\n    },\n    \"resources\": {\n        \"cpu_limit\": 10,\n        \"memory_limit\": 32212254720,\n    },\n    \"bind_local\": [\n        {\n            \"source\": \"./main.py\",\n            \"destination\": \"/tractorun/main.py\",\n        },\n    ],\n    \"command\": [\"python3\", \"/tractorun/main.py\"],\n    \"proxy_stderr_mode\": \"primary\",\n}\n\nwith open(\"run_config.yaml\", \"w\") as f:\n    yaml.dump(config, f)"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "cb6f07a0-071d-4423-8a6c-4e8c8c69c4d5",
            "metadata": {
                "cell_id": "cb6f07a0-071d-4423-8a6c-4e8c8c69c4d5",
                "tracto": {
                    "execution_end": 1750366632664,
                    "execution_session_id": "22e7c26b-d0e7-428d-913f-e4fbab33db75",
                    "execution_start": 1750366267174,
                    "metadata_version": "1",
                    "source_hash": "d25ec465",
                    "view_cell_type": "CODE",
                    "view_source": "!tractorun --run-config-path run_config.yaml"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r  0%|                                                               | 0.00/19.6k\rStarting upload:   0%|                                              | 0.00/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(1/2) [UPLOAD] __file_0.zip:   0%|                                  | 0.00/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(1/2) [UPLOAD] __file_0.zip:  33%|##########7                      | 6.39k/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(1/2) [OK] __file_0.zip:  33%|############                         | 6.39k/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(1/2) [OK] __file_0.zip:  33%|############                         | 6.39k/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(2/2) [UPLOAD] __bootstrap_config:  33%|########7                  | 6.39k/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(2/2) [UPLOAD] __bootstrap_config: 100%|###########################| 19.6k/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(2/2) [OK] __bootstrap_config: 100%|###############################| 19.6k/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(2/2) [OK] __bootstrap_config: 100%|###############################| 19.6k/19.6k"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r(2/2) [OK] __bootstrap_config: 100%|###############################| 19.6k/19.6k\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2025-06-19 20:51:09,904\tINFO\tOperation started: https://playground.tracto.ai/playground/operations/af3941d-fca6ac76-24dd03e8-9b60f6d2/details\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2025-06-19 20:51:09,944\tINFO\t( 0 min) operation af3941d-fca6ac76-24dd03e8-9b60f6d2 starting\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2025-06-19 20:51:10,480\tINFO\t( 0 min) operation af3941d-fca6ac76-24dd03e8-9b60f6d2 initializing\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2025-06-19 20:51:12,694\tINFO\t( 0 min) Unrecognized spec: {'enable_partitioned_data_balancing': false}\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2025-06-19 20:51:12,734\tINFO\t( 0 min) operation af3941d-fca6ac76-24dd03e8-9b60f6d2: running=0     completed=0     pending=1     failed=0     aborted=0     lost=0     total=1     blocked=0    \r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2025-06-19 20:51:13,893\tINFO\t( 0 min) operation af3941d-fca6ac76-24dd03e8-9b60f6d2: running=1     completed=0     pending=0     failed=0     aborted=0     lost=0     total=1     blocked=0    \r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305400\r\nTrain Epoch: 1 [640/60000 (1%)]\tLoss: 1.359776\r\nTrain Epoch: 1 [1280/60000 (2%)]\tLoss: 0.842926\r\nTrain Epoch: 1 [1920/60000 (3%)]\tLoss: 0.593959\r\nTrain Epoch: 1 [2560/60000 (4%)]\tLoss: 0.366616\r\nTrain Epoch: 1 [3200/60000 (5%)]\tLoss: 0.465908\r\nTrain Epoch: 1 [3840/60000 (6%)]\tLoss: 0.269250\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.285698\r\nTrain Epoch: 1 [5120/60000 (9%)]\tLoss: 0.580031\r\nTrain Epoch: 1 [5760/60000 (10%)]\tLoss: 0.202717\r\nTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.240631\r\nTrain Epoch: 1 [7040/60000 (12%)]\tLoss: 0.354062\r\nTrain Epoch: 1 [7680/60000 (13%)]\tLoss: 0.241132\r\nTrain Epoch: 1 [8320/60000 (14%)]\tLoss: 0.222914\r\nTrain Epoch: 1 [8960/60000 (15%)]\tLoss: 0.259533\r\nTrain Epoch: 1 [9600/60000 (16%)]\tLoss: 0.109460\r\nTrain Epoch: 1 [10240/60000 (17%)]\tLoss: 0.298256\r\nTrain Epoch: 1 [10880/60000 (18%)]\tLoss: 0.090736\r\nTrain Epoch: 1 [11520/60000 (19%)]\tLoss: 0.502739\r\nTrain Epoch: 1 [12160/60000 (20%)]\tLoss: 0.280272\r\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.244995\r\nTrain Epoch: 1 [13440/60000 (22%)]\tLoss: 0.226256\r\nTrain Epoch: 1 [14080/60000 (23%)]\tLoss: 0.117108\r\nTrain Epoch: 1 [14720/60000 (25%)]\tLoss: 0.417386\r\nTrain Epoch: 1 [15360/60000 (26%)]\tLoss: 0.170742\r\nTrain Epoch: 1 [16000/60000 (27%)]\tLoss: 0.126738\r\nTrain Epoch: 1 [16640/60000 (28%)]\tLoss: 0.170978\r\nTrain Epoch: 1 [17280/60000 (29%)]\tLoss: 0.069432\r\nTrain Epoch: 1 [17920/60000 (30%)]\tLoss: 0.207928\r\nTrain Epoch: 1 [18560/60000 (31%)]\tLoss: 0.179033\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.249938\r\nTrain Epoch: 1 [19840/60000 (33%)]\tLoss: 0.075640\r\nTrain Epoch: 1 [20480/60000 (34%)]\tLoss: 0.055365\r\nTrain Epoch: 1 [21120/60000 (35%)]\tLoss: 0.229498\r\nTrain Epoch: 1 [21760/60000 (36%)]\tLoss: 0.003967\r\nTrain Epoch: 1 [22400/60000 (37%)]\tLoss: 0.056236\r\nTrain Epoch: 1 [23040/60000 (38%)]\tLoss: 0.243042\r\nTrain Epoch: 1 [23680/60000 (39%)]\tLoss: 0.137981\r\nTrain Epoch: 1 [24320/60000 (41%)]\tLoss: 0.012077\r\nTrain Epoch: 1 [24960/60000 (42%)]\tLoss: 0.107106\r\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.078230\r\nTrain Epoch: 1 [26240/60000 (44%)]\tLoss: 0.066636\r\nTrain Epoch: 1 [26880/60000 (45%)]\tLoss: 0.339630\r\nTrain Epoch: 1 [27520/60000 (46%)]\tLoss: 0.329679\r\nTrain Epoch: 1 [28160/60000 (47%)]\tLoss: 0.113985\r\nTrain Epoch: 1 [28800/60000 (48%)]\tLoss: 0.098892\r\nTrain Epoch: 1 [29440/60000 (49%)]\tLoss: 0.033294\r\nTrain Epoch: 1 [30080/60000 (50%)]\tLoss: 0.151899\r\nTrain Epoch: 1 [30720/60000 (51%)]\tLoss: 0.050745\r\nTrain Epoch: 1 [31360/60000 (52%)]\tLoss: 0.092044\r\nTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.182142\r\nTrain Epoch: 1 [32640/60000 (54%)]\tLoss: 0.108336\r\nTrain Epoch: 1 [33280/60000 (55%)]\tLoss: 0.036727\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.014055\r\nTrain Epoch: 1 [34560/60000 (58%)]\tLoss: 0.015452\r\nTrain Epoch: 1 [35200/60000 (59%)]\tLoss: 0.196742\r\nTrain Epoch: 1 [35840/60000 (60%)]\tLoss: 0.189072\r\nTrain Epoch: 1 [36480/60000 (61%)]\tLoss: 0.042055\r\nTrain Epoch: 1 [37120/60000 (62%)]\tLoss: 0.097097\r\nTrain Epoch: 1 [37760/60000 (63%)]\tLoss: 0.141476\r\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.143970\r\nTrain Epoch: 1 [39040/60000 (65%)]\tLoss: 0.030714\r\nTrain Epoch: 1 [39680/60000 (66%)]\tLoss: 0.028830\r\nTrain Epoch: 1 [40320/60000 (67%)]\tLoss: 0.105118\r\nTrain Epoch: 1 [40960/60000 (68%)]\tLoss: 0.094071\r\nTrain Epoch: 1 [41600/60000 (69%)]\tLoss: 0.105060\r\nTrain Epoch: 1 [42240/60000 (70%)]\tLoss: 0.083696\r\nTrain Epoch: 1 [42880/60000 (71%)]\tLoss: 0.090230\r\nTrain Epoch: 1 [43520/60000 (72%)]\tLoss: 0.234341\r\nTrain Epoch: 1 [44160/60000 (74%)]\tLoss: 0.052024\r\nTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.110237\r\nTrain Epoch: 1 [45440/60000 (76%)]\tLoss: 0.193215\r\nTrain Epoch: 1 [46080/60000 (77%)]\tLoss: 0.100087\r\nTrain Epoch: 1 [46720/60000 (78%)]\tLoss: 0.169076\r\nTrain Epoch: 1 [47360/60000 (79%)]\tLoss: 0.117941\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.074993\r\nTrain Epoch: 1 [48640/60000 (81%)]\tLoss: 0.029282\r\nTrain Epoch: 1 [49280/60000 (82%)]\tLoss: 0.050464\r\nTrain Epoch: 1 [49920/60000 (83%)]\tLoss: 0.061806\r\nTrain Epoch: 1 [50560/60000 (84%)]\tLoss: 0.075060\r\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.277400\r\nTrain Epoch: 1 [51840/60000 (86%)]\tLoss: 0.012938\r\nTrain Epoch: 1 [52480/60000 (87%)]\tLoss: 0.022713\r\nTrain Epoch: 1 [53120/60000 (88%)]\tLoss: 0.154698\r\nTrain Epoch: 1 [53760/60000 (90%)]\tLoss: 0.082866\r\nTrain Epoch: 1 [54400/60000 (91%)]\tLoss: 0.057838\r\nTrain Epoch: 1 [55040/60000 (92%)]\tLoss: 0.051151\r\nTrain Epoch: 1 [55680/60000 (93%)]\tLoss: 0.101481\r\nTrain Epoch: 1 [56320/60000 (94%)]\tLoss: 0.077030\r\nTrain Epoch: 1 [56960/60000 (95%)]\tLoss: 0.112287\r\nTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.112989\r\nTrain Epoch: 1 [58240/60000 (97%)]\tLoss: 0.012752\r\nTrain Epoch: 1 [58880/60000 (98%)]\tLoss: 0.024528\r\nTrain Epoch: 1 [59520/60000 (99%)]\tLoss: 0.000881\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0464, Accuracy: 9843/10000 (98%)\r\n\r\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.118124\r\nTrain Epoch: 2 [640/60000 (1%)]\tLoss: 0.048372\r\nTrain Epoch: 2 [1280/60000 (2%)]\tLoss: 0.065819\r\nTrain Epoch: 2 [1920/60000 (3%)]\tLoss: 0.102890\r\nTrain Epoch: 2 [2560/60000 (4%)]\tLoss: 0.073310\r\nTrain Epoch: 2 [3200/60000 (5%)]\tLoss: 0.069214\r\nTrain Epoch: 2 [3840/60000 (6%)]\tLoss: 0.026889\r\nTrain Epoch: 2 [4480/60000 (7%)]\tLoss: 0.085517\r\nTrain Epoch: 2 [5120/60000 (9%)]\tLoss: 0.120542\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.145780\r\nTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.203504\r\nTrain Epoch: 2 [7040/60000 (12%)]\tLoss: 0.164841\r\nTrain Epoch: 2 [7680/60000 (13%)]\tLoss: 0.105734\r\nTrain Epoch: 2 [8320/60000 (14%)]\tLoss: 0.028832\r\nTrain Epoch: 2 [8960/60000 (15%)]\tLoss: 0.121541\r\nTrain Epoch: 2 [9600/60000 (16%)]\tLoss: 0.042910\r\nTrain Epoch: 2 [10240/60000 (17%)]\tLoss: 0.100423\r\nTrain Epoch: 2 [10880/60000 (18%)]\tLoss: 0.053938\r\nTrain Epoch: 2 [11520/60000 (19%)]\tLoss: 0.128980\r\nTrain Epoch: 2 [12160/60000 (20%)]\tLoss: 0.065199\r\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.098547\r\nTrain Epoch: 2 [13440/60000 (22%)]\tLoss: 0.013551\r\nTrain Epoch: 2 [14080/60000 (23%)]\tLoss: 0.018215\r\nTrain Epoch: 2 [14720/60000 (25%)]\tLoss: 0.087778\r\nTrain Epoch: 2 [15360/60000 (26%)]\tLoss: 0.072197\r\nTrain Epoch: 2 [16000/60000 (27%)]\tLoss: 0.095925\r\nTrain Epoch: 2 [16640/60000 (28%)]\tLoss: 0.080837\r\nTrain Epoch: 2 [17280/60000 (29%)]\tLoss: 0.005668\r\nTrain Epoch: 2 [17920/60000 (30%)]\tLoss: 0.053104\r\nTrain Epoch: 2 [18560/60000 (31%)]\tLoss: 0.093683\r\nTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.059018\r\nTrain Epoch: 2 [19840/60000 (33%)]\tLoss: 0.131711\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.024347\r\nTrain Epoch: 2 [21120/60000 (35%)]\tLoss: 0.099053\r\nTrain Epoch: 2 [21760/60000 (36%)]\tLoss: 0.002408\r\nTrain Epoch: 2 [22400/60000 (37%)]\tLoss: 0.005465\r\nTrain Epoch: 2 [23040/60000 (38%)]\tLoss: 0.078746\r\nTrain Epoch: 2 [23680/60000 (39%)]\tLoss: 0.062910\r\nTrain Epoch: 2 [24320/60000 (41%)]\tLoss: 0.001700\r\nTrain Epoch: 2 [24960/60000 (42%)]\tLoss: 0.007283\r\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.037258\r\nTrain Epoch: 2 [26240/60000 (44%)]\tLoss: 0.017618\r\nTrain Epoch: 2 [26880/60000 (45%)]\tLoss: 0.176907\r\nTrain Epoch: 2 [27520/60000 (46%)]\tLoss: 0.042792\r\nTrain Epoch: 2 [28160/60000 (47%)]\tLoss: 0.096764\r\nTrain Epoch: 2 [28800/60000 (48%)]\tLoss: 0.009297\r\nTrain Epoch: 2 [29440/60000 (49%)]\tLoss: 0.040708\r\nTrain Epoch: 2 [30080/60000 (50%)]\tLoss: 0.025022\r\nTrain Epoch: 2 [30720/60000 (51%)]\tLoss: 0.064806\r\nTrain Epoch: 2 [31360/60000 (52%)]\tLoss: 0.097695\r\nTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.108286\r\nTrain Epoch: 2 [32640/60000 (54%)]\tLoss: 0.066752\r\nTrain Epoch: 2 [33280/60000 (55%)]\tLoss: 0.054359\r\nTrain Epoch: 2 [33920/60000 (57%)]\tLoss: 0.007130\r\nTrain Epoch: 2 [34560/60000 (58%)]\tLoss: 0.021796\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.071000\r\nTrain Epoch: 2 [35840/60000 (60%)]\tLoss: 0.054432\r\nTrain Epoch: 2 [36480/60000 (61%)]\tLoss: 0.049493\r\nTrain Epoch: 2 [37120/60000 (62%)]\tLoss: 0.047352\r\nTrain Epoch: 2 [37760/60000 (63%)]\tLoss: 0.049388\r\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.081749\r\nTrain Epoch: 2 [39040/60000 (65%)]\tLoss: 0.001239\r\nTrain Epoch: 2 [39680/60000 (66%)]\tLoss: 0.018804\r\nTrain Epoch: 2 [40320/60000 (67%)]\tLoss: 0.033999\r\nTrain Epoch: 2 [40960/60000 (68%)]\tLoss: 0.040932\r\nTrain Epoch: 2 [41600/60000 (69%)]\tLoss: 0.044035\r\nTrain Epoch: 2 [42240/60000 (70%)]\tLoss: 0.031046\r\nTrain Epoch: 2 [42880/60000 (71%)]\tLoss: 0.099683\r\nTrain Epoch: 2 [43520/60000 (72%)]\tLoss: 0.047774\r\nTrain Epoch: 2 [44160/60000 (74%)]\tLoss: 0.003201\r\nTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.063082\r\nTrain Epoch: 2 [45440/60000 (76%)]\tLoss: 0.097725\r\nTrain Epoch: 2 [46080/60000 (77%)]\tLoss: 0.114870\r\nTrain Epoch: 2 [46720/60000 (78%)]\tLoss: 0.101061\r\nTrain Epoch: 2 [47360/60000 (79%)]\tLoss: 0.074061\r\nTrain Epoch: 2 [48000/60000 (80%)]\tLoss: 0.056852\r\nTrain Epoch: 2 [48640/60000 (81%)]\tLoss: 0.027510\r\nTrain Epoch: 2 [49280/60000 (82%)]\tLoss: 0.036689\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.045341\r\nTrain Epoch: 2 [50560/60000 (84%)]\tLoss: 0.084605\r\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.128099\r\nTrain Epoch: 2 [51840/60000 (86%)]\tLoss: 0.014248\r\nTrain Epoch: 2 [52480/60000 (87%)]\tLoss: 0.010912\r\nTrain Epoch: 2 [53120/60000 (88%)]\tLoss: 0.040843\r\nTrain Epoch: 2 [53760/60000 (90%)]\tLoss: 0.098477\r\nTrain Epoch: 2 [54400/60000 (91%)]\tLoss: 0.031662\r\nTrain Epoch: 2 [55040/60000 (92%)]\tLoss: 0.022802\r\nTrain Epoch: 2 [55680/60000 (93%)]\tLoss: 0.052728\r\nTrain Epoch: 2 [56320/60000 (94%)]\tLoss: 0.025954\r\nTrain Epoch: 2 [56960/60000 (95%)]\tLoss: 0.042939\r\nTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.012254\r\nTrain Epoch: 2 [58240/60000 (97%)]\tLoss: 0.004422\r\nTrain Epoch: 2 [58880/60000 (98%)]\tLoss: 0.023146\r\nTrain Epoch: 2 [59520/60000 (99%)]\tLoss: 0.009901\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0366, Accuracy: 9869/10000 (99%)\r\n\r\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.048442\r\nTrain Epoch: 3 [640/60000 (1%)]\tLoss: 0.042866\r\nTrain Epoch: 3 [1280/60000 (2%)]\tLoss: 0.018422\r\nTrain Epoch: 3 [1920/60000 (3%)]\tLoss: 0.058884\r\nTrain Epoch: 3 [2560/60000 (4%)]\tLoss: 0.062569\r\nTrain Epoch: 3 [3200/60000 (5%)]\tLoss: 0.070180\r\nTrain Epoch: 3 [3840/60000 (6%)]\tLoss: 0.030547\r\nTrain Epoch: 3 [4480/60000 (7%)]\tLoss: 0.042518\r\nTrain Epoch: 3 [5120/60000 (9%)]\tLoss: 0.047587\r\nTrain Epoch: 3 [5760/60000 (10%)]\tLoss: 0.054566\r\nTrain Epoch: 3 [6400/60000 (11%)]\tLoss: 0.052134\r\nTrain Epoch: 3 [7040/60000 (12%)]\tLoss: 0.220239\r\nTrain Epoch: 3 [7680/60000 (13%)]\tLoss: 0.019823\r\nTrain Epoch: 3 [8320/60000 (14%)]\tLoss: 0.036662\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.069142\r\nTrain Epoch: 3 [9600/60000 (16%)]\tLoss: 0.090159\r\nTrain Epoch: 3 [10240/60000 (17%)]\tLoss: 0.149716\r\nTrain Epoch: 3 [10880/60000 (18%)]\tLoss: 0.014557\r\nTrain Epoch: 3 [11520/60000 (19%)]\tLoss: 0.070681\r\nTrain Epoch: 3 [12160/60000 (20%)]\tLoss: 0.029492\r\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 0.057193\r\nTrain Epoch: 3 [13440/60000 (22%)]\tLoss: 0.010748\r\nTrain Epoch: 3 [14080/60000 (23%)]\tLoss: 0.009651\r\nTrain Epoch: 3 [14720/60000 (25%)]\tLoss: 0.101676\r\nTrain Epoch: 3 [15360/60000 (26%)]\tLoss: 0.017846\r\nTrain Epoch: 3 [16000/60000 (27%)]\tLoss: 0.017839\r\nTrain Epoch: 3 [16640/60000 (28%)]\tLoss: 0.068720\r\nTrain Epoch: 3 [17280/60000 (29%)]\tLoss: 0.000638\r\nTrain Epoch: 3 [17920/60000 (30%)]\tLoss: 0.072613\r\nTrain Epoch: 3 [18560/60000 (31%)]\tLoss: 0.017942\r\nTrain Epoch: 3 [19200/60000 (32%)]\tLoss: 0.086327\r\nTrain Epoch: 3 [19840/60000 (33%)]\tLoss: 0.089233\r\nTrain Epoch: 3 [20480/60000 (34%)]\tLoss: 0.003126\r\nTrain Epoch: 3 [21120/60000 (35%)]\tLoss: 0.092734\r\nTrain Epoch: 3 [21760/60000 (36%)]\tLoss: 0.010777\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.000975\r\nTrain Epoch: 3 [23040/60000 (38%)]\tLoss: 0.085902\r\nTrain Epoch: 3 [23680/60000 (39%)]\tLoss: 0.018550\r\nTrain Epoch: 3 [24320/60000 (41%)]\tLoss: 0.000702\r\nTrain Epoch: 3 [24960/60000 (42%)]\tLoss: 0.009240\r\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 0.051888\r\nTrain Epoch: 3 [26240/60000 (44%)]\tLoss: 0.035875\r\nTrain Epoch: 3 [26880/60000 (45%)]\tLoss: 0.036626\r\nTrain Epoch: 3 [27520/60000 (46%)]\tLoss: 0.128153\r\nTrain Epoch: 3 [28160/60000 (47%)]\tLoss: 0.092409\r\nTrain Epoch: 3 [28800/60000 (48%)]\tLoss: 0.022411\r\nTrain Epoch: 3 [29440/60000 (49%)]\tLoss: 0.012486\r\nTrain Epoch: 3 [30080/60000 (50%)]\tLoss: 0.026788\r\nTrain Epoch: 3 [30720/60000 (51%)]\tLoss: 0.085569\r\nTrain Epoch: 3 [31360/60000 (52%)]\tLoss: 0.029057\r\nTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.029598\r\nTrain Epoch: 3 [32640/60000 (54%)]\tLoss: 0.004433\r\nTrain Epoch: 3 [33280/60000 (55%)]\tLoss: 0.024579\r\nTrain Epoch: 3 [33920/60000 (57%)]\tLoss: 0.001182\r\nTrain Epoch: 3 [34560/60000 (58%)]\tLoss: 0.003459\r\nTrain Epoch: 3 [35200/60000 (59%)]\tLoss: 0.070542\r\nTrain Epoch: 3 [35840/60000 (60%)]\tLoss: 0.048433\r\nTrain Epoch: 3 [36480/60000 (61%)]\tLoss: 0.018497\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.064531\r\nTrain Epoch: 3 [37760/60000 (63%)]\tLoss: 0.044632\r\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 0.066347\r\nTrain Epoch: 3 [39040/60000 (65%)]\tLoss: 0.000999\r\nTrain Epoch: 3 [39680/60000 (66%)]\tLoss: 0.023287\r\nTrain Epoch: 3 [40320/60000 (67%)]\tLoss: 0.049301\r\nTrain Epoch: 3 [40960/60000 (68%)]\tLoss: 0.060403\r\nTrain Epoch: 3 [41600/60000 (69%)]\tLoss: 0.043240\r\nTrain Epoch: 3 [42240/60000 (70%)]\tLoss: 0.026273\r\nTrain Epoch: 3 [42880/60000 (71%)]\tLoss: 0.019598\r\nTrain Epoch: 3 [43520/60000 (72%)]\tLoss: 0.064424\r\nTrain Epoch: 3 [44160/60000 (74%)]\tLoss: 0.008217\r\nTrain Epoch: 3 [44800/60000 (75%)]\tLoss: 0.022281\r\nTrain Epoch: 3 [45440/60000 (76%)]\tLoss: 0.046030\r\nTrain Epoch: 3 [46080/60000 (77%)]\tLoss: 0.066212\r\nTrain Epoch: 3 [46720/60000 (78%)]\tLoss: 0.065459\r\nTrain Epoch: 3 [47360/60000 (79%)]\tLoss: 0.082534\r\nTrain Epoch: 3 [48000/60000 (80%)]\tLoss: 0.018311\r\nTrain Epoch: 3 [48640/60000 (81%)]\tLoss: 0.014792\r\nTrain Epoch: 3 [49280/60000 (82%)]\tLoss: 0.002591\r\nTrain Epoch: 3 [49920/60000 (83%)]\tLoss: 0.033364\r\nTrain Epoch: 3 [50560/60000 (84%)]\tLoss: 0.035610\r\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.048212\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.038437\r\nTrain Epoch: 3 [52480/60000 (87%)]\tLoss: 0.002343\r\nTrain Epoch: 3 [53120/60000 (88%)]\tLoss: 0.014847\r\nTrain Epoch: 3 [53760/60000 (90%)]\tLoss: 0.106999\r\nTrain Epoch: 3 [54400/60000 (91%)]\tLoss: 0.009365\r\nTrain Epoch: 3 [55040/60000 (92%)]\tLoss: 0.008381\r\nTrain Epoch: 3 [55680/60000 (93%)]\tLoss: 0.060557\r\nTrain Epoch: 3 [56320/60000 (94%)]\tLoss: 0.026103\r\nTrain Epoch: 3 [56960/60000 (95%)]\tLoss: 0.008004\r\nTrain Epoch: 3 [57600/60000 (96%)]\tLoss: 0.059439\r\nTrain Epoch: 3 [58240/60000 (97%)]\tLoss: 0.012732\r\nTrain Epoch: 3 [58880/60000 (98%)]\tLoss: 0.015464\r\nTrain Epoch: 3 [59520/60000 (99%)]\tLoss: 0.001325\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0353, Accuracy: 9874/10000 (99%)\r\n\r\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.016954\r\nTrain Epoch: 4 [640/60000 (1%)]\tLoss: 0.014896\r\nTrain Epoch: 4 [1280/60000 (2%)]\tLoss: 0.017627\r\nTrain Epoch: 4 [1920/60000 (3%)]\tLoss: 0.078781\r\nTrain Epoch: 4 [2560/60000 (4%)]\tLoss: 0.032063\r\nTrain Epoch: 4 [3200/60000 (5%)]\tLoss: 0.011204\r\nTrain Epoch: 4 [3840/60000 (6%)]\tLoss: 0.002311\r\nTrain Epoch: 4 [4480/60000 (7%)]\tLoss: 0.065141\r\nTrain Epoch: 4 [5120/60000 (9%)]\tLoss: 0.238773\r\nTrain Epoch: 4 [5760/60000 (10%)]\tLoss: 0.019326\r\nTrain Epoch: 4 [6400/60000 (11%)]\tLoss: 0.103883\r\nTrain Epoch: 4 [7040/60000 (12%)]\tLoss: 0.125773\r\nTrain Epoch: 4 [7680/60000 (13%)]\tLoss: 0.010834\r\nTrain Epoch: 4 [8320/60000 (14%)]\tLoss: 0.019096\r\nTrain Epoch: 4 [8960/60000 (15%)]\tLoss: 0.082827\r\nTrain Epoch: 4 [9600/60000 (16%)]\tLoss: 0.062891\r\nTrain Epoch: 4 [10240/60000 (17%)]\tLoss: 0.179991\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.021714\r\nTrain Epoch: 4 [11520/60000 (19%)]\tLoss: 0.069327\r\nTrain Epoch: 4 [12160/60000 (20%)]\tLoss: 0.008867\r\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 0.009450\r\nTrain Epoch: 4 [13440/60000 (22%)]\tLoss: 0.002137\r\nTrain Epoch: 4 [14080/60000 (23%)]\tLoss: 0.001676\r\nTrain Epoch: 4 [14720/60000 (25%)]\tLoss: 0.126089\r\nTrain Epoch: 4 [15360/60000 (26%)]\tLoss: 0.003785\r\nTrain Epoch: 4 [16000/60000 (27%)]\tLoss: 0.024527\r\nTrain Epoch: 4 [16640/60000 (28%)]\tLoss: 0.082082\r\nTrain Epoch: 4 [17280/60000 (29%)]\tLoss: 0.001569\r\nTrain Epoch: 4 [17920/60000 (30%)]\tLoss: 0.037644\r\nTrain Epoch: 4 [18560/60000 (31%)]\tLoss: 0.006633\r\nTrain Epoch: 4 [19200/60000 (32%)]\tLoss: 0.029617\r\nTrain Epoch: 4 [19840/60000 (33%)]\tLoss: 0.062031\r\nTrain Epoch: 4 [20480/60000 (34%)]\tLoss: 0.007786\r\nTrain Epoch: 4 [21120/60000 (35%)]\tLoss: 0.076232\r\nTrain Epoch: 4 [21760/60000 (36%)]\tLoss: 0.006893\r\nTrain Epoch: 4 [22400/60000 (37%)]\tLoss: 0.001348\r\nTrain Epoch: 4 [23040/60000 (38%)]\tLoss: 0.141643\r\nTrain Epoch: 4 [23680/60000 (39%)]\tLoss: 0.025766\r\nTrain Epoch: 4 [24320/60000 (41%)]\tLoss: 0.003422\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.004754\r\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 0.033668\r\nTrain Epoch: 4 [26240/60000 (44%)]\tLoss: 0.044216\r\nTrain Epoch: 4 [26880/60000 (45%)]\tLoss: 0.090532\r\nTrain Epoch: 4 [27520/60000 (46%)]\tLoss: 0.076336\r\nTrain Epoch: 4 [28160/60000 (47%)]\tLoss: 0.039177\r\nTrain Epoch: 4 [28800/60000 (48%)]\tLoss: 0.004516\r\nTrain Epoch: 4 [29440/60000 (49%)]\tLoss: 0.015426\r\nTrain Epoch: 4 [30080/60000 (50%)]\tLoss: 0.005804\r\nTrain Epoch: 4 [30720/60000 (51%)]\tLoss: 0.007853\r\nTrain Epoch: 4 [31360/60000 (52%)]\tLoss: 0.007625\r\nTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 0.135325\r\nTrain Epoch: 4 [32640/60000 (54%)]\tLoss: 0.013438\r\nTrain Epoch: 4 [33280/60000 (55%)]\tLoss: 0.018923\r\nTrain Epoch: 4 [33920/60000 (57%)]\tLoss: 0.004502\r\nTrain Epoch: 4 [34560/60000 (58%)]\tLoss: 0.029440\r\nTrain Epoch: 4 [35200/60000 (59%)]\tLoss: 0.053383\r\nTrain Epoch: 4 [35840/60000 (60%)]\tLoss: 0.018204\r\nTrain Epoch: 4 [36480/60000 (61%)]\tLoss: 0.001251\r\nTrain Epoch: 4 [37120/60000 (62%)]\tLoss: 0.045699\r\nTrain Epoch: 4 [37760/60000 (63%)]\tLoss: 0.113319\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.094273\r\nTrain Epoch: 4 [39040/60000 (65%)]\tLoss: 0.003118\r\nTrain Epoch: 4 [39680/60000 (66%)]\tLoss: 0.036022\r\nTrain Epoch: 4 [40320/60000 (67%)]\tLoss: 0.016996\r\nTrain Epoch: 4 [40960/60000 (68%)]\tLoss: 0.045760\r\nTrain Epoch: 4 [41600/60000 (69%)]\tLoss: 0.046271\r\nTrain Epoch: 4 [42240/60000 (70%)]\tLoss: 0.008908\r\nTrain Epoch: 4 [42880/60000 (71%)]\tLoss: 0.004586\r\nTrain Epoch: 4 [43520/60000 (72%)]\tLoss: 0.025216\r\nTrain Epoch: 4 [44160/60000 (74%)]\tLoss: 0.007458\r\nTrain Epoch: 4 [44800/60000 (75%)]\tLoss: 0.037676\r\nTrain Epoch: 4 [45440/60000 (76%)]\tLoss: 0.044353\r\nTrain Epoch: 4 [46080/60000 (77%)]\tLoss: 0.032816\r\nTrain Epoch: 4 [46720/60000 (78%)]\tLoss: 0.047424\r\nTrain Epoch: 4 [47360/60000 (79%)]\tLoss: 0.041935\r\nTrain Epoch: 4 [48000/60000 (80%)]\tLoss: 0.020273\r\nTrain Epoch: 4 [48640/60000 (81%)]\tLoss: 0.033879\r\nTrain Epoch: 4 [49280/60000 (82%)]\tLoss: 0.007945\r\nTrain Epoch: 4 [49920/60000 (83%)]\tLoss: 0.024864\r\nTrain Epoch: 4 [50560/60000 (84%)]\tLoss: 0.010498\r\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 0.124084\r\nTrain Epoch: 4 [51840/60000 (86%)]\tLoss: 0.010306\r\nTrain Epoch: 4 [52480/60000 (87%)]\tLoss: 0.016471\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.003740\r\nTrain Epoch: 4 [53760/60000 (90%)]\tLoss: 0.152695\r\nTrain Epoch: 4 [54400/60000 (91%)]\tLoss: 0.102813\r\nTrain Epoch: 4 [55040/60000 (92%)]\tLoss: 0.004439\r\nTrain Epoch: 4 [55680/60000 (93%)]\tLoss: 0.051224\r\nTrain Epoch: 4 [56320/60000 (94%)]\tLoss: 0.014168\r\nTrain Epoch: 4 [56960/60000 (95%)]\tLoss: 0.002459\r\nTrain Epoch: 4 [57600/60000 (96%)]\tLoss: 0.040854\r\nTrain Epoch: 4 [58240/60000 (97%)]\tLoss: 0.054161\r\nTrain Epoch: 4 [58880/60000 (98%)]\tLoss: 0.000710\r\nTrain Epoch: 4 [59520/60000 (99%)]\tLoss: 0.001639\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0313, Accuracy: 9895/10000 (99%)\r\n\r\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.008610\r\nTrain Epoch: 5 [640/60000 (1%)]\tLoss: 0.013616\r\nTrain Epoch: 5 [1280/60000 (2%)]\tLoss: 0.004592\r\nTrain Epoch: 5 [1920/60000 (3%)]\tLoss: 0.080426\r\nTrain Epoch: 5 [2560/60000 (4%)]\tLoss: 0.045325\r\nTrain Epoch: 5 [3200/60000 (5%)]\tLoss: 0.004110\r\nTrain Epoch: 5 [3840/60000 (6%)]\tLoss: 0.001141\r\nTrain Epoch: 5 [4480/60000 (7%)]\tLoss: 0.056697\r\nTrain Epoch: 5 [5120/60000 (9%)]\tLoss: 0.184885\r\nTrain Epoch: 5 [5760/60000 (10%)]\tLoss: 0.038158\r\nTrain Epoch: 5 [6400/60000 (11%)]\tLoss: 0.133815\r\nTrain Epoch: 5 [7040/60000 (12%)]\tLoss: 0.144765\r\nTrain Epoch: 5 [7680/60000 (13%)]\tLoss: 0.012555\r\nTrain Epoch: 5 [8320/60000 (14%)]\tLoss: 0.014407\r\nTrain Epoch: 5 [8960/60000 (15%)]\tLoss: 0.072581\r\nTrain Epoch: 5 [9600/60000 (16%)]\tLoss: 0.044225\r\nTrain Epoch: 5 [10240/60000 (17%)]\tLoss: 0.060217\r\nTrain Epoch: 5 [10880/60000 (18%)]\tLoss: 0.005877\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.028135\r\nTrain Epoch: 5 [12160/60000 (20%)]\tLoss: 0.016779\r\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.011359\r\nTrain Epoch: 5 [13440/60000 (22%)]\tLoss: 0.005990\r\nTrain Epoch: 5 [14080/60000 (23%)]\tLoss: 0.001807\r\nTrain Epoch: 5 [14720/60000 (25%)]\tLoss: 0.061068\r\nTrain Epoch: 5 [15360/60000 (26%)]\tLoss: 0.005222\r\nTrain Epoch: 5 [16000/60000 (27%)]\tLoss: 0.085434\r\nTrain Epoch: 5 [16640/60000 (28%)]\tLoss: 0.091211\r\nTrain Epoch: 5 [17280/60000 (29%)]\tLoss: 0.005664\r\nTrain Epoch: 5 [17920/60000 (30%)]\tLoss: 0.010164\r\nTrain Epoch: 5 [18560/60000 (31%)]\tLoss: 0.035299\r\nTrain Epoch: 5 [19200/60000 (32%)]\tLoss: 0.009893\r\nTrain Epoch: 5 [19840/60000 (33%)]\tLoss: 0.085550\r\nTrain Epoch: 5 [20480/60000 (34%)]\tLoss: 0.013833\r\nTrain Epoch: 5 [21120/60000 (35%)]\tLoss: 0.100487\r\nTrain Epoch: 5 [21760/60000 (36%)]\tLoss: 0.005029\r\nTrain Epoch: 5 [22400/60000 (37%)]\tLoss: 0.003315\r\nTrain Epoch: 5 [23040/60000 (38%)]\tLoss: 0.062545\r\nTrain Epoch: 5 [23680/60000 (39%)]\tLoss: 0.015209\r\nTrain Epoch: 5 [24320/60000 (41%)]\tLoss: 0.001976\r\nTrain Epoch: 5 [24960/60000 (42%)]\tLoss: 0.001171\r\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.015461\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.006658\r\nTrain Epoch: 5 [26880/60000 (45%)]\tLoss: 0.061723\r\nTrain Epoch: 5 [27520/60000 (46%)]\tLoss: 0.078030\r\nTrain Epoch: 5 [28160/60000 (47%)]\tLoss: 0.043976\r\nTrain Epoch: 5 [28800/60000 (48%)]\tLoss: 0.018537\r\nTrain Epoch: 5 [29440/60000 (49%)]\tLoss: 0.064546\r\nTrain Epoch: 5 [30080/60000 (50%)]\tLoss: 0.036371\r\nTrain Epoch: 5 [30720/60000 (51%)]\tLoss: 0.003494\r\nTrain Epoch: 5 [31360/60000 (52%)]\tLoss: 0.050450\r\nTrain Epoch: 5 [32000/60000 (53%)]\tLoss: 0.083736\r\nTrain Epoch: 5 [32640/60000 (54%)]\tLoss: 0.058828\r\nTrain Epoch: 5 [33280/60000 (55%)]\tLoss: 0.016122\r\nTrain Epoch: 5 [33920/60000 (57%)]\tLoss: 0.000491\r\nTrain Epoch: 5 [34560/60000 (58%)]\tLoss: 0.006345\r\nTrain Epoch: 5 [35200/60000 (59%)]\tLoss: 0.106711\r\nTrain Epoch: 5 [35840/60000 (60%)]\tLoss: 0.015227\r\nTrain Epoch: 5 [36480/60000 (61%)]\tLoss: 0.012408\r\nTrain Epoch: 5 [37120/60000 (62%)]\tLoss: 0.071871\r\nTrain Epoch: 5 [37760/60000 (63%)]\tLoss: 0.087740\r\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 0.060008\r\nTrain Epoch: 5 [39040/60000 (65%)]\tLoss: 0.003126\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.024940\r\nTrain Epoch: 5 [40320/60000 (67%)]\tLoss: 0.030930\r\nTrain Epoch: 5 [40960/60000 (68%)]\tLoss: 0.021100\r\nTrain Epoch: 5 [41600/60000 (69%)]\tLoss: 0.009124\r\nTrain Epoch: 5 [42240/60000 (70%)]\tLoss: 0.013460\r\nTrain Epoch: 5 [42880/60000 (71%)]\tLoss: 0.022530\r\nTrain Epoch: 5 [43520/60000 (72%)]\tLoss: 0.018394\r\nTrain Epoch: 5 [44160/60000 (74%)]\tLoss: 0.005177\r\nTrain Epoch: 5 [44800/60000 (75%)]\tLoss: 0.060817\r\nTrain Epoch: 5 [45440/60000 (76%)]\tLoss: 0.020513\r\nTrain Epoch: 5 [46080/60000 (77%)]\tLoss: 0.039582\r\nTrain Epoch: 5 [46720/60000 (78%)]\tLoss: 0.103885\r\nTrain Epoch: 5 [47360/60000 (79%)]\tLoss: 0.051545\r\nTrain Epoch: 5 [48000/60000 (80%)]\tLoss: 0.021612\r\nTrain Epoch: 5 [48640/60000 (81%)]\tLoss: 0.003647\r\nTrain Epoch: 5 [49280/60000 (82%)]\tLoss: 0.006669\r\nTrain Epoch: 5 [49920/60000 (83%)]\tLoss: 0.023823\r\nTrain Epoch: 5 [50560/60000 (84%)]\tLoss: 0.007305\r\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.107909\r\nTrain Epoch: 5 [51840/60000 (86%)]\tLoss: 0.001612\r\nTrain Epoch: 5 [52480/60000 (87%)]\tLoss: 0.001654\r\nTrain Epoch: 5 [53120/60000 (88%)]\tLoss: 0.045410\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.121097\r\nTrain Epoch: 5 [54400/60000 (91%)]\tLoss: 0.013893\r\nTrain Epoch: 5 [55040/60000 (92%)]\tLoss: 0.003782\r\nTrain Epoch: 5 [55680/60000 (93%)]\tLoss: 0.008787\r\nTrain Epoch: 5 [56320/60000 (94%)]\tLoss: 0.034441\r\nTrain Epoch: 5 [56960/60000 (95%)]\tLoss: 0.006220\r\nTrain Epoch: 5 [57600/60000 (96%)]\tLoss: 0.009830\r\nTrain Epoch: 5 [58240/60000 (97%)]\tLoss: 0.009454\r\nTrain Epoch: 5 [58880/60000 (98%)]\tLoss: 0.000300\r\nTrain Epoch: 5 [59520/60000 (99%)]\tLoss: 0.000553\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0298, Accuracy: 9902/10000 (99%)\r\n\r\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 0.012355\r\nTrain Epoch: 6 [640/60000 (1%)]\tLoss: 0.009176\r\nTrain Epoch: 6 [1280/60000 (2%)]\tLoss: 0.008961\r\nTrain Epoch: 6 [1920/60000 (3%)]\tLoss: 0.042873\r\nTrain Epoch: 6 [2560/60000 (4%)]\tLoss: 0.073430\r\nTrain Epoch: 6 [3200/60000 (5%)]\tLoss: 0.015642\r\nTrain Epoch: 6 [3840/60000 (6%)]\tLoss: 0.002341\r\nTrain Epoch: 6 [4480/60000 (7%)]\tLoss: 0.036673\r\nTrain Epoch: 6 [5120/60000 (9%)]\tLoss: 0.034416\r\nTrain Epoch: 6 [5760/60000 (10%)]\tLoss: 0.031803\r\nTrain Epoch: 6 [6400/60000 (11%)]\tLoss: 0.338856\r\nTrain Epoch: 6 [7040/60000 (12%)]\tLoss: 0.186388\r\nTrain Epoch: 6 [7680/60000 (13%)]\tLoss: 0.014726\r\nTrain Epoch: 6 [8320/60000 (14%)]\tLoss: 0.003458\r\nTrain Epoch: 6 [8960/60000 (15%)]\tLoss: 0.047179\r\nTrain Epoch: 6 [9600/60000 (16%)]\tLoss: 0.021056\r\nTrain Epoch: 6 [10240/60000 (17%)]\tLoss: 0.060580\r\nTrain Epoch: 6 [10880/60000 (18%)]\tLoss: 0.007121\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.008491\r\nTrain Epoch: 6 [12160/60000 (20%)]\tLoss: 0.006178\r\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.050188\r\nTrain Epoch: 6 [13440/60000 (22%)]\tLoss: 0.013657\r\nTrain Epoch: 6 [14080/60000 (23%)]\tLoss: 0.018556\r\nTrain Epoch: 6 [14720/60000 (25%)]\tLoss: 0.075935\r\nTrain Epoch: 6 [15360/60000 (26%)]\tLoss: 0.018821\r\nTrain Epoch: 6 [16000/60000 (27%)]\tLoss: 0.023635\r\nTrain Epoch: 6 [16640/60000 (28%)]\tLoss: 0.045003\r\nTrain Epoch: 6 [17280/60000 (29%)]\tLoss: 0.002473\r\nTrain Epoch: 6 [17920/60000 (30%)]\tLoss: 0.023894\r\nTrain Epoch: 6 [18560/60000 (31%)]\tLoss: 0.010475\r\nTrain Epoch: 6 [19200/60000 (32%)]\tLoss: 0.018820\r\nTrain Epoch: 6 [19840/60000 (33%)]\tLoss: 0.021363\r\nTrain Epoch: 6 [20480/60000 (34%)]\tLoss: 0.000703\r\nTrain Epoch: 6 [21120/60000 (35%)]\tLoss: 0.022988\r\nTrain Epoch: 6 [21760/60000 (36%)]\tLoss: 0.002102\r\nTrain Epoch: 6 [22400/60000 (37%)]\tLoss: 0.063657\r\nTrain Epoch: 6 [23040/60000 (38%)]\tLoss: 0.111154\r\nTrain Epoch: 6 [23680/60000 (39%)]\tLoss: 0.028492\r\nTrain Epoch: 6 [24320/60000 (41%)]\tLoss: 0.000184\r\nTrain Epoch: 6 [24960/60000 (42%)]\tLoss: 0.005334\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.018699\r\nTrain Epoch: 6 [26240/60000 (44%)]\tLoss: 0.007696\r\nTrain Epoch: 6 [26880/60000 (45%)]\tLoss: 0.026267\r\nTrain Epoch: 6 [27520/60000 (46%)]\tLoss: 0.163446\r\nTrain Epoch: 6 [28160/60000 (47%)]\tLoss: 0.119488\r\nTrain Epoch: 6 [28800/60000 (48%)]\tLoss: 0.010142\r\nTrain Epoch: 6 [29440/60000 (49%)]\tLoss: 0.044644\r\nTrain Epoch: 6 [30080/60000 (50%)]\tLoss: 0.039851\r\nTrain Epoch: 6 [30720/60000 (51%)]\tLoss: 0.020072\r\nTrain Epoch: 6 [31360/60000 (52%)]\tLoss: 0.044840\r\nTrain Epoch: 6 [32000/60000 (53%)]\tLoss: 0.011236\r\nTrain Epoch: 6 [32640/60000 (54%)]\tLoss: 0.022789\r\nTrain Epoch: 6 [33280/60000 (55%)]\tLoss: 0.016403\r\nTrain Epoch: 6 [33920/60000 (57%)]\tLoss: 0.002161\r\nTrain Epoch: 6 [34560/60000 (58%)]\tLoss: 0.012760\r\nTrain Epoch: 6 [35200/60000 (59%)]\tLoss: 0.131744\r\nTrain Epoch: 6 [35840/60000 (60%)]\tLoss: 0.004399\r\nTrain Epoch: 6 [36480/60000 (61%)]\tLoss: 0.002752\r\nTrain Epoch: 6 [37120/60000 (62%)]\tLoss: 0.038982\r\nTrain Epoch: 6 [37760/60000 (63%)]\tLoss: 0.052056\r\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 0.080173\r\nTrain Epoch: 6 [39040/60000 (65%)]\tLoss: 0.000784\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.015205\r\nTrain Epoch: 6 [40320/60000 (67%)]\tLoss: 0.013483\r\nTrain Epoch: 6 [40960/60000 (68%)]\tLoss: 0.058307\r\nTrain Epoch: 6 [41600/60000 (69%)]\tLoss: 0.011726\r\nTrain Epoch: 6 [42240/60000 (70%)]\tLoss: 0.024659\r\nTrain Epoch: 6 [42880/60000 (71%)]\tLoss: 0.001468\r\nTrain Epoch: 6 [43520/60000 (72%)]\tLoss: 0.012469\r\nTrain Epoch: 6 [44160/60000 (74%)]\tLoss: 0.007880\r\nTrain Epoch: 6 [44800/60000 (75%)]\tLoss: 0.014775\r\nTrain Epoch: 6 [45440/60000 (76%)]\tLoss: 0.013326\r\nTrain Epoch: 6 [46080/60000 (77%)]\tLoss: 0.080283\r\nTrain Epoch: 6 [46720/60000 (78%)]\tLoss: 0.068543\r\nTrain Epoch: 6 [47360/60000 (79%)]\tLoss: 0.017462\r\nTrain Epoch: 6 [48000/60000 (80%)]\tLoss: 0.038610\r\nTrain Epoch: 6 [48640/60000 (81%)]\tLoss: 0.045395\r\nTrain Epoch: 6 [49280/60000 (82%)]\tLoss: 0.020623\r\nTrain Epoch: 6 [49920/60000 (83%)]\tLoss: 0.004798\r\nTrain Epoch: 6 [50560/60000 (84%)]\tLoss: 0.015025\r\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.139915\r\nTrain Epoch: 6 [51840/60000 (86%)]\tLoss: 0.005599\r\nTrain Epoch: 6 [52480/60000 (87%)]\tLoss: 0.005762\r\nTrain Epoch: 6 [53120/60000 (88%)]\tLoss: 0.024027\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.036274\r\nTrain Epoch: 6 [54400/60000 (91%)]\tLoss: 0.005790\r\nTrain Epoch: 6 [55040/60000 (92%)]\tLoss: 0.001516\r\nTrain Epoch: 6 [55680/60000 (93%)]\tLoss: 0.088362\r\nTrain Epoch: 6 [56320/60000 (94%)]\tLoss: 0.003034\r\nTrain Epoch: 6 [56960/60000 (95%)]\tLoss: 0.001277\r\nTrain Epoch: 6 [57600/60000 (96%)]\tLoss: 0.009975\r\nTrain Epoch: 6 [58240/60000 (97%)]\tLoss: 0.018162\r\nTrain Epoch: 6 [58880/60000 (98%)]\tLoss: 0.007716\r\nTrain Epoch: 6 [59520/60000 (99%)]\tLoss: 0.000021\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0325, Accuracy: 9892/10000 (99%)\r\n\r\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.004102\r\nTrain Epoch: 7 [640/60000 (1%)]\tLoss: 0.003272\r\nTrain Epoch: 7 [1280/60000 (2%)]\tLoss: 0.016303\r\nTrain Epoch: 7 [1920/60000 (3%)]\tLoss: 0.032459\r\nTrain Epoch: 7 [2560/60000 (4%)]\tLoss: 0.015575\r\nTrain Epoch: 7 [3200/60000 (5%)]\tLoss: 0.025756\r\nTrain Epoch: 7 [3840/60000 (6%)]\tLoss: 0.001495\r\nTrain Epoch: 7 [4480/60000 (7%)]\tLoss: 0.029712\r\nTrain Epoch: 7 [5120/60000 (9%)]\tLoss: 0.046699\r\nTrain Epoch: 7 [5760/60000 (10%)]\tLoss: 0.011859\r\nTrain Epoch: 7 [6400/60000 (11%)]\tLoss: 0.181714\r\nTrain Epoch: 7 [7040/60000 (12%)]\tLoss: 0.108233\r\nTrain Epoch: 7 [7680/60000 (13%)]\tLoss: 0.014631\r\nTrain Epoch: 7 [8320/60000 (14%)]\tLoss: 0.003049\r\nTrain Epoch: 7 [8960/60000 (15%)]\tLoss: 0.005638\r\nTrain Epoch: 7 [9600/60000 (16%)]\tLoss: 0.026908\r\nTrain Epoch: 7 [10240/60000 (17%)]\tLoss: 0.128394\r\nTrain Epoch: 7 [10880/60000 (18%)]\tLoss: 0.000502\r\nTrain Epoch: 7 [11520/60000 (19%)]\tLoss: 0.002443\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.059483\r\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.023974\r\nTrain Epoch: 7 [13440/60000 (22%)]\tLoss: 0.009146\r\nTrain Epoch: 7 [14080/60000 (23%)]\tLoss: 0.007903\r\nTrain Epoch: 7 [14720/60000 (25%)]\tLoss: 0.039326\r\nTrain Epoch: 7 [15360/60000 (26%)]\tLoss: 0.009764\r\nTrain Epoch: 7 [16000/60000 (27%)]\tLoss: 0.021064\r\nTrain Epoch: 7 [16640/60000 (28%)]\tLoss: 0.049399\r\nTrain Epoch: 7 [17280/60000 (29%)]\tLoss: 0.005392\r\nTrain Epoch: 7 [17920/60000 (30%)]\tLoss: 0.002898\r\nTrain Epoch: 7 [18560/60000 (31%)]\tLoss: 0.008132\r\nTrain Epoch: 7 [19200/60000 (32%)]\tLoss: 0.011573\r\nTrain Epoch: 7 [19840/60000 (33%)]\tLoss: 0.060853\r\nTrain Epoch: 7 [20480/60000 (34%)]\tLoss: 0.007672\r\nTrain Epoch: 7 [21120/60000 (35%)]\tLoss: 0.048747\r\nTrain Epoch: 7 [21760/60000 (36%)]\tLoss: 0.053389\r\nTrain Epoch: 7 [22400/60000 (37%)]\tLoss: 0.003063\r\nTrain Epoch: 7 [23040/60000 (38%)]\tLoss: 0.056401\r\nTrain Epoch: 7 [23680/60000 (39%)]\tLoss: 0.029589\r\nTrain Epoch: 7 [24320/60000 (41%)]\tLoss: 0.001237\r\nTrain Epoch: 7 [24960/60000 (42%)]\tLoss: 0.000221\r\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.024039\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.007741\r\nTrain Epoch: 7 [26880/60000 (45%)]\tLoss: 0.024937\r\nTrain Epoch: 7 [27520/60000 (46%)]\tLoss: 0.025528\r\nTrain Epoch: 7 [28160/60000 (47%)]\tLoss: 0.006736\r\nTrain Epoch: 7 [28800/60000 (48%)]\tLoss: 0.003376\r\nTrain Epoch: 7 [29440/60000 (49%)]\tLoss: 0.014003\r\nTrain Epoch: 7 [30080/60000 (50%)]\tLoss: 0.019977\r\nTrain Epoch: 7 [30720/60000 (51%)]\tLoss: 0.005059\r\nTrain Epoch: 7 [31360/60000 (52%)]\tLoss: 0.006606\r\nTrain Epoch: 7 [32000/60000 (53%)]\tLoss: 0.055867\r\nTrain Epoch: 7 [32640/60000 (54%)]\tLoss: 0.003596\r\nTrain Epoch: 7 [33280/60000 (55%)]\tLoss: 0.016465\r\nTrain Epoch: 7 [33920/60000 (57%)]\tLoss: 0.001511\r\nTrain Epoch: 7 [34560/60000 (58%)]\tLoss: 0.044633\r\nTrain Epoch: 7 [35200/60000 (59%)]\tLoss: 0.147406\r\nTrain Epoch: 7 [35840/60000 (60%)]\tLoss: 0.029269\r\nTrain Epoch: 7 [36480/60000 (61%)]\tLoss: 0.003659\r\nTrain Epoch: 7 [37120/60000 (62%)]\tLoss: 0.013223\r\nTrain Epoch: 7 [37760/60000 (63%)]\tLoss: 0.004709\r\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 0.056562\r\nTrain Epoch: 7 [39040/60000 (65%)]\tLoss: 0.002580\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.019460\r\nTrain Epoch: 7 [40320/60000 (67%)]\tLoss: 0.005965\r\nTrain Epoch: 7 [40960/60000 (68%)]\tLoss: 0.011698\r\nTrain Epoch: 7 [41600/60000 (69%)]\tLoss: 0.015191\r\nTrain Epoch: 7 [42240/60000 (70%)]\tLoss: 0.011591\r\nTrain Epoch: 7 [42880/60000 (71%)]\tLoss: 0.002201\r\nTrain Epoch: 7 [43520/60000 (72%)]\tLoss: 0.002298\r\nTrain Epoch: 7 [44160/60000 (74%)]\tLoss: 0.000968\r\nTrain Epoch: 7 [44800/60000 (75%)]\tLoss: 0.015030\r\nTrain Epoch: 7 [45440/60000 (76%)]\tLoss: 0.013544\r\nTrain Epoch: 7 [46080/60000 (77%)]\tLoss: 0.007884\r\nTrain Epoch: 7 [46720/60000 (78%)]\tLoss: 0.071249\r\nTrain Epoch: 7 [47360/60000 (79%)]\tLoss: 0.025333\r\nTrain Epoch: 7 [48000/60000 (80%)]\tLoss: 0.005490\r\nTrain Epoch: 7 [48640/60000 (81%)]\tLoss: 0.011680\r\nTrain Epoch: 7 [49280/60000 (82%)]\tLoss: 0.007689\r\nTrain Epoch: 7 [49920/60000 (83%)]\tLoss: 0.015370\r\nTrain Epoch: 7 [50560/60000 (84%)]\tLoss: 0.050602\r\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.105428\r\nTrain Epoch: 7 [51840/60000 (86%)]\tLoss: 0.014137\r\nTrain Epoch: 7 [52480/60000 (87%)]\tLoss: 0.001505\r\nTrain Epoch: 7 [53120/60000 (88%)]\tLoss: 0.004214\r\nTrain Epoch: 7 [53760/60000 (90%)]\tLoss: 0.088213\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.005411\r\nTrain Epoch: 7 [55040/60000 (92%)]\tLoss: 0.004201\r\nTrain Epoch: 7 [55680/60000 (93%)]\tLoss: 0.012585\r\nTrain Epoch: 7 [56320/60000 (94%)]\tLoss: 0.008106\r\nTrain Epoch: 7 [56960/60000 (95%)]\tLoss: 0.000689\r\nTrain Epoch: 7 [57600/60000 (96%)]\tLoss: 0.050080\r\nTrain Epoch: 7 [58240/60000 (97%)]\tLoss: 0.004284\r\nTrain Epoch: 7 [58880/60000 (98%)]\tLoss: 0.000389\r\nTrain Epoch: 7 [59520/60000 (99%)]\tLoss: 0.000140\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0292, Accuracy: 9903/10000 (99%)\r\n\r\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.007171\r\nTrain Epoch: 8 [640/60000 (1%)]\tLoss: 0.007471\r\nTrain Epoch: 8 [1280/60000 (2%)]\tLoss: 0.020095\r\nTrain Epoch: 8 [1920/60000 (3%)]\tLoss: 0.017891\r\nTrain Epoch: 8 [2560/60000 (4%)]\tLoss: 0.007018\r\nTrain Epoch: 8 [3200/60000 (5%)]\tLoss: 0.020339\r\nTrain Epoch: 8 [3840/60000 (6%)]\tLoss: 0.004486\r\nTrain Epoch: 8 [4480/60000 (7%)]\tLoss: 0.002498\r\nTrain Epoch: 8 [5120/60000 (9%)]\tLoss: 0.081314\r\nTrain Epoch: 8 [5760/60000 (10%)]\tLoss: 0.060344\r\nTrain Epoch: 8 [6400/60000 (11%)]\tLoss: 0.127458\r\nTrain Epoch: 8 [7040/60000 (12%)]\tLoss: 0.147324\r\nTrain Epoch: 8 [7680/60000 (13%)]\tLoss: 0.016061\r\nTrain Epoch: 8 [8320/60000 (14%)]\tLoss: 0.009977\r\nTrain Epoch: 8 [8960/60000 (15%)]\tLoss: 0.006426\r\nTrain Epoch: 8 [9600/60000 (16%)]\tLoss: 0.098455\r\nTrain Epoch: 8 [10240/60000 (17%)]\tLoss: 0.047465\r\nTrain Epoch: 8 [10880/60000 (18%)]\tLoss: 0.004682\r\nTrain Epoch: 8 [11520/60000 (19%)]\tLoss: 0.026923\r\nTrain Epoch: 8 [12160/60000 (20%)]\tLoss: 0.046767\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.004828\r\nTrain Epoch: 8 [13440/60000 (22%)]\tLoss: 0.006645\r\nTrain Epoch: 8 [14080/60000 (23%)]\tLoss: 0.129955\r\nTrain Epoch: 8 [14720/60000 (25%)]\tLoss: 0.028113\r\nTrain Epoch: 8 [15360/60000 (26%)]\tLoss: 0.001407\r\nTrain Epoch: 8 [16000/60000 (27%)]\tLoss: 0.005582\r\nTrain Epoch: 8 [16640/60000 (28%)]\tLoss: 0.042599\r\nTrain Epoch: 8 [17280/60000 (29%)]\tLoss: 0.004453\r\nTrain Epoch: 8 [17920/60000 (30%)]\tLoss: 0.009554\r\nTrain Epoch: 8 [18560/60000 (31%)]\tLoss: 0.015796\r\nTrain Epoch: 8 [19200/60000 (32%)]\tLoss: 0.049120\r\nTrain Epoch: 8 [19840/60000 (33%)]\tLoss: 0.038725\r\nTrain Epoch: 8 [20480/60000 (34%)]\tLoss: 0.000674\r\nTrain Epoch: 8 [21120/60000 (35%)]\tLoss: 0.057430\r\nTrain Epoch: 8 [21760/60000 (36%)]\tLoss: 0.006726\r\nTrain Epoch: 8 [22400/60000 (37%)]\tLoss: 0.109023\r\nTrain Epoch: 8 [23040/60000 (38%)]\tLoss: 0.055652\r\nTrain Epoch: 8 [23680/60000 (39%)]\tLoss: 0.042373\r\nTrain Epoch: 8 [24320/60000 (41%)]\tLoss: 0.002446\r\nTrain Epoch: 8 [24960/60000 (42%)]\tLoss: 0.007932\r\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 0.005731\r\nTrain Epoch: 8 [26240/60000 (44%)]\tLoss: 0.002691\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.024842\r\nTrain Epoch: 8 [27520/60000 (46%)]\tLoss: 0.088866\r\nTrain Epoch: 8 [28160/60000 (47%)]\tLoss: 0.014883\r\nTrain Epoch: 8 [28800/60000 (48%)]\tLoss: 0.018430\r\nTrain Epoch: 8 [29440/60000 (49%)]\tLoss: 0.007585\r\nTrain Epoch: 8 [30080/60000 (50%)]\tLoss: 0.006305\r\nTrain Epoch: 8 [30720/60000 (51%)]\tLoss: 0.044737\r\nTrain Epoch: 8 [31360/60000 (52%)]\tLoss: 0.006416\r\nTrain Epoch: 8 [32000/60000 (53%)]\tLoss: 0.015292\r\nTrain Epoch: 8 [32640/60000 (54%)]\tLoss: 0.011853\r\nTrain Epoch: 8 [33280/60000 (55%)]\tLoss: 0.032130\r\nTrain Epoch: 8 [33920/60000 (57%)]\tLoss: 0.001041\r\nTrain Epoch: 8 [34560/60000 (58%)]\tLoss: 0.024406\r\nTrain Epoch: 8 [35200/60000 (59%)]\tLoss: 0.132065\r\nTrain Epoch: 8 [35840/60000 (60%)]\tLoss: 0.017504\r\nTrain Epoch: 8 [36480/60000 (61%)]\tLoss: 0.002158\r\nTrain Epoch: 8 [37120/60000 (62%)]\tLoss: 0.017932\r\nTrain Epoch: 8 [37760/60000 (63%)]\tLoss: 0.035419\r\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.085492\r\nTrain Epoch: 8 [39040/60000 (65%)]\tLoss: 0.006924\r\nTrain Epoch: 8 [39680/60000 (66%)]\tLoss: 0.043941\r\nTrain Epoch: 8 [40320/60000 (67%)]\tLoss: 0.006848\r\nTrain Epoch: 8 [40960/60000 (68%)]\tLoss: 0.029529\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.011003\r\nTrain Epoch: 8 [42240/60000 (70%)]\tLoss: 0.009517\r\nTrain Epoch: 8 [42880/60000 (71%)]\tLoss: 0.003475\r\nTrain Epoch: 8 [43520/60000 (72%)]\tLoss: 0.040994\r\nTrain Epoch: 8 [44160/60000 (74%)]\tLoss: 0.002957\r\nTrain Epoch: 8 [44800/60000 (75%)]\tLoss: 0.015372\r\nTrain Epoch: 8 [45440/60000 (76%)]\tLoss: 0.011436\r\nTrain Epoch: 8 [46080/60000 (77%)]\tLoss: 0.007440\r\nTrain Epoch: 8 [46720/60000 (78%)]\tLoss: 0.024746\r\nTrain Epoch: 8 [47360/60000 (79%)]\tLoss: 0.040610\r\nTrain Epoch: 8 [48000/60000 (80%)]\tLoss: 0.005733\r\nTrain Epoch: 8 [48640/60000 (81%)]\tLoss: 0.008595\r\nTrain Epoch: 8 [49280/60000 (82%)]\tLoss: 0.002690\r\nTrain Epoch: 8 [49920/60000 (83%)]\tLoss: 0.032467\r\nTrain Epoch: 8 [50560/60000 (84%)]\tLoss: 0.010269\r\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.086961\r\nTrain Epoch: 8 [51840/60000 (86%)]\tLoss: 0.000308\r\nTrain Epoch: 8 [52480/60000 (87%)]\tLoss: 0.004866\r\nTrain Epoch: 8 [53120/60000 (88%)]\tLoss: 0.002802\r\nTrain Epoch: 8 [53760/60000 (90%)]\tLoss: 0.040430\r\nTrain Epoch: 8 [54400/60000 (91%)]\tLoss: 0.016728\r\nTrain Epoch: 8 [55040/60000 (92%)]\tLoss: 0.004550\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.014813\r\nTrain Epoch: 8 [56320/60000 (94%)]\tLoss: 0.002623\r\nTrain Epoch: 8 [56960/60000 (95%)]\tLoss: 0.001894\r\nTrain Epoch: 8 [57600/60000 (96%)]\tLoss: 0.005375\r\nTrain Epoch: 8 [58240/60000 (97%)]\tLoss: 0.000736\r\nTrain Epoch: 8 [58880/60000 (98%)]\tLoss: 0.000034\r\nTrain Epoch: 8 [59520/60000 (99%)]\tLoss: 0.000435\r\n\r\nTest set: Average loss: 0.0285, Accuracy: 9905/10000 (99%)\r\n\r\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.001898\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.003195\r\nTrain Epoch: 9 [1280/60000 (2%)]\tLoss: 0.017634\r\nTrain Epoch: 9 [1920/60000 (3%)]\tLoss: 0.012691\r\nTrain Epoch: 9 [2560/60000 (4%)]\tLoss: 0.041731\r\nTrain Epoch: 9 [3200/60000 (5%)]\tLoss: 0.013325\r\nTrain Epoch: 9 [3840/60000 (6%)]\tLoss: 0.000111\r\nTrain Epoch: 9 [4480/60000 (7%)]\tLoss: 0.017672\r\nTrain Epoch: 9 [5120/60000 (9%)]\tLoss: 0.053275\r\nTrain Epoch: 9 [5760/60000 (10%)]\tLoss: 0.012993\r\nTrain Epoch: 9 [6400/60000 (11%)]\tLoss: 0.131882\r\nTrain Epoch: 9 [7040/60000 (12%)]\tLoss: 0.202044\r\nTrain Epoch: 9 [7680/60000 (13%)]\tLoss: 0.010146\r\nTrain Epoch: 9 [8320/60000 (14%)]\tLoss: 0.016342\r\nTrain Epoch: 9 [8960/60000 (15%)]\tLoss: 0.003734\r\nTrain Epoch: 9 [9600/60000 (16%)]\tLoss: 0.003695\r\nTrain Epoch: 9 [10240/60000 (17%)]\tLoss: 0.205848\r\nTrain Epoch: 9 [10880/60000 (18%)]\tLoss: 0.004869\r\nTrain Epoch: 9 [11520/60000 (19%)]\tLoss: 0.012665\r\nTrain Epoch: 9 [12160/60000 (20%)]\tLoss: 0.014581\r\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 0.074782\r\nTrain Epoch: 9 [13440/60000 (22%)]\tLoss: 0.000673\r\nTrain Epoch: 9 [14080/60000 (23%)]\tLoss: 0.016848\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.018425\r\nTrain Epoch: 9 [15360/60000 (26%)]\tLoss: 0.019676\r\nTrain Epoch: 9 [16000/60000 (27%)]\tLoss: 0.004867\r\nTrain Epoch: 9 [16640/60000 (28%)]\tLoss: 0.035778\r\nTrain Epoch: 9 [17280/60000 (29%)]\tLoss: 0.000197\r\nTrain Epoch: 9 [17920/60000 (30%)]\tLoss: 0.011570\r\nTrain Epoch: 9 [18560/60000 (31%)]\tLoss: 0.009675\r\nTrain Epoch: 9 [19200/60000 (32%)]\tLoss: 0.010041\r\nTrain Epoch: 9 [19840/60000 (33%)]\tLoss: 0.025014\r\nTrain Epoch: 9 [20480/60000 (34%)]\tLoss: 0.002077\r\nTrain Epoch: 9 [21120/60000 (35%)]\tLoss: 0.044220\r\nTrain Epoch: 9 [21760/60000 (36%)]\tLoss: 0.013230\r\nTrain Epoch: 9 [22400/60000 (37%)]\tLoss: 0.000853\r\nTrain Epoch: 9 [23040/60000 (38%)]\tLoss: 0.032751\r\nTrain Epoch: 9 [23680/60000 (39%)]\tLoss: 0.041225\r\nTrain Epoch: 9 [24320/60000 (41%)]\tLoss: 0.000234\r\nTrain Epoch: 9 [24960/60000 (42%)]\tLoss: 0.001508\r\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001058\r\nTrain Epoch: 9 [26240/60000 (44%)]\tLoss: 0.005203\r\nTrain Epoch: 9 [26880/60000 (45%)]\tLoss: 0.030368\r\nTrain Epoch: 9 [27520/60000 (46%)]\tLoss: 0.076488\r\nTrain Epoch: 9 [28160/60000 (47%)]\tLoss: 0.042985\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.027904\r\nTrain Epoch: 9 [29440/60000 (49%)]\tLoss: 0.008368\r\nTrain Epoch: 9 [30080/60000 (50%)]\tLoss: 0.011385\r\nTrain Epoch: 9 [30720/60000 (51%)]\tLoss: 0.010030\r\nTrain Epoch: 9 [31360/60000 (52%)]\tLoss: 0.002927\r\nTrain Epoch: 9 [32000/60000 (53%)]\tLoss: 0.017438\r\nTrain Epoch: 9 [32640/60000 (54%)]\tLoss: 0.066532\r\nTrain Epoch: 9 [33280/60000 (55%)]\tLoss: 0.040767\r\nTrain Epoch: 9 [33920/60000 (57%)]\tLoss: 0.000841\r\nTrain Epoch: 9 [34560/60000 (58%)]\tLoss: 0.004346\r\nTrain Epoch: 9 [35200/60000 (59%)]\tLoss: 0.099692\r\nTrain Epoch: 9 [35840/60000 (60%)]\tLoss: 0.017451\r\nTrain Epoch: 9 [36480/60000 (61%)]\tLoss: 0.005651\r\nTrain Epoch: 9 [37120/60000 (62%)]\tLoss: 0.013822\r\nTrain Epoch: 9 [37760/60000 (63%)]\tLoss: 0.020836\r\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.085602\r\nTrain Epoch: 9 [39040/60000 (65%)]\tLoss: 0.021244\r\nTrain Epoch: 9 [39680/60000 (66%)]\tLoss: 0.011511\r\nTrain Epoch: 9 [40320/60000 (67%)]\tLoss: 0.020966\r\nTrain Epoch: 9 [40960/60000 (68%)]\tLoss: 0.018003\r\nTrain Epoch: 9 [41600/60000 (69%)]\tLoss: 0.010531\r\nTrain Epoch: 9 [42240/60000 (70%)]\tLoss: 0.006091\r\nTrain Epoch: 9 [42880/60000 (71%)]\tLoss: 0.005991\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.092971\r\nTrain Epoch: 9 [44160/60000 (74%)]\tLoss: 0.002782\r\nTrain Epoch: 9 [44800/60000 (75%)]\tLoss: 0.015507\r\nTrain Epoch: 9 [45440/60000 (76%)]\tLoss: 0.006582\r\nTrain Epoch: 9 [46080/60000 (77%)]\tLoss: 0.034217\r\nTrain Epoch: 9 [46720/60000 (78%)]\tLoss: 0.050521\r\nTrain Epoch: 9 [47360/60000 (79%)]\tLoss: 0.094645\r\nTrain Epoch: 9 [48000/60000 (80%)]\tLoss: 0.004819\r\nTrain Epoch: 9 [48640/60000 (81%)]\tLoss: 0.009535\r\nTrain Epoch: 9 [49280/60000 (82%)]\tLoss: 0.001461\r\nTrain Epoch: 9 [49920/60000 (83%)]\tLoss: 0.008520\r\nTrain Epoch: 9 [50560/60000 (84%)]\tLoss: 0.018107\r\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.025137\r\nTrain Epoch: 9 [51840/60000 (86%)]\tLoss: 0.001631\r\nTrain Epoch: 9 [52480/60000 (87%)]\tLoss: 0.015115\r\nTrain Epoch: 9 [53120/60000 (88%)]\tLoss: 0.006229\r\nTrain Epoch: 9 [53760/60000 (90%)]\tLoss: 0.067584\r\nTrain Epoch: 9 [54400/60000 (91%)]\tLoss: 0.011394\r\nTrain Epoch: 9 [55040/60000 (92%)]\tLoss: 0.015050\r\nTrain Epoch: 9 [55680/60000 (93%)]\tLoss: 0.003633\r\nTrain Epoch: 9 [56320/60000 (94%)]\tLoss: 0.009590\r\nTrain Epoch: 9 [56960/60000 (95%)]\tLoss: 0.046686\r\nTrain Epoch: 9 [57600/60000 (96%)]\tLoss: 0.058662\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.003188\r\nTrain Epoch: 9 [58880/60000 (98%)]\tLoss: 0.002990\r\nTrain Epoch: 9 [59520/60000 (99%)]\tLoss: 0.000256\r\n\r\nTest set: Average loss: 0.0277, Accuracy: 9915/10000 (99%)\r\n\r\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.004911\r\nTrain Epoch: 10 [640/60000 (1%)]\tLoss: 0.004714\r\nTrain Epoch: 10 [1280/60000 (2%)]\tLoss: 0.053329\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.014490\r\nTrain Epoch: 10 [2560/60000 (4%)]\tLoss: 0.001621\r\nTrain Epoch: 10 [3200/60000 (5%)]\tLoss: 0.003457\r\nTrain Epoch: 10 [3840/60000 (6%)]\tLoss: 0.001952\r\nTrain Epoch: 10 [4480/60000 (7%)]\tLoss: 0.005528\r\nTrain Epoch: 10 [5120/60000 (9%)]\tLoss: 0.048126\r\nTrain Epoch: 10 [5760/60000 (10%)]\tLoss: 0.062621\r\nTrain Epoch: 10 [6400/60000 (11%)]\tLoss: 0.091591\r\nTrain Epoch: 10 [7040/60000 (12%)]\tLoss: 0.144053\r\nTrain Epoch: 10 [7680/60000 (13%)]\tLoss: 0.052804\r\nTrain Epoch: 10 [8320/60000 (14%)]\tLoss: 0.001518\r\nTrain Epoch: 10 [8960/60000 (15%)]\tLoss: 0.075341\r\nTrain Epoch: 10 [9600/60000 (16%)]\tLoss: 0.147038\r\nTrain Epoch: 10 [10240/60000 (17%)]\tLoss: 0.058992\r\nTrain Epoch: 10 [10880/60000 (18%)]\tLoss: 0.002717\r\nTrain Epoch: 10 [11520/60000 (19%)]\tLoss: 0.055562\r\nTrain Epoch: 10 [12160/60000 (20%)]\tLoss: 0.007078\r\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 0.012960\r\nTrain Epoch: 10 [13440/60000 (22%)]\tLoss: 0.000777\r\nTrain Epoch: 10 [14080/60000 (23%)]\tLoss: 0.000852\r\nTrain Epoch: 10 [14720/60000 (25%)]\tLoss: 0.021872\r\nTrain Epoch: 10 [15360/60000 (26%)]\tLoss: 0.004109\r\nTrain Epoch: 10 [16000/60000 (27%)]\tLoss: 0.020662\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.119444\r\nTrain Epoch: 10 [17280/60000 (29%)]\tLoss: 0.002866\r\nTrain Epoch: 10 [17920/60000 (30%)]\tLoss: 0.015122\r\nTrain Epoch: 10 [18560/60000 (31%)]\tLoss: 0.021461\r\nTrain Epoch: 10 [19200/60000 (32%)]\tLoss: 0.032696\r\nTrain Epoch: 10 [19840/60000 (33%)]\tLoss: 0.102248\r\nTrain Epoch: 10 [20480/60000 (34%)]\tLoss: 0.001606\r\nTrain Epoch: 10 [21120/60000 (35%)]\tLoss: 0.087760\r\nTrain Epoch: 10 [21760/60000 (36%)]\tLoss: 0.043507\r\nTrain Epoch: 10 [22400/60000 (37%)]\tLoss: 0.004563\r\nTrain Epoch: 10 [23040/60000 (38%)]\tLoss: 0.006575\r\nTrain Epoch: 10 [23680/60000 (39%)]\tLoss: 0.047741\r\nTrain Epoch: 10 [24320/60000 (41%)]\tLoss: 0.002187\r\nTrain Epoch: 10 [24960/60000 (42%)]\tLoss: 0.000167\r\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.008723\r\nTrain Epoch: 10 [26240/60000 (44%)]\tLoss: 0.003698\r\nTrain Epoch: 10 [26880/60000 (45%)]\tLoss: 0.027679\r\nTrain Epoch: 10 [27520/60000 (46%)]\tLoss: 0.034176\r\nTrain Epoch: 10 [28160/60000 (47%)]\tLoss: 0.030329\r\nTrain Epoch: 10 [28800/60000 (48%)]\tLoss: 0.002240\r\nTrain Epoch: 10 [29440/60000 (49%)]\tLoss: 0.010606\r\nTrain Epoch: 10 [30080/60000 (50%)]\tLoss: 0.026845\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.026229\r\nTrain Epoch: 10 [31360/60000 (52%)]\tLoss: 0.005288\r\nTrain Epoch: 10 [32000/60000 (53%)]\tLoss: 0.003524\r\nTrain Epoch: 10 [32640/60000 (54%)]\tLoss: 0.002558\r\nTrain Epoch: 10 [33280/60000 (55%)]\tLoss: 0.019170\r\nTrain Epoch: 10 [33920/60000 (57%)]\tLoss: 0.000348\r\nTrain Epoch: 10 [34560/60000 (58%)]\tLoss: 0.018711\r\nTrain Epoch: 10 [35200/60000 (59%)]\tLoss: 0.110968\r\nTrain Epoch: 10 [35840/60000 (60%)]\tLoss: 0.075565\r\nTrain Epoch: 10 [36480/60000 (61%)]\tLoss: 0.021187\r\nTrain Epoch: 10 [37120/60000 (62%)]\tLoss: 0.009722\r\nTrain Epoch: 10 [37760/60000 (63%)]\tLoss: 0.073936\r\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.054965\r\nTrain Epoch: 10 [39040/60000 (65%)]\tLoss: 0.006879\r\nTrain Epoch: 10 [39680/60000 (66%)]\tLoss: 0.006144\r\nTrain Epoch: 10 [40320/60000 (67%)]\tLoss: 0.009288\r\nTrain Epoch: 10 [40960/60000 (68%)]\tLoss: 0.046621\r\nTrain Epoch: 10 [41600/60000 (69%)]\tLoss: 0.003466\r\nTrain Epoch: 10 [42240/60000 (70%)]\tLoss: 0.025517\r\nTrain Epoch: 10 [42880/60000 (71%)]\tLoss: 0.001413\r\nTrain Epoch: 10 [43520/60000 (72%)]\tLoss: 0.076675\r\nTrain Epoch: 10 [44160/60000 (74%)]\tLoss: 0.001476\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.080734\r\nTrain Epoch: 10 [45440/60000 (76%)]\tLoss: 0.035257\r\nTrain Epoch: 10 [46080/60000 (77%)]\tLoss: 0.013394\r\nTrain Epoch: 10 [46720/60000 (78%)]\tLoss: 0.054960\r\nTrain Epoch: 10 [47360/60000 (79%)]\tLoss: 0.041754\r\nTrain Epoch: 10 [48000/60000 (80%)]\tLoss: 0.017145\r\nTrain Epoch: 10 [48640/60000 (81%)]\tLoss: 0.011733\r\nTrain Epoch: 10 [49280/60000 (82%)]\tLoss: 0.027643\r\nTrain Epoch: 10 [49920/60000 (83%)]\tLoss: 0.013767\r\nTrain Epoch: 10 [50560/60000 (84%)]\tLoss: 0.040930\r\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.091910\r\nTrain Epoch: 10 [51840/60000 (86%)]\tLoss: 0.001426\r\nTrain Epoch: 10 [52480/60000 (87%)]\tLoss: 0.000930\r\nTrain Epoch: 10 [53120/60000 (88%)]\tLoss: 0.047275\r\nTrain Epoch: 10 [53760/60000 (90%)]\tLoss: 0.062888\r\nTrain Epoch: 10 [54400/60000 (91%)]\tLoss: 0.048673\r\nTrain Epoch: 10 [55040/60000 (92%)]\tLoss: 0.007231\r\nTrain Epoch: 10 [55680/60000 (93%)]\tLoss: 0.014935\r\nTrain Epoch: 10 [56320/60000 (94%)]\tLoss: 0.007433\r\nTrain Epoch: 10 [56960/60000 (95%)]\tLoss: 0.003697\r\nTrain Epoch: 10 [57600/60000 (96%)]\tLoss: 0.007743\r\nTrain Epoch: 10 [58240/60000 (97%)]\tLoss: 0.003051\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.000516\r\nTrain Epoch: 10 [59520/60000 (99%)]\tLoss: 0.000501\r\n\r\nTest set: Average loss: 0.0276, Accuracy: 9912/10000 (99%)\r\n\r\nTrain Epoch: 11 [0/60000 (0%)]\tLoss: 0.021224\r\nTrain Epoch: 11 [640/60000 (1%)]\tLoss: 0.001821\r\nTrain Epoch: 11 [1280/60000 (2%)]\tLoss: 0.009626\r\nTrain Epoch: 11 [1920/60000 (3%)]\tLoss: 0.027796\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.001811\r\nTrain Epoch: 11 [3200/60000 (5%)]\tLoss: 0.002942\r\nTrain Epoch: 11 [3840/60000 (6%)]\tLoss: 0.001105\r\nTrain Epoch: 11 [4480/60000 (7%)]\tLoss: 0.004295\r\nTrain Epoch: 11 [5120/60000 (9%)]\tLoss: 0.014260\r\nTrain Epoch: 11 [5760/60000 (10%)]\tLoss: 0.088293\r\nTrain Epoch: 11 [6400/60000 (11%)]\tLoss: 0.105729\r\nTrain Epoch: 11 [7040/60000 (12%)]\tLoss: 0.172894\r\nTrain Epoch: 11 [7680/60000 (13%)]\tLoss: 0.019626\r\nTrain Epoch: 11 [8320/60000 (14%)]\tLoss: 0.002626\r\nTrain Epoch: 11 [8960/60000 (15%)]\tLoss: 0.099489\r\nTrain Epoch: 11 [9600/60000 (16%)]\tLoss: 0.022632\r\nTrain Epoch: 11 [10240/60000 (17%)]\tLoss: 0.056683\r\nTrain Epoch: 11 [10880/60000 (18%)]\tLoss: 0.002863\r\nTrain Epoch: 11 [11520/60000 (19%)]\tLoss: 0.023036\r\nTrain Epoch: 11 [12160/60000 (20%)]\tLoss: 0.020021\r\nTrain Epoch: 11 [12800/60000 (21%)]\tLoss: 0.030162\r\nTrain Epoch: 11 [13440/60000 (22%)]\tLoss: 0.007165\r\nTrain Epoch: 11 [14080/60000 (23%)]\tLoss: 0.012681\r\nTrain Epoch: 11 [14720/60000 (25%)]\tLoss: 0.060500\r\nTrain Epoch: 11 [15360/60000 (26%)]\tLoss: 0.002498\r\nTrain Epoch: 11 [16000/60000 (27%)]\tLoss: 0.063292\r\nTrain Epoch: 11 [16640/60000 (28%)]\tLoss: 0.034786\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.000629\r\nTrain Epoch: 11 [17920/60000 (30%)]\tLoss: 0.015184\r\nTrain Epoch: 11 [18560/60000 (31%)]\tLoss: 0.007634\r\nTrain Epoch: 11 [19200/60000 (32%)]\tLoss: 0.005090\r\nTrain Epoch: 11 [19840/60000 (33%)]\tLoss: 0.026164\r\nTrain Epoch: 11 [20480/60000 (34%)]\tLoss: 0.001653\r\nTrain Epoch: 11 [21120/60000 (35%)]\tLoss: 0.045288\r\nTrain Epoch: 11 [21760/60000 (36%)]\tLoss: 0.005102\r\nTrain Epoch: 11 [22400/60000 (37%)]\tLoss: 0.007836\r\nTrain Epoch: 11 [23040/60000 (38%)]\tLoss: 0.019077\r\nTrain Epoch: 11 [23680/60000 (39%)]\tLoss: 0.011138\r\nTrain Epoch: 11 [24320/60000 (41%)]\tLoss: 0.014572\r\nTrain Epoch: 11 [24960/60000 (42%)]\tLoss: 0.001537\r\nTrain Epoch: 11 [25600/60000 (43%)]\tLoss: 0.013356\r\nTrain Epoch: 11 [26240/60000 (44%)]\tLoss: 0.007508\r\nTrain Epoch: 11 [26880/60000 (45%)]\tLoss: 0.022103\r\nTrain Epoch: 11 [27520/60000 (46%)]\tLoss: 0.032073\r\nTrain Epoch: 11 [28160/60000 (47%)]\tLoss: 0.003012\r\nTrain Epoch: 11 [28800/60000 (48%)]\tLoss: 0.016765\r\nTrain Epoch: 11 [29440/60000 (49%)]\tLoss: 0.012846\r\nTrain Epoch: 11 [30080/60000 (50%)]\tLoss: 0.006219\r\nTrain Epoch: 11 [30720/60000 (51%)]\tLoss: 0.004804\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.007015\r\nTrain Epoch: 11 [32000/60000 (53%)]\tLoss: 0.015884\r\nTrain Epoch: 11 [32640/60000 (54%)]\tLoss: 0.025590\r\nTrain Epoch: 11 [33280/60000 (55%)]\tLoss: 0.018325\r\nTrain Epoch: 11 [33920/60000 (57%)]\tLoss: 0.001051\r\nTrain Epoch: 11 [34560/60000 (58%)]\tLoss: 0.002549\r\nTrain Epoch: 11 [35200/60000 (59%)]\tLoss: 0.124128\r\nTrain Epoch: 11 [35840/60000 (60%)]\tLoss: 0.004573\r\nTrain Epoch: 11 [36480/60000 (61%)]\tLoss: 0.038798\r\nTrain Epoch: 11 [37120/60000 (62%)]\tLoss: 0.028478\r\nTrain Epoch: 11 [37760/60000 (63%)]\tLoss: 0.069545\r\nTrain Epoch: 11 [38400/60000 (64%)]\tLoss: 0.055610\r\nTrain Epoch: 11 [39040/60000 (65%)]\tLoss: 0.001986\r\nTrain Epoch: 11 [39680/60000 (66%)]\tLoss: 0.024364\r\nTrain Epoch: 11 [40320/60000 (67%)]\tLoss: 0.003392\r\nTrain Epoch: 11 [40960/60000 (68%)]\tLoss: 0.034626\r\nTrain Epoch: 11 [41600/60000 (69%)]\tLoss: 0.002870\r\nTrain Epoch: 11 [42240/60000 (70%)]\tLoss: 0.017867\r\nTrain Epoch: 11 [42880/60000 (71%)]\tLoss: 0.044863\r\nTrain Epoch: 11 [43520/60000 (72%)]\tLoss: 0.033598\r\nTrain Epoch: 11 [44160/60000 (74%)]\tLoss: 0.000687\r\nTrain Epoch: 11 [44800/60000 (75%)]\tLoss: 0.034112\r\nTrain Epoch: 11 [45440/60000 (76%)]\tLoss: 0.037071\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.005589\r\nTrain Epoch: 11 [46720/60000 (78%)]\tLoss: 0.031180\r\nTrain Epoch: 11 [47360/60000 (79%)]\tLoss: 0.022799\r\nTrain Epoch: 11 [48000/60000 (80%)]\tLoss: 0.005887\r\nTrain Epoch: 11 [48640/60000 (81%)]\tLoss: 0.004613\r\nTrain Epoch: 11 [49280/60000 (82%)]\tLoss: 0.035647\r\nTrain Epoch: 11 [49920/60000 (83%)]\tLoss: 0.009813\r\nTrain Epoch: 11 [50560/60000 (84%)]\tLoss: 0.006748\r\nTrain Epoch: 11 [51200/60000 (85%)]\tLoss: 0.019169\r\nTrain Epoch: 11 [51840/60000 (86%)]\tLoss: 0.002413\r\nTrain Epoch: 11 [52480/60000 (87%)]\tLoss: 0.005018\r\nTrain Epoch: 11 [53120/60000 (88%)]\tLoss: 0.004109\r\nTrain Epoch: 11 [53760/60000 (90%)]\tLoss: 0.044968\r\nTrain Epoch: 11 [54400/60000 (91%)]\tLoss: 0.039511\r\nTrain Epoch: 11 [55040/60000 (92%)]\tLoss: 0.013302\r\nTrain Epoch: 11 [55680/60000 (93%)]\tLoss: 0.037255\r\nTrain Epoch: 11 [56320/60000 (94%)]\tLoss: 0.050338\r\nTrain Epoch: 11 [56960/60000 (95%)]\tLoss: 0.005812\r\nTrain Epoch: 11 [57600/60000 (96%)]\tLoss: 0.002453\r\nTrain Epoch: 11 [58240/60000 (97%)]\tLoss: 0.045186\r\nTrain Epoch: 11 [58880/60000 (98%)]\tLoss: 0.000218\r\nTrain Epoch: 11 [59520/60000 (99%)]\tLoss: 0.000161\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0279, Accuracy: 9909/10000 (99%)\r\n\r\nTrain Epoch: 12 [0/60000 (0%)]\tLoss: 0.003117\r\nTrain Epoch: 12 [640/60000 (1%)]\tLoss: 0.001867\r\nTrain Epoch: 12 [1280/60000 (2%)]\tLoss: 0.008355\r\nTrain Epoch: 12 [1920/60000 (3%)]\tLoss: 0.007711\r\nTrain Epoch: 12 [2560/60000 (4%)]\tLoss: 0.016372\r\nTrain Epoch: 12 [3200/60000 (5%)]\tLoss: 0.011065\r\nTrain Epoch: 12 [3840/60000 (6%)]\tLoss: 0.000227\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.004001\r\nTrain Epoch: 12 [5120/60000 (9%)]\tLoss: 0.049129\r\nTrain Epoch: 12 [5760/60000 (10%)]\tLoss: 0.013787\r\nTrain Epoch: 12 [6400/60000 (11%)]\tLoss: 0.195075\r\nTrain Epoch: 12 [7040/60000 (12%)]\tLoss: 0.167475\r\nTrain Epoch: 12 [7680/60000 (13%)]\tLoss: 0.002004\r\nTrain Epoch: 12 [8320/60000 (14%)]\tLoss: 0.003897\r\nTrain Epoch: 12 [8960/60000 (15%)]\tLoss: 0.023590\r\nTrain Epoch: 12 [9600/60000 (16%)]\tLoss: 0.019106\r\nTrain Epoch: 12 [10240/60000 (17%)]\tLoss: 0.054332\r\nTrain Epoch: 12 [10880/60000 (18%)]\tLoss: 0.016759\r\nTrain Epoch: 12 [11520/60000 (19%)]\tLoss: 0.079585\r\nTrain Epoch: 12 [12160/60000 (20%)]\tLoss: 0.013900\r\nTrain Epoch: 12 [12800/60000 (21%)]\tLoss: 0.088880\r\nTrain Epoch: 12 [13440/60000 (22%)]\tLoss: 0.000416\r\nTrain Epoch: 12 [14080/60000 (23%)]\tLoss: 0.004917\r\nTrain Epoch: 12 [14720/60000 (25%)]\tLoss: 0.007029\r\nTrain Epoch: 12 [15360/60000 (26%)]\tLoss: 0.024546\r\nTrain Epoch: 12 [16000/60000 (27%)]\tLoss: 0.029377\r\nTrain Epoch: 12 [16640/60000 (28%)]\tLoss: 0.040745\r\nTrain Epoch: 12 [17280/60000 (29%)]\tLoss: 0.000282\r\nTrain Epoch: 12 [17920/60000 (30%)]\tLoss: 0.002436\r\nTrain Epoch: 12 [18560/60000 (31%)]\tLoss: 0.044622\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.006582\r\nTrain Epoch: 12 [19840/60000 (33%)]\tLoss: 0.043941\r\nTrain Epoch: 12 [20480/60000 (34%)]\tLoss: 0.002436\r\nTrain Epoch: 12 [21120/60000 (35%)]\tLoss: 0.045319\r\nTrain Epoch: 12 [21760/60000 (36%)]\tLoss: 0.009352\r\nTrain Epoch: 12 [22400/60000 (37%)]\tLoss: 0.000696\r\nTrain Epoch: 12 [23040/60000 (38%)]\tLoss: 0.007506\r\nTrain Epoch: 12 [23680/60000 (39%)]\tLoss: 0.046933\r\nTrain Epoch: 12 [24320/60000 (41%)]\tLoss: 0.000867\r\nTrain Epoch: 12 [24960/60000 (42%)]\tLoss: 0.001087\r\nTrain Epoch: 12 [25600/60000 (43%)]\tLoss: 0.011645\r\nTrain Epoch: 12 [26240/60000 (44%)]\tLoss: 0.005148\r\nTrain Epoch: 12 [26880/60000 (45%)]\tLoss: 0.017921\r\nTrain Epoch: 12 [27520/60000 (46%)]\tLoss: 0.107948\r\nTrain Epoch: 12 [28160/60000 (47%)]\tLoss: 0.010032\r\nTrain Epoch: 12 [28800/60000 (48%)]\tLoss: 0.005590\r\nTrain Epoch: 12 [29440/60000 (49%)]\tLoss: 0.015512\r\nTrain Epoch: 12 [30080/60000 (50%)]\tLoss: 0.002400\r\nTrain Epoch: 12 [30720/60000 (51%)]\tLoss: 0.001940\r\nTrain Epoch: 12 [31360/60000 (52%)]\tLoss: 0.010087\r\nTrain Epoch: 12 [32000/60000 (53%)]\tLoss: 0.013695\r\nTrain Epoch: 12 [32640/60000 (54%)]\tLoss: 0.000776\r\nTrain Epoch: 12 [33280/60000 (55%)]\tLoss: 0.020436\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.003071\r\nTrain Epoch: 12 [34560/60000 (58%)]\tLoss: 0.001165\r\nTrain Epoch: 12 [35200/60000 (59%)]\tLoss: 0.049087\r\nTrain Epoch: 12 [35840/60000 (60%)]\tLoss: 0.006825\r\nTrain Epoch: 12 [36480/60000 (61%)]\tLoss: 0.002466\r\nTrain Epoch: 12 [37120/60000 (62%)]\tLoss: 0.004363\r\nTrain Epoch: 12 [37760/60000 (63%)]\tLoss: 0.045194\r\nTrain Epoch: 12 [38400/60000 (64%)]\tLoss: 0.016294\r\nTrain Epoch: 12 [39040/60000 (65%)]\tLoss: 0.002674\r\nTrain Epoch: 12 [39680/60000 (66%)]\tLoss: 0.003674\r\nTrain Epoch: 12 [40320/60000 (67%)]\tLoss: 0.057260\r\nTrain Epoch: 12 [40960/60000 (68%)]\tLoss: 0.091592\r\nTrain Epoch: 12 [41600/60000 (69%)]\tLoss: 0.001442\r\nTrain Epoch: 12 [42240/60000 (70%)]\tLoss: 0.003500\r\nTrain Epoch: 12 [42880/60000 (71%)]\tLoss: 0.003408\r\nTrain Epoch: 12 [43520/60000 (72%)]\tLoss: 0.019217\r\nTrain Epoch: 12 [44160/60000 (74%)]\tLoss: 0.005751\r\nTrain Epoch: 12 [44800/60000 (75%)]\tLoss: 0.002860\r\nTrain Epoch: 12 [45440/60000 (76%)]\tLoss: 0.015529\r\nTrain Epoch: 12 [46080/60000 (77%)]\tLoss: 0.019046\r\nTrain Epoch: 12 [46720/60000 (78%)]\tLoss: 0.042518\r\nTrain Epoch: 12 [47360/60000 (79%)]\tLoss: 0.041918\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.010896\r\nTrain Epoch: 12 [48640/60000 (81%)]\tLoss: 0.019460\r\nTrain Epoch: 12 [49280/60000 (82%)]\tLoss: 0.003091\r\nTrain Epoch: 12 [49920/60000 (83%)]\tLoss: 0.002485\r\nTrain Epoch: 12 [50560/60000 (84%)]\tLoss: 0.007453\r\nTrain Epoch: 12 [51200/60000 (85%)]\tLoss: 0.061551\r\nTrain Epoch: 12 [51840/60000 (86%)]\tLoss: 0.004429\r\nTrain Epoch: 12 [52480/60000 (87%)]\tLoss: 0.002757\r\nTrain Epoch: 12 [53120/60000 (88%)]\tLoss: 0.007573\r\nTrain Epoch: 12 [53760/60000 (90%)]\tLoss: 0.008524\r\nTrain Epoch: 12 [54400/60000 (91%)]\tLoss: 0.010851\r\nTrain Epoch: 12 [55040/60000 (92%)]\tLoss: 0.001302\r\nTrain Epoch: 12 [55680/60000 (93%)]\tLoss: 0.025935\r\nTrain Epoch: 12 [56320/60000 (94%)]\tLoss: 0.024573\r\nTrain Epoch: 12 [56960/60000 (95%)]\tLoss: 0.002293\r\nTrain Epoch: 12 [57600/60000 (96%)]\tLoss: 0.055000\r\nTrain Epoch: 12 [58240/60000 (97%)]\tLoss: 0.002222\r\nTrain Epoch: 12 [58880/60000 (98%)]\tLoss: 0.002859\r\nTrain Epoch: 12 [59520/60000 (99%)]\tLoss: 0.000468\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0269, Accuracy: 9912/10000 (99%)\r\n\r\nTrain Epoch: 13 [0/60000 (0%)]\tLoss: 0.009951\r\nTrain Epoch: 13 [640/60000 (1%)]\tLoss: 0.007301\r\nTrain Epoch: 13 [1280/60000 (2%)]\tLoss: 0.003980\r\nTrain Epoch: 13 [1920/60000 (3%)]\tLoss: 0.018733\r\nTrain Epoch: 13 [2560/60000 (4%)]\tLoss: 0.001357\r\nTrain Epoch: 13 [3200/60000 (5%)]\tLoss: 0.003559\r\nTrain Epoch: 13 [3840/60000 (6%)]\tLoss: 0.001059\r\nTrain Epoch: 13 [4480/60000 (7%)]\tLoss: 0.001210\r\nTrain Epoch: 13 [5120/60000 (9%)]\tLoss: 0.162786\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.004084\r\nTrain Epoch: 13 [6400/60000 (11%)]\tLoss: 0.021091\r\nTrain Epoch: 13 [7040/60000 (12%)]\tLoss: 0.198883\r\nTrain Epoch: 13 [7680/60000 (13%)]\tLoss: 0.048268\r\nTrain Epoch: 13 [8320/60000 (14%)]\tLoss: 0.027622\r\nTrain Epoch: 13 [8960/60000 (15%)]\tLoss: 0.016420\r\nTrain Epoch: 13 [9600/60000 (16%)]\tLoss: 0.022515\r\nTrain Epoch: 13 [10240/60000 (17%)]\tLoss: 0.083788\r\nTrain Epoch: 13 [10880/60000 (18%)]\tLoss: 0.000545\r\nTrain Epoch: 13 [11520/60000 (19%)]\tLoss: 0.027371\r\nTrain Epoch: 13 [12160/60000 (20%)]\tLoss: 0.010828\r\nTrain Epoch: 13 [12800/60000 (21%)]\tLoss: 0.080491\r\nTrain Epoch: 13 [13440/60000 (22%)]\tLoss: 0.000689\r\nTrain Epoch: 13 [14080/60000 (23%)]\tLoss: 0.009789\r\nTrain Epoch: 13 [14720/60000 (25%)]\tLoss: 0.057977\r\nTrain Epoch: 13 [15360/60000 (26%)]\tLoss: 0.000340\r\nTrain Epoch: 13 [16000/60000 (27%)]\tLoss: 0.028923\r\nTrain Epoch: 13 [16640/60000 (28%)]\tLoss: 0.054953\r\nTrain Epoch: 13 [17280/60000 (29%)]\tLoss: 0.000140\r\nTrain Epoch: 13 [17920/60000 (30%)]\tLoss: 0.001591\r\nTrain Epoch: 13 [18560/60000 (31%)]\tLoss: 0.016495\r\nTrain Epoch: 13 [19200/60000 (32%)]\tLoss: 0.050562\r\nTrain Epoch: 13 [19840/60000 (33%)]\tLoss: 0.022582\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.001857\r\nTrain Epoch: 13 [21120/60000 (35%)]\tLoss: 0.014521\r\nTrain Epoch: 13 [21760/60000 (36%)]\tLoss: 0.065817\r\nTrain Epoch: 13 [22400/60000 (37%)]\tLoss: 0.002592\r\nTrain Epoch: 13 [23040/60000 (38%)]\tLoss: 0.060742\r\nTrain Epoch: 13 [23680/60000 (39%)]\tLoss: 0.015665\r\nTrain Epoch: 13 [24320/60000 (41%)]\tLoss: 0.002044\r\nTrain Epoch: 13 [24960/60000 (42%)]\tLoss: 0.001605\r\nTrain Epoch: 13 [25600/60000 (43%)]\tLoss: 0.003798\r\nTrain Epoch: 13 [26240/60000 (44%)]\tLoss: 0.061673\r\nTrain Epoch: 13 [26880/60000 (45%)]\tLoss: 0.019762\r\nTrain Epoch: 13 [27520/60000 (46%)]\tLoss: 0.051875\r\nTrain Epoch: 13 [28160/60000 (47%)]\tLoss: 0.056123\r\nTrain Epoch: 13 [28800/60000 (48%)]\tLoss: 0.002044\r\nTrain Epoch: 13 [29440/60000 (49%)]\tLoss: 0.045411\r\nTrain Epoch: 13 [30080/60000 (50%)]\tLoss: 0.006584\r\nTrain Epoch: 13 [30720/60000 (51%)]\tLoss: 0.028374\r\nTrain Epoch: 13 [31360/60000 (52%)]\tLoss: 0.002950\r\nTrain Epoch: 13 [32000/60000 (53%)]\tLoss: 0.028537\r\nTrain Epoch: 13 [32640/60000 (54%)]\tLoss: 0.004859\r\nTrain Epoch: 13 [33280/60000 (55%)]\tLoss: 0.011867\r\nTrain Epoch: 13 [33920/60000 (57%)]\tLoss: 0.001649\r\nTrain Epoch: 13 [34560/60000 (58%)]\tLoss: 0.021104\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.135352\r\nTrain Epoch: 13 [35840/60000 (60%)]\tLoss: 0.044590\r\nTrain Epoch: 13 [36480/60000 (61%)]\tLoss: 0.005236\r\nTrain Epoch: 13 [37120/60000 (62%)]\tLoss: 0.023858\r\nTrain Epoch: 13 [37760/60000 (63%)]\tLoss: 0.029911\r\nTrain Epoch: 13 [38400/60000 (64%)]\tLoss: 0.060795\r\nTrain Epoch: 13 [39040/60000 (65%)]\tLoss: 0.000496\r\nTrain Epoch: 13 [39680/60000 (66%)]\tLoss: 0.015166\r\nTrain Epoch: 13 [40320/60000 (67%)]\tLoss: 0.035257\r\nTrain Epoch: 13 [40960/60000 (68%)]\tLoss: 0.039872\r\nTrain Epoch: 13 [41600/60000 (69%)]\tLoss: 0.019796\r\nTrain Epoch: 13 [42240/60000 (70%)]\tLoss: 0.027580\r\nTrain Epoch: 13 [42880/60000 (71%)]\tLoss: 0.000812\r\nTrain Epoch: 13 [43520/60000 (72%)]\tLoss: 0.037562\r\nTrain Epoch: 13 [44160/60000 (74%)]\tLoss: 0.001272\r\nTrain Epoch: 13 [44800/60000 (75%)]\tLoss: 0.016902\r\nTrain Epoch: 13 [45440/60000 (76%)]\tLoss: 0.003902\r\nTrain Epoch: 13 [46080/60000 (77%)]\tLoss: 0.012064\r\nTrain Epoch: 13 [46720/60000 (78%)]\tLoss: 0.019447\r\nTrain Epoch: 13 [47360/60000 (79%)]\tLoss: 0.081657\r\nTrain Epoch: 13 [48000/60000 (80%)]\tLoss: 0.004399\r\nTrain Epoch: 13 [48640/60000 (81%)]\tLoss: 0.002241\r\nTrain Epoch: 13 [49280/60000 (82%)]\tLoss: 0.065215\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.009592\r\nTrain Epoch: 13 [50560/60000 (84%)]\tLoss: 0.065594\r\nTrain Epoch: 13 [51200/60000 (85%)]\tLoss: 0.024048\r\nTrain Epoch: 13 [51840/60000 (86%)]\tLoss: 0.003979\r\nTrain Epoch: 13 [52480/60000 (87%)]\tLoss: 0.004266\r\nTrain Epoch: 13 [53120/60000 (88%)]\tLoss: 0.024286\r\nTrain Epoch: 13 [53760/60000 (90%)]\tLoss: 0.109197\r\nTrain Epoch: 13 [54400/60000 (91%)]\tLoss: 0.017441\r\nTrain Epoch: 13 [55040/60000 (92%)]\tLoss: 0.006755\r\nTrain Epoch: 13 [55680/60000 (93%)]\tLoss: 0.019546\r\nTrain Epoch: 13 [56320/60000 (94%)]\tLoss: 0.001008\r\nTrain Epoch: 13 [56960/60000 (95%)]\tLoss: 0.004209\r\nTrain Epoch: 13 [57600/60000 (96%)]\tLoss: 0.030113\r\nTrain Epoch: 13 [58240/60000 (97%)]\tLoss: 0.004862\r\nTrain Epoch: 13 [58880/60000 (98%)]\tLoss: 0.027156\r\nTrain Epoch: 13 [59520/60000 (99%)]\tLoss: 0.005200\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0269, Accuracy: 9913/10000 (99%)\r\n\r\nTrain Epoch: 14 [0/60000 (0%)]\tLoss: 0.004419\r\nTrain Epoch: 14 [640/60000 (1%)]\tLoss: 0.000895\r\nTrain Epoch: 14 [1280/60000 (2%)]\tLoss: 0.006163\r\nTrain Epoch: 14 [1920/60000 (3%)]\tLoss: 0.022425\r\nTrain Epoch: 14 [2560/60000 (4%)]\tLoss: 0.001830\r\nTrain Epoch: 14 [3200/60000 (5%)]\tLoss: 0.053465\r\nTrain Epoch: 14 [3840/60000 (6%)]\tLoss: 0.000409\r\nTrain Epoch: 14 [4480/60000 (7%)]\tLoss: 0.006675\r\nTrain Epoch: 14 [5120/60000 (9%)]\tLoss: 0.039101\r\nTrain Epoch: 14 [5760/60000 (10%)]\tLoss: 0.007779\r\nTrain Epoch: 14 [6400/60000 (11%)]\tLoss: 0.262179\r\nTrain Epoch: 14 [7040/60000 (12%)]\tLoss: 0.147090\r\nTrain Epoch: 14 [7680/60000 (13%)]\tLoss: 0.007032\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.000570\r\nTrain Epoch: 14 [8960/60000 (15%)]\tLoss: 0.023228\r\nTrain Epoch: 14 [9600/60000 (16%)]\tLoss: 0.018134\r\nTrain Epoch: 14 [10240/60000 (17%)]\tLoss: 0.059644\r\nTrain Epoch: 14 [10880/60000 (18%)]\tLoss: 0.003038\r\nTrain Epoch: 14 [11520/60000 (19%)]\tLoss: 0.042322\r\nTrain Epoch: 14 [12160/60000 (20%)]\tLoss: 0.025198\r\nTrain Epoch: 14 [12800/60000 (21%)]\tLoss: 0.013131\r\nTrain Epoch: 14 [13440/60000 (22%)]\tLoss: 0.012800\r\nTrain Epoch: 14 [14080/60000 (23%)]\tLoss: 0.000873\r\nTrain Epoch: 14 [14720/60000 (25%)]\tLoss: 0.034714\r\nTrain Epoch: 14 [15360/60000 (26%)]\tLoss: 0.004934\r\nTrain Epoch: 14 [16000/60000 (27%)]\tLoss: 0.030047\r\nTrain Epoch: 14 [16640/60000 (28%)]\tLoss: 0.092633\r\nTrain Epoch: 14 [17280/60000 (29%)]\tLoss: 0.000209\r\nTrain Epoch: 14 [17920/60000 (30%)]\tLoss: 0.023252\r\nTrain Epoch: 14 [18560/60000 (31%)]\tLoss: 0.096665\r\nTrain Epoch: 14 [19200/60000 (32%)]\tLoss: 0.001450\r\nTrain Epoch: 14 [19840/60000 (33%)]\tLoss: 0.033909\r\nTrain Epoch: 14 [20480/60000 (34%)]\tLoss: 0.003506\r\nTrain Epoch: 14 [21120/60000 (35%)]\tLoss: 0.031981\r\nTrain Epoch: 14 [21760/60000 (36%)]\tLoss: 0.000330\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.006572\r\nTrain Epoch: 14 [23040/60000 (38%)]\tLoss: 0.006415\r\nTrain Epoch: 14 [23680/60000 (39%)]\tLoss: 0.016754\r\nTrain Epoch: 14 [24320/60000 (41%)]\tLoss: 0.000284\r\nTrain Epoch: 14 [24960/60000 (42%)]\tLoss: 0.000512\r\nTrain Epoch: 14 [25600/60000 (43%)]\tLoss: 0.005908\r\nTrain Epoch: 14 [26240/60000 (44%)]\tLoss: 0.010500\r\nTrain Epoch: 14 [26880/60000 (45%)]\tLoss: 0.015310\r\nTrain Epoch: 14 [27520/60000 (46%)]\tLoss: 0.020943\r\nTrain Epoch: 14 [28160/60000 (47%)]\tLoss: 0.007476\r\nTrain Epoch: 14 [28800/60000 (48%)]\tLoss: 0.001789\r\nTrain Epoch: 14 [29440/60000 (49%)]\tLoss: 0.026523\r\nTrain Epoch: 14 [30080/60000 (50%)]\tLoss: 0.022921\r\nTrain Epoch: 14 [30720/60000 (51%)]\tLoss: 0.004790\r\nTrain Epoch: 14 [31360/60000 (52%)]\tLoss: 0.025908\r\nTrain Epoch: 14 [32000/60000 (53%)]\tLoss: 0.015199\r\nTrain Epoch: 14 [32640/60000 (54%)]\tLoss: 0.029967\r\nTrain Epoch: 14 [33280/60000 (55%)]\tLoss: 0.026125\r\nTrain Epoch: 14 [33920/60000 (57%)]\tLoss: 0.001464\r\nTrain Epoch: 14 [34560/60000 (58%)]\tLoss: 0.001012\r\nTrain Epoch: 14 [35200/60000 (59%)]\tLoss: 0.090502\r\nTrain Epoch: 14 [35840/60000 (60%)]\tLoss: 0.004096\r\nTrain Epoch: 14 [36480/60000 (61%)]\tLoss: 0.023374\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.009436\r\nTrain Epoch: 14 [37760/60000 (63%)]\tLoss: 0.078857\r\nTrain Epoch: 14 [38400/60000 (64%)]\tLoss: 0.042718\r\nTrain Epoch: 14 [39040/60000 (65%)]\tLoss: 0.000777\r\nTrain Epoch: 14 [39680/60000 (66%)]\tLoss: 0.042462\r\nTrain Epoch: 14 [40320/60000 (67%)]\tLoss: 0.004990\r\nTrain Epoch: 14 [40960/60000 (68%)]\tLoss: 0.011255\r\nTrain Epoch: 14 [41600/60000 (69%)]\tLoss: 0.005602\r\nTrain Epoch: 14 [42240/60000 (70%)]\tLoss: 0.008519\r\nTrain Epoch: 14 [42880/60000 (71%)]\tLoss: 0.009623\r\nTrain Epoch: 14 [43520/60000 (72%)]\tLoss: 0.011592\r\nTrain Epoch: 14 [44160/60000 (74%)]\tLoss: 0.000449\r\nTrain Epoch: 14 [44800/60000 (75%)]\tLoss: 0.009011\r\nTrain Epoch: 14 [45440/60000 (76%)]\tLoss: 0.025084\r\nTrain Epoch: 14 [46080/60000 (77%)]\tLoss: 0.020728\r\nTrain Epoch: 14 [46720/60000 (78%)]\tLoss: 0.054390\r\nTrain Epoch: 14 [47360/60000 (79%)]\tLoss: 0.012415\r\nTrain Epoch: 14 [48000/60000 (80%)]\tLoss: 0.024724\r\nTrain Epoch: 14 [48640/60000 (81%)]\tLoss: 0.007399\r\nTrain Epoch: 14 [49280/60000 (82%)]\tLoss: 0.001043\r\nTrain Epoch: 14 [49920/60000 (83%)]\tLoss: 0.018420\r\nTrain Epoch: 14 [50560/60000 (84%)]\tLoss: 0.031877\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.042542\r\nTrain Epoch: 14 [51840/60000 (86%)]\tLoss: 0.002729\r\nTrain Epoch: 14 [52480/60000 (87%)]\tLoss: 0.004932\r\nTrain Epoch: 14 [53120/60000 (88%)]\tLoss: 0.000500\r\nTrain Epoch: 14 [53760/60000 (90%)]\tLoss: 0.027206\r\nTrain Epoch: 14 [54400/60000 (91%)]\tLoss: 0.002522\r\nTrain Epoch: 14 [55040/60000 (92%)]\tLoss: 0.001117\r\nTrain Epoch: 14 [55680/60000 (93%)]\tLoss: 0.045572\r\nTrain Epoch: 14 [56320/60000 (94%)]\tLoss: 0.027302\r\nTrain Epoch: 14 [56960/60000 (95%)]\tLoss: 0.018647\r\nTrain Epoch: 14 [57600/60000 (96%)]\tLoss: 0.036808\r\nTrain Epoch: 14 [58240/60000 (97%)]\tLoss: 0.000488\r\nTrain Epoch: 14 [58880/60000 (98%)]\tLoss: 0.006383\r\nTrain Epoch: 14 [59520/60000 (99%)]\tLoss: 0.000301\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\r\nTest set: Average loss: 0.0267, Accuracy: 9915/10000 (99%)\r\n\r\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2025-06-19 20:57:06,591\tINFO\t( 5 min) operation af3941d-fca6ac76-24dd03e8-9b60f6d2 completed\r\n"
                }
            ],
            "source": "!tractorun --run-config-path run_config.yaml"
        }
    ],
    "metadata": {
        "is_solution_notebook": true,
        "kernelspec": {
            "display_name": "chiffa_solutions_torch",
            "name": "chiffa_solutions_torch"
        },
        "tracto": {
            "is_solution_notebook": true,
            "metadata_version": "1",
            "notebook_cypress_id": "13e53a41-d4a7-4e09-8c9a-b021eb86173d"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}