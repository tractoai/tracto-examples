{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "151e6e1b-8925-4bf6-abf9-ecca93758e82",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "cc00ada4",
                    "view_cell_type": "MD",
                    "view_source": "## Trigger types\n### Cron\nRun workflow every 10 minutes:\n```yaml\ntrigger_type: cron\nparams:\n    cron_expression: \"*/10 * * * *\"\n```\n### Yt node update\nRun workflow when `//some/cypress/path` is modified:\n```yaml\ntrigger_type: node_update\nparams:\n    node_path: \"//some/cypress/path\""
                }
            },
            "source": "## Trigger types\n### Cron\nRun workflow every 10 minutes:\n```yaml\ntrigger_type: cron\nparams:\n    cron_expression: \"*/10 * * * *\"\n```\n### Yt node update\nRun workflow when `//some/cypress/path` is modified:\n```yaml\ntrigger_type: node_update\nparams:\n    node_path: \"//some/cypress/path\""
        },
        {
            "cell_type": "markdown",
            "id": "3e9b01e7-b179-48ee-a713-eed676e259db",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "258cf5c9",
                    "view_cell_type": "MD",
                    "view_source": "## Step params\n\n### `step_id` (required)\nDefines a unique (within workflow) identificator of the step. Start with a letter, use alphanumeric values and `_` for it.\n\n### `task_type` (required)\nDetermines what kind of processor will be used to execute the step. See the list of available task types below.\n\n### `task_params`\nA map of parameters to be passed to the chosen task processor.\n\n### `depends_on`\nList of steps (identified with `step_id`) that must be successfully completed before the current step will be executed.\n\n### `secrets`\nList of objects of the following structure:\n```yaml\nkey: <string>\nvalue_src_type: secret_store | cypress_docker_creds | predefined\nvalue_ref: <string>\n```\n- `key` - the key to use to obtain the secret in the step\n- `value_src_type` - one of:\n  - `secret_store` - take the value from a secret store. `value_ref` must be of the following structure: `<cypress_path>:<key_in_secret_store>`. For example, `//some/workflow:my_token`\n  - `secret_store_docker_creds` - use it to authenticate in a docker registry for `docker` task type. The secret store in `value_ref` must contain `username`+`password` or `auth` keys\n  - `predefined` - if `value_ref` is set to `YT_TOKEN`, a default user's token will be available in the step (with predefined key `YT_TOKEN`)\n\n### `outputs`\nSteps can have output params. Those must be defined in outputs section of the step. They can be used as input arguments for children steps. Outputs must be printed to stderr in json format in the last line of the program's output.\n```\n{\n\t\t\t\"step_id\": \"assert_foo_bar\",\n\t\t\t\"task_type\": \"docker\",\n\t\t\t\"task_params\": {\n\t\t\t\t\"env\": {\n\t\t\t\t\t\"FOO\": \"BAR\"\n\t\t\t\t},\n\t\t\t\t\"command\": \"python3 -c 'import json, os; assert os.environ[\\\"FOO\\\"] == \\\"BAR\\\"; print(json.dumps({\\\"key1\\\": \\\"value1\\\"}))' >&2\",\n\t\t\t\t\"docker_image\": \"docker.io/library/python:3.11\"\n\t\t\t},\n            \"max_retries\": 3,\n            \"min_retry_interval_seconds\": 10,\n\t\t\t\"outputs\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"key1\"\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"step_id\": \"get_prev_step_out\",\n\t\t\t\"task_type\": \"docker\",\n\t\t\t\"task_params\": {\n\t\t\t\t\"command\": \"python3 -c 'import os; the_arg = os.environ[\\\"ORC_PARAM_the_arg\\\"]; print(f\\\"Hello, {the_arg}\\\")' >&2\",\n\t\t\t\t\"docker_image\": \"docker.io/library/python:3.11\"\n\t\t\t},\n\t\t\t\"args\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"the_arg\",\n\t\t\t\t\t\"src_type\": \"step_output\",\n\t\t\t\t\t\"src_ref\": \"assert_foo_bar.key1\"\n\t\t\t\t}\n\t\t\t],\n\t\t\t\"secrets\": [\n\t\t\t\t{\n\t\t\t\t\t\"key\": \"YT_TOKEN\",\n\t\t\t\t\t\"value_ref\": \"YT_TOKEN\"\n\t\t\t\t}\n\t\t\t],\n\t\t\t\"depends_on\": [\n\t\t\t\t\"assert_foo_bar\"\n\t\t\t]\n\t\t}\n```\n\n### `max_retries`\nDefines how many times the step will be run in case of failure. Default is `0`\n\n### `min_retry_interval_seconds`\nHow many seconds (at least) should pass before the next retry will happen."
                }
            },
            "source": "## Step params\n\n### `step_id` (required)\nDefines a unique (within workflow) identificator of the step. Start with a letter, use alphanumeric values and `_` for it.\n\n### `task_type` (required)\nDetermines what kind of processor will be used to execute the step. See the list of available task types below.\n\n### `task_params`\nA map of parameters to be passed to the chosen task processor. String parameters can be rendered with args:\n```json\n\"depends_on\": [\"prev_step\"],\n\"args\": [\n    {\n        \"name\": \"image\",\n        \"src_type\": \"step_output\",\n        \"src_ref\": \"prev_step.docker_image\"\n    }\n],\n\"task_params\": {\n    \"docker_image\": \"{{ args.image }}\",\n    \"command\": \"echo hello >&2\"\n}\n```\n\n### `depends_on`\nList of steps (identified with `step_id`) that must be successfully completed before the current step will be executed.\n\n### `secrets`\nList of objects of the following structure:\n```yaml\nkey: <string>\nvalue_src_type: secret_store | cypress_docker_creds | predefined\nvalue_ref: <string>\n```\n- `key` - the key to use to obtain the secret in the step\n- `value_src_type` - one of:\n  - `secret_store` - take the value from a secret store. `value_ref` must be of the following structure: `<cypress_path>:<key_in_secret_store>`. For example, `//some/workflow:my_token`\n  - `secret_store_docker_creds` - use it to authenticate in a docker registry for `docker` task type. The secret store in `value_ref` must contain `username`+`password` or `auth` keys\n  - `predefined` - if `value_ref` is set to `YT_TOKEN`, a default user's token will be available in the step (with predefined key `YT_TOKEN`)\n\n### `outputs`\nSteps can have output params. Those must be defined in outputs section of the step. They can be used as input arguments for children steps. Outputs must be printed to stderr in json format in the last line of the program's output.\n```\n{\n\t\t\t\"step_id\": \"assert_foo_bar\",\n\t\t\t\"task_type\": \"docker\",\n\t\t\t\"task_params\": {\n\t\t\t\t\"env\": {\n\t\t\t\t\t\"FOO\": \"BAR\"\n\t\t\t\t},\n\t\t\t\t\"command\": \"python3 -c 'import json, os; assert os.environ[\\\"FOO\\\"] == \\\"BAR\\\"; print(json.dumps({\\\"key1\\\": \\\"value1\\\"}))' >&2\",\n\t\t\t\t\"docker_image\": \"docker.io/library/python:3.11\"\n\t\t\t},\n            \"max_retries\": 3,\n            \"min_retry_interval_seconds\": 10,\n\t\t\t\"outputs\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"key1\"\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"step_id\": \"get_prev_step_out\",\n\t\t\t\"task_type\": \"docker\",\n\t\t\t\"task_params\": {\n\t\t\t\t\"command\": \"python3 -c 'import os; the_arg = os.environ[\\\"ORC_PARAM_the_arg\\\"]; print(f\\\"Hello, {the_arg}\\\")' >&2\",\n\t\t\t\t\"docker_image\": \"docker.io/library/python:3.11\"\n\t\t\t},\n\t\t\t\"args\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"the_arg\",\n\t\t\t\t\t\"src_type\": \"step_output\",\n\t\t\t\t\t\"src_ref\": \"assert_foo_bar.key1\"\n\t\t\t\t}\n\t\t\t],\n\t\t\t\"secrets\": [\n\t\t\t\t{\n\t\t\t\t\t\"key\": \"YT_TOKEN\",\n\t\t\t\t\t\"value_ref\": \"YT_TOKEN\"\n\t\t\t\t}\n\t\t\t],\n\t\t\t\"depends_on\": [\n\t\t\t\t\"assert_foo_bar\"\n\t\t\t]\n\t\t}\n```\n\n### `max_retries`\nDefines how many times the step will be run in case of failure. Default is `0`\n\n### `min_retry_interval_seconds`\nHow many seconds (at least) should pass before the next retry will happen.\n\n### `cache`\nEnables step caching. If step key is found in cache, its output will be taken from there and the step will not be executed.\nCache key consist of task_type, task_params, args and cache_version.\n```json\n\"cache\": {\n    \"enable\": true,  # means \"write result from cache and take result from cache\" - the same as `enable_write` and `enable_read` both turned on.\n    \"enable_write\": null,  # only write to cache, do not take result from there\n    \"enable_read\": null,  # take result from cache, but do not update cache\n    \"cache_version\": \"v1\"  # change it if you need to invalidate cache record\n}\n```\n\n### `for_each`\nHaving a list input argument LST_ARG, you can run N instances (where N = length of LST_ARG) of the step, and each of these substeps will receive its own element of LST_ARG. Outputs of the substeps will be concated into a list.\n\n```yaml\n- step_id: st1\n  task_type: docker\n  task_params:\n\tdocker_image: docker.io/library/python:3.11\n\tcommand: \"python3 -c 'import json, sys; print(json.dumps({\\\"key_list\\\": [\\\"litem1\\\", \\\"litem2\\\"]}), file=sys.stderr)'\"\n  outputs:\n\t- name: key_list\n- step_id: st2:\n  for_each:\n\tloop_arg_name: for_each_arg\n  task_type: docker\n  task_params:\n\tdocker_image: docker.io/library/python:3.11\n\tcommand: \"python3 -c 'import json, os, sys; for_each = os.environ[\\\"ORC_PARAM_for_each_arg\\\"]; print(json.dumps({\\\"some_output\\\": f\\\"{for_each}_42\\\"}), file=sys.stderr)'\"\n  args:\n\t- name: for_each_arg\n\t  src_type: step_output\n\t  src_ref: st1.key_list\n  outputs:\n\t- name: some_output\n  depends_on:\n\t- st1\n```\nIn this example `some_output` will contain `[\"litem1_42\", \"litem2_42\"]`"
        },
        {
            "cell_type": "markdown",
            "id": "f3e431ea-dbd0-4830-a666-f0b55069246b",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "5d9f6bd7",
                    "view_cell_type": "MD",
                    "view_source": "## `yt_pool_settings`\n\nDefines what YT pool will be used to execute the workflow and its tasks. \n```json\n\"yt_pool_settings\": {\n    \"pool\": \"my_pool\",\n    \"secret_ref\": \"//path/to/secret/store:key_with_token\",\n    \"pool_tree\": null\n}\n```\nToken in `secret_ref` must have access to the specified pool. If `pool_tree` is not set, `default` will be used. If you specify `pool_tree`, `pool` also must be set."
                }
            },
            "source": "## `yt_pool_settings`\n\nDefines what YT pool will be used to execute the workflow and its tasks. \n```json\n\"yt_pool_settings\": {\n    \"pool\": \"my_pool\",\n    \"secret_ref\": \"//path/to/secret/store:key_with_token\",\n    \"pool_tree\": null\n}\n```\nToken in `secret_ref` must have access to the specified pool. If `pool_tree` is not set, `default` will be used. If you specify `pool_tree`, `pool` also must be set."
        },
        {
            "cell_type": "markdown",
            "id": "27bd533a-c28a-4661-893f-64ea5fb6631b",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "c908647b",
                    "view_cell_type": "MD",
                    "view_source": "## Task types\n### `docker`\nRuns a command in a container.\nAvailable task params:\n- `docker_image` - the image code will be run in\n- `command` - the command to run\n- `env` - map of additional environment variables\n\nNuances:\n- secrets are available in environment with prefix `YT_SECURE_VAULT_`. So if you specify a secret with key `my_secret`, it will be available in env as `YT_SECURE_VAULT_my_secret`\n- use only stderr for your code output. The easiest way is to add `>&2` at the end of the command, for example:\n```yaml\ncommand: python3 -c 'print(\"hello world\")' >&2\n```\n\n### `notebook`\nRuns a jupyter notebook stored in YT.\nAvailable task params:\n- `yt_jupyter_kernel` - name of the jupyter kernel to run the notebook on\n- `notebook_path` - path to the notebook\n\n### `python_code`\nSimilar to `docker`, but runs python code (defined in `code` parameter) with default python3 interpreter from specified container.\n\n### `for_each`\nSpecial task type to run several tasks of the same type using a previous step's output. Example:\n```json\n{\n    \"triggers\": [],\n    \"steps\": [\n        {\n            \"step_id\": \"st1\",\n            \"task_type\": \"docker\",\n            \"task_params\": {\n                \"command\": \"python3 -c 'import json, sys; print(json.dumps({\\\"key_list\\\": [\\\"litem1\\\", \\\"litem2\\\"]}), file=sys.stderr)'\",\n                \"docker_image\": \"docker.io/library/python:3.11\"\n            },\n            \"outputs\": [\n                {\n                    \"name\": \"key_list\"\n                }\n            ]\n        },\n        {\n            \"step_id\": \"st2\",\n            \"task_type\": \"for_each\",\n            \"task_params\": {\n                \"nested_arg_name\": \"the_str_arg\",\n                \"loop_arg_name\": \"the_list_arg\",\n                \"nested_task_params\": {\n                    \"command\": \"python3 -c 'import json, os, sys; the_str_arg = os.environ[\\\"ORC_PARAM_the_str_arg\\\"]; print(json.dumps({\\\"some_output\\\": f\\\"value {the_str_arg}\\\"}), file=sys.stderr)'\",\n                    \"docker_image\": \"docker.io/library/python:3.11\"\n                },\n                \"nested_task_type\": \"docker\"\n            },\n            \"args\": [\n                {\n                    \"name\": \"the_list_arg\",\n                    \"src_type\": \"step_output\",\n                    \"src_ref\": \"st1.key_list\"\n                }\n            ],\n            \"depends_on\": [\n                \"st1\"\n            ],\n            \"outputs\": [\n                {\n                    \"name\": \"result\"\n                }\n            ]\n        },\n        {\n            \"step_id\": \"st3\",\n            \"task_type\": \"docker\",\n            \"task_params\": {\n                \"command\": \"python3 -c 'import os, sys; print(os.environ[\\\"ORC_PARAM_the_arg\\\"], file=sys.stderr)'\",\n                \"docker_image\": \"docker.io/library/python:3.11\"\n            },\n            \"args\": [\n                {\n                    \"name\": \"the_arg\",\n                    \"src_type\": \"step_output\",\n                    \"src_ref\": \"st2.result\"\n                }\n            ],\n            \"depends_on\": [\n                \"st2\"\n            ]\n        }\n    ]\n}\n```\nHere step `st2` gets list argument `the_list_arg` produced by step `st1`, and for each value of the list a task (which params are defined in `nested_task_params`) will be launched. Output of this step must be named `result`, and it contains list of nested tasks's outputs."
                }
            },
            "source": "## Task types\n### `docker`\nRuns a command in a container.\nAvailable task params:\n- `docker_image` - the image code will be run in\n- `command` - the command to run\n- `env` - map of additional environment variables\n\nNuances:\n- secrets are available in environment with prefix `YT_SECURE_VAULT_`. So if you specify a secret with key `my_secret`, it will be available in env as `YT_SECURE_VAULT_my_secret`\n- use only stderr for your code output. The easiest way is to add `>&2` at the end of the command, for example:\n```yaml\ncommand: python3 -c 'print(\"hello world\")' >&2\n```\n\n### `notebook`\nRuns a jupyter notebook stored in YT.\nAvailable task params:\n- `yt_jupyter_kernel` - name of the jupyter kernel to run the notebook on\n- `notebook_path` - path to the notebook\n\n### `python_code`\nSimilar to `docker`, but runs python code (defined in `code` parameter) with default python3 interpreter from specified container.\n"
        },
        {
            "cell_type": "markdown",
            "id": "0819b63c-5959-47ad-9396-f437e8ee6c96",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "d30dab89",
                    "view_cell_type": "MD"
                }
            },
            "source": "## Workflow-level parameters\nWorkflow-level parameters can be used in different steps. Values can be set when creating a new run; if a value is not set, a default value will be used (null, if not specified in workflow config).\n```yaml\nworkflow_params:\n    - name: my_param_1\n      default_value: 123\n    - name: my_param_2  # => default_value: null\n```\nPress \"Run with params\" to set required values.\n\nThese params now can be used in step args:\n```yaml\nargs:\n  - name: my_arg\n    src_type: workflow_param\n    src_ref: my_param_1\n```"
        },
        {
            "cell_type": "markdown",
            "id": "f15b62fc-7201-4ac3-a50e-726781f7ae0b",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "7929ba3d",
                    "view_cell_type": "MD",
                    "view_source": "## Command line interface\n\n### Installation\n`pip install orchestracto-client`\n\n### Usage\n`YT_PROXY` and `YT_TOKEN` env vars must be set.\n\nCreating or updating workflows:\n```bash\n$ orc workflow update --wf-path //path/to/workflow --from-file /local/path/to/workflow/config.yaml\n\n$ orc workflow update --wf-path //path/to/workflow --from-file /local/path/to/workflow/config.json --input-format json\n```\n\nCreating runs:\n```bash\n$ orc run create --wf-path //path/to/workflow\nrun_id: dc8f2600-972441b2-82331579-5654fcb1\n\n$ orc run create --wf-path //path/to/workflow --label mylbl\nrun_id: 2c8fe31b-62ad446d-a4716d54-861ca02c\n\n$ orc run create --wf-path //path/to/workflow --label mylbl --label anotherlbl\nrun_id: f11b8da4-75974a2c-86a594e9-5a4e481a\n```\n\nGetting runs:\n```bash\n$ orc workflow get-runs --wf-path //path/to/workflow --label mylbl --label anotherlbl\n- created_at: '2025-01-22T11:36:58Z'\n  finished_at: null\n  labels:\n  - mylbl\n  - anotherlbl\n  run_id: f11b8da4-75974a2c-86a594e9-5a4e481a\n  stage: to_do\n  trigger_type: one_time_run\n  workflow_path: //path/to/workflow\n\n$ orc --format json workflow get-runs --wf-path //path/to/workflow --start-dt 2025-01-21T18:40:00Z --end-dt 2025-01-21T18:50:00Z | jq -r '.[] | \"\\(.stage) \\(.created_at)\"'\ndone 2025-01-21T18:46:05Z\n\n$ orc --format json run get --wf-path //path/to/workflow --run-id 9ae01f21-16784aea-8d48bba3-33404adf | jq -r .yt_operation_id\n3c17e72-fa9c3a2-270703e8-4007351d\n```\n\nGetting run logs:\n```bash\n$ orc run get-logs --wf-path //path/to/workflow --run-id ab3b0734-da174153-8bc6c6be-ced658db | tail -n 5\nts=2025-01-15 16:21:55.226\tname=orc.wf_executor.executor\tlevel=INFO    \trun_id=ab3b0734-da174153-8bc6c6be-ced658db\tmsg=Handling running steps\nts=2025-01-15 16:21:55.238\tname=orc.wf_executor.executor\tlevel=INFO    \trun_id=ab3b0734-da174153-8bc6c6be-ced658db\tmsg=Checking running step step_2_2\nts=2025-01-15 16:21:55.491\tname=orc.wf_executor.executor\tlevel=INFO    \trun_id=ab3b0734-da174153-8bc6c6be-ced658db\tmsg=step step_2_2 finished: failed=False\nts=2025-01-15 16:21:58.518\tname=orc.wf_executor.executor\tlevel=INFO    \trun_id=ab3b0734-da174153-8bc6c6be-ced658db\tmsg=All steps have been done\n```\n"
                }
            },
            "source": "## Command line interface\n\n### Installation\n`pip install orchestracto-client`\n\n### Usage\n`YT_PROXY` and `YT_TOKEN` env vars must be set.\n\nCreating or updating workflows:\n```bash\n$ orc workflow update --wf-path //path/to/workflow --from-file /local/path/to/workflow/config.yaml\n\n$ orc workflow update --wf-path //path/to/workflow --from-file /local/path/to/workflow/config.json --input-format json\n```\n\nCreating runs:\n```bash\n$ orc run create --wf-path //path/to/workflow\nrun_id: dc8f2600-972441b2-82331579-5654fcb1\n\n$ orc run create --wf-path //path/to/workflow --label mylbl\nrun_id: 2c8fe31b-62ad446d-a4716d54-861ca02c\n\n$ orc run create --wf-path //path/to/workflow --label mylbl --label anotherlbl\nrun_id: f11b8da4-75974a2c-86a594e9-5a4e481a\n```\n\nGetting runs:\n```bash\n$ orc workflow get-runs --wf-path //path/to/workflow --label mylbl --label anotherlbl\n- created_at: '2025-01-22T11:36:58Z'\n  finished_at: null\n  labels:\n  - mylbl\n  - anotherlbl\n  run_id: f11b8da4-75974a2c-86a594e9-5a4e481a\n  stage: to_do\n  trigger_type: one_time_run\n  workflow_path: //path/to/workflow\n\n$ orc --format json workflow get-runs --wf-path //path/to/workflow --start-dt 2025-01-21T18:40:00Z --end-dt 2025-01-21T18:50:00Z | jq -r '.[] | \"\\(.stage) \\(.created_at)\"'\ndone 2025-01-21T18:46:05Z\n\n$ orc --format json run get --wf-path //path/to/workflow --run-id 9ae01f21-16784aea-8d48bba3-33404adf | jq -r .yt_operation_id\n3c17e72-fa9c3a2-270703e8-4007351d\n```\n\nGetting run logs:\n```bash\n$ orc run get-logs --wf-path //path/to/workflow --run-id ab3b0734-da174153-8bc6c6be-ced658db | tail -n 5\nts=2025-01-15 16:21:55.226\tname=orc.wf_executor.executor\tlevel=INFO    \trun_id=ab3b0734-da174153-8bc6c6be-ced658db\tmsg=Handling running steps\nts=2025-01-15 16:21:55.238\tname=orc.wf_executor.executor\tlevel=INFO    \trun_id=ab3b0734-da174153-8bc6c6be-ced658db\tmsg=Checking running step step_2_2\nts=2025-01-15 16:21:55.491\tname=orc.wf_executor.executor\tlevel=INFO    \trun_id=ab3b0734-da174153-8bc6c6be-ced658db\tmsg=step step_2_2 finished: failed=False\nts=2025-01-15 16:21:58.518\tname=orc.wf_executor.executor\tlevel=INFO    \trun_id=ab3b0734-da174153-8bc6c6be-ced658db\tmsg=All steps have been done\n```\n\nRestart failed steps of a finished run:\n```bash\n$ orc run restart --wf-path //path/to/workflow --run-id 95a81a56-186f420d-a037d6ad-81a54fd8\n```\n\nRestart all steps:\n```bash\n$ orc run restart --wf-path //path/to/workflow --run-id 95a81a56-186f420d-a037d6ad-81a54fd8 --restart-successful-steps\n```\n\nRestart failed and specified steps (and their descendants):\n```bash\n$ orc run restart --wf-path //path/to/workflow --run-id 95a81a56-186f420d-a037d6ad-81a54fd8 --restart-step st_3-1 --restart-step st_3-2\n```\n"
        },
        {
            "cell_type": "markdown",
            "id": "c5a4a400-0bc0-4cb6-beb2-94e0e2ed4c5e",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "952ecf7c",
                    "view_cell_type": "MD"
                }
            },
            "source": "## Workflow cancellation\n\nPress \"Cancel\" on running workflow to stop its execution. All running steps will also be stopped.\nSDK tasks (built with sdk `>= 0.0.15`) and docker tasks (with command call via `exec`) can handle `SIGTERM` signal, they are given 30 seconds (and after that process will receive `SIGKILL`)."
        },
        {
            "cell_type": "markdown",
            "id": "c424eda9-9a48-4520-a6d8-d66ce302f537",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "b14bf93d",
                    "view_cell_type": "MD"
                }
            },
            "source": "## Python SDK\n\n[Available on pypi](https://pypi.org/project/orchestracto-sdk/)\n\nSome examples [are available on Github](https://github.com/tractoai/tracto-examples/tree/main/orchestracto)\n\nRun `WF_BASE_PATH=//home/some_map_node YT_PROXY=... YT_TOKEN=... orc sdk process ./orchestracto/example_yt/example.py` - it will create workflow config with required docker images and upload them to cypress. \n**Create required secret stores (in this case - `//home/some_map_node/secrets`) in advance**"
        },
        {
            "cell_type": "markdown",
            "id": "c14a76d3-eb36-46e8-875e-a365bd82dd59",
            "metadata": {
                "tracto": {
                    "metadata_version": "1",
                    "source_hash": "e3b0c442",
                    "view_cell_type": "MD"
                }
            },
            "source": ""
        }
    ],
    "metadata": {
        "is_solution_notebook": true,
        "tracto": {
            "is_solution_notebook": true,
            "metadata_version": "1",
            "notebook_cypress_id": "b051b9a5-d1d8-4c2d-a139-98480df0e914"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}