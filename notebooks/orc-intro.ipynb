{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "13970fea",
            "metadata": {
                "cell_id": "f0394c64-dfab-46af-89a2-968f9cddd32a"
            },
            "source": "Orchestracto is a scheduling system that can help you run your regular pipelines easily.\n\n## Installation and initialization\n\n- Create new virtualenv (or use existing one) and install the provided wheel. Python >=3.11 is required.\n```bash\npython -m venv /path/to/venv\nsource /path/to/venv/bin/activate\npython -m pip install ./orchestracto-0.0.0-py3-none-any.whl\n```\n- Or you can just use a jupyter kernel image with installed orchestracto:\n`cr.eu-north1.nebius.cloud/e00faee7vas5hpsh3s/solutions/orchestracto:v1`"
        },
        {
            "cell_type": "markdown",
            "id": "c56b23c0",
            "metadata": {
                "cell_id": "7e3eab56-2450-402b-9f7f-1688696333a8"
            },
            "source": "Now you have `orc` command line tool available:"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "892a89b1",
            "metadata": {
                "cell_id": "6349e5cf-e97c-4b82-82e5-d07440802cd8"
            },
            "outputs": [],
            "source": "!orc --help"
        },
        {
            "cell_type": "markdown",
            "id": "c3afe32b",
            "metadata": {
                "cell_id": "c73e7993-c309-46ff-bf41-31582fc25810"
            },
            "source": "Let's init a new instance of orchestracto. You need a directory on your Tracto cluster for that (more reliable that a dir in `//tmp`)"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "372b2afe",
            "metadata": {
                "cell_id": "fc805c00-cd28-44ed-94fd-b969be3ae05b"
            },
            "outputs": [],
            "source": "!orc init //tmp/orc-example"
        },
        {
            "cell_type": "markdown",
            "id": "b5abcfa0",
            "metadata": {
                "cell_id": "c909b6b8-7b13-4053-aaae-c5b38ca810f3"
            },
            "source": "Now you have some directories prepared:"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "ea354cbf",
            "metadata": {
                "cell_id": "fd9f3fbc-d3c1-4dab-a01f-3b1fb2d2f42f"
            },
            "outputs": [],
            "source": "!yt list //tmp/orc-example"
        },
        {
            "cell_type": "markdown",
            "id": "5f708790",
            "metadata": {
                "cell_id": "5362ef4c-41cb-4744-8bef-075556c0b652"
            },
            "source": "Let's start a scheduler process:"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "1e0bf71e",
            "metadata": {
                "cell_id": "a677e4a1-de25-4820-9b0c-17b6f44ab841"
            },
            "outputs": [],
            "source": "!orc --orc-dir //tmp/orc-example scheduler start --detached"
        },
        {
            "cell_type": "markdown",
            "id": "d3237e33",
            "metadata": {
                "cell_id": "d5dd78b2-da8c-45ff-b2bb-0fa7a8d3d741"
            },
            "source": "## Workflows\n\nLet's open this dir in navigation. There in `workflows` directory you can create a new workflow, just like you create a notebook or a simple document.\nThe name of the document becomes `workflow_id` that identifies it within this orchestracto instance. Let's use `my_wf` as workflow id here.\nYou will see a JSON editor with some dummy workflow. Let's replace it with something more interesting:\n```JSON\n{\n\t\"triggers\": [\n\t\t{\n\t\t\t\"trigger_type\": \"cron\",\n\t\t\t\"params\": {\n\t\t\t\t\"cron_expression\": \"*/10 * * * *\"\n\t\t\t}\n\t\t}\n\t],\n\t\"steps\": [\n\t\t{\n\t\t\t\"step_id\": \"assert_foo_bar\",\n\t\t\t\"task_type\": \"docker\",\n\t\t\t\"task_params\": {\n\t\t\t\t\"env\": {\n\t\t\t\t\t\"FOO\": \"BAR\"\n\t\t\t\t},\n\t\t\t\t\"command\": \"python3 -c 'import json, os; assert os.environ[\\\"FOO\\\"] == \\\"BAR\\\"; print(json.dumps({\\\"key1\\\": \\\"value1\\\"}))' >&2\",\n\t\t\t\t\"docker_image\": \"docker.io/library/python:3.11\"\n\t\t\t},\n            \"max_retries\": 3,\n            \"min_retry_interval_seconds\": 10,\n\t\t\t\"outputs\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"key1\"\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"step_id\": \"get_prev_step_out\",\n\t\t\t\"task_type\": \"docker\",\n\t\t\t\"task_params\": {\n\t\t\t\t\"command\": \"python3 -c 'import os; the_arg = os.environ[\\\"ORC_PARAM_the_arg\\\"]; print(f\\\"Hello, {the_arg}\\\")' >&2\",\n\t\t\t\t\"docker_image\": \"docker.io/library/python:3.11\"\n\t\t\t},\n\t\t\t\"args\": [\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"the_arg\",\n\t\t\t\t\t\"src_type\": \"step_output\",\n\t\t\t\t\t\"src_ref\": \"assert_foo_bar.key1\"\n\t\t\t\t}\n\t\t\t],\n\t\t\t\"secrets\": [\n\t\t\t\t{\n\t\t\t\t\t\"key\": \"YT_TOKEN\",\n\t\t\t\t\t\"value_ref\": \"YT_TOKEN\"\n\t\t\t\t}\n\t\t\t],\n\t\t\t\"depends_on\": [\n\t\t\t\t\"assert_foo_bar\"\n\t\t\t]\n\t\t}\n\t]\n}\n```\n### Dependencies\nHere we define a workflow consisting of two steps: `assert_foo_bar` and `get_prev_step_out`. The second step depends on the first one, as you can see in `depends_on` parameter. \n\n### Task types\nBoth steps are of the same task type: `docker`. This task type allows you to run a command (`task_params->command`) in a docker container (image is defined in `task_params->docker_image`). The command should not write anything to stdout: please use stderr instead. If your docker registry is private, add a secret with the following content (the document in `value_ref` must contain `username`+`password` or `auth` fields):\n```json\n{\n    \"key\": \"docker_auth\",\n    \"value_src_type\": \"cypress_docker_creds\",\n    \"value_ref\": \"//cypress/path/to/document\"\n}\n```\n\nThere is also `notebook` task type available. It runs a jupyter notebook in an existing kernel. Its configuration looks something like this:\n```json\n{\n    \"task_type\": \"notebook\",\n    \"task_params\": {\n        \"notebook_path\": \"//some/path/to/notebook\",\n        \"yt_jupyter_kernel\": \"my_lovely_kernel\"\n    }\n}\n```\n\n### Output params and args\nSteps can have output params. Those must be defined in `outputs` section of the step. They can be used as input arguments for children steps, just like step `get_prev_step_out` uses an output of `assert_foo_bar` step -- see `args` section. Outputs must be printed to stderr in json format in the last line of the program's output.\n\n### Retries\nFailing steps can be retries: `max_retries` times with `min_retry_interval_seconds` between attempts.\n\n### Secrets\nSteps also have `secrets` section. As for now there is only one type of secrets available - YT_TOKEN that orchestracto has been launched with. It will be available in environment variable `YT_SECURE_VAULT_YT_TOKEN`.\n\n### Triggers\nThis workflow also has a trigger. There are two types of triggers available so far: `cron` and `node_update`. In the example above the trigger is configured to run the workflow every 10 minutes. `node_update` trigger watches if a cypress node has changed, here is a configuration example:\n```json\n{\n    \"trigger_type\": \"node_update\",\n    \"params\": {\n        \"node_path\": \"//some/cypress/path\"\n    }\n}\n```"
        },
        {
            "cell_type": "markdown",
            "id": "0fbc5619",
            "metadata": {
                "cell_id": "e7c5db93-9061-4f81-b349-e24ffe15b47c"
            },
            "source": "## Manual run\nYou can run it manually in workflow editor UI or via cli tool:"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "23143122",
            "metadata": {
                "cell_id": "6c59fc21-22cd-46cd-a450-92119f9d3f0f"
            },
            "outputs": [],
            "source": "!orc --orc-dir //tmp/orc-example workflow run my_wf"
        },
        {
            "cell_type": "markdown",
            "id": "03147e10",
            "metadata": {
                "cell_id": "c7bc9088-f653-45c3-8d12-66770cccfcc5"
            },
            "source": "It creates a run request that is handled by scheduler process.\nYou can track the execution progress on `Runs` tab on the workflow's page."
        },
        {
            "cell_type": "markdown",
            "id": "499fa1d5",
            "metadata": {
                "cell_id": "1b5c425a-b05a-46a7-b16e-42fa9203b2b8"
            },
            "source": "## Logs\nWill be better soon. As for now you can find execution logs in workflow and step execution operations, which can be found by their run_id (filter operations by title, run_id are in it). There you can find job stderr links (on `Jobs` tab)."
        }
    ],
    "metadata": {
        "is_solution_notebook": true,
        "notebook_cypress_id": "80bc54e1-51cd-4e59-9be7-ebd575db3756"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}